[
  {
    "objectID": "workflows/ssu/sampledata/index.html",
    "href": "workflows/ssu/sampledata/index.html",
    "title": "1. Sample Metadata",
    "section": "",
    "text": "NoteData & Scripts\n\n\n\nThe table below contains links to files and scripts needed to run this workflow–specifically the sampledata_processing.zip archive which contains the necessary assets. Also included are the sample data tables generated at the end of the pipeline.\n\n\n\n\n\n\nFile Name\n\n\n\nDescription\n\n\n\n\n\n\nsampledata_processing.zip\n\n\nArchive contains metadata and fastq rename lookup files for all six sequencing runs, the bash renaming script, and the fastq_info.txt file. \n\n\n\n\n\nall_metadata.txt\n\n\nComplete sample metadata table. \n\n\n\n\n\nsample_data.rds\n\n\nRDS file of sample dataframe that contains only the most relevant metadata for downstream analyses. \n\n\n\n\n\nsample_data.txt\n\n\nTab delimited text file version of sample dataframe that contains only the most relevant metadata for downstream analyses. \n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home",
      "16S rRNA",
      "1. Sample Metadata"
    ]
  },
  {
    "objectID": "workflows/ssu/sampledata/index.html#workflow-input",
    "href": "workflows/ssu/sampledata/index.html#workflow-input",
    "title": "1. Sample Metadata",
    "section": "",
    "text": "NoteData & Scripts\n\n\n\nThe table below contains links to files and scripts needed to run this workflow–specifically the sampledata_processing.zip archive which contains the necessary assets. Also included are the sample data tables generated at the end of the pipeline.\n\n\n\n\n\n\nFile Name\n\n\n\nDescription\n\n\n\n\n\n\nsampledata_processing.zip\n\n\nArchive contains metadata and fastq rename lookup files for all six sequencing runs, the bash renaming script, and the fastq_info.txt file. \n\n\n\n\n\nall_metadata.txt\n\n\nComplete sample metadata table. \n\n\n\n\n\nsample_data.rds\n\n\nRDS file of sample dataframe that contains only the most relevant metadata for downstream analyses. \n\n\n\n\n\nsample_data.txt\n\n\nTab delimited text file version of sample dataframe that contains only the most relevant metadata for downstream analyses. \n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home",
      "16S rRNA",
      "1. Sample Metadata"
    ]
  },
  {
    "objectID": "workflows/ssu/sampledata/index.html#overview",
    "href": "workflows/ssu/sampledata/index.html#overview",
    "title": "1. Sample Metadata",
    "section": "Overview",
    "text": "Overview\nThis workflow is designed to process our raw metadata files and generate more informative sample names. Our aim is to standardize metadata entries, generate new sample names, and save sample data frames that are used in downstream analyses.",
    "crumbs": [
      "Home",
      "16S rRNA",
      "1. Sample Metadata"
    ]
  },
  {
    "objectID": "workflows/ssu/sampledata/index.html#curate-metadata",
    "href": "workflows/ssu/sampledata/index.html#curate-metadata",
    "title": "1. Sample Metadata",
    "section": "Curate Metadata",
    "text": "Curate Metadata\nThe first step in to load all metadata files (one for each sequencing run) and combine the files.\n\nfile_paths &lt;- list.files(path = paste0(here(work_here, \"/metadata_files\")), \n                         pattern = \"\\\\.csv$\", full.names = TRUE)\ntmp_meta &lt;- vroom::vroom(file_paths)\ncolnames(tmp_meta)[1] &lt;- \"Original_name\"\n\nNext we remove any duplicate entries. We have duplicates because some samples were sequenced more than once due to low read count from the initial run.\n\ndups_in_md &lt;- tmp_meta[duplicated(tmp_meta[,1:1]),]\ntmp_meta &lt;- tmp_meta[!duplicated(tmp_meta[,1:1]),]\n\nNext we remove several samples due to low initial read count. We pulled these out before doing any renaming, processing, etc. so they are only listed in the metadata files.\n\n#### Regex Code for partial matches\ntmp_lc_samp &lt;- c(\"7512\", \"8978-S\", \"8978-H\", \"7640-S\")\ntmp_meta &lt;- tmp_meta[!grepl(paste(tmp_lc_samp, collapse = \"|\"), \n                            tmp_meta$Original_name), ]\n\nAnd finally do a little maintenance on column names. What we are after here is standardized column names without spaces, periods, dashes, etc.\n\ncolnames(tmp_meta) &lt;- gsub(\"-| |\\\\.+\", \"_\", colnames(tmp_meta))\ncolnames(tmp_meta)[12] &lt;- \"Species_pair\"\ncolnames(tmp_meta) &lt;- gsub(\"_Original\", \"\", colnames(tmp_meta))\n\nOnce this is finished we can go ahead and select columns in the metadata that are most important for downstream analysis.\n\ntmp_meta &lt;- tmp_meta %&gt;% \n  dplyr::relocate(c(\"Ocean\", \"Morphospecies\", \"Tissue\", \n                    \"Habitat\", \"Site\", \"Site_2\", \"Taxon\", \n                    \"Length\", \"Station_no\", \"Species_pair\", \n                    \"Species_group\", \"Species_complex\", \n                    \"Plate\", \"Row\", \"Column\"), \n                  .after = \"Original_name\")\ntmp_meta[17:ncol(tmp_meta)] &lt;- NULL\n\nAt this point we need to standardize some of the values in the metadata categories. This includes things like replacing spaces with underscores (_). The goal here is to eliminate anything that may cause issues later in the workflows.\n\ntmp_meta &lt;- tmp_meta %&gt;%\n  # Fix specific replacements\n  mutate(\n    Ocean = str_replace_all(Ocean, c(\n      \"Pacific\" = \"Eastern_Pacific\",\n      \"Western Atlantic\" = \"Western_Atlantic\"\n    )),\n    Taxon = str_replace_all(Taxon, \"Snapping shrimp\", \"Snapping_shrimp\")\n  ) %&gt;%\n  \n  # Normalize text by replacing spaces/dots in multiple columns\n  mutate(\n    Morphospecies = str_replace_all(Morphospecies, \" +\", \"_\"),\n    Species_pair  = str_replace_all(Species_pair, c(\n      \"\\\\. +\" = \"_\",\n      \" / \"   = \"-\",\n      \" +\"    = \"_\"\n    )),\n    Species_group   = str_replace_all(Species_group, c(\"\\\\. \" = \"_\", \" \" = \"_\")),\n    Species_complex = str_replace_all(Species_complex, \"\\\\. \", \"_\"),\n    Habitat   = str_replace_all(Habitat, \" +\", \"_\"),\n    Site      = str_replace_all(Site, \" +\", \"_\"),\n    Site_2    = str_replace_all(Site_2, \" +\", \"_\"),\n    Plate     = str_replace_all(Plate, \" \", \"_\")\n  ) %&gt;%\n  \n  # Map multiple Taxon values to \"Environmental\"\n  mutate(\n    Taxon = str_replace_all(Taxon, c(\n      \"Rubble\"   = \"Environmental\", \n      \"Sediment\" = \"Environmental\", \n      \"Mud\"      = \"Environmental\",\n      \"Water\"    = \"Environmental\"\n    ))\n  )\n\n# Replace NA in Morphospecies with \"Environmental\"\ntmp_meta &lt;- tmp_meta %&gt;%\n  mutate(Morphospecies = replace_na(Morphospecies, \"Environmental\"))",
    "crumbs": [
      "Home",
      "16S rRNA",
      "1. Sample Metadata"
    ]
  },
  {
    "objectID": "workflows/ssu/sampledata/index.html#sample-shortcodes",
    "href": "workflows/ssu/sampledata/index.html#sample-shortcodes",
    "title": "1. Sample Metadata",
    "section": "Sample Shortcodes",
    "text": "Sample Shortcodes\nNext we want to create more meaningful sample names. For this we generate some shortcodes that capture key information about each sample, specifically:\n\nthe sampling ocean\nthe species name\nthe tissue type\n\nThese we will combine with the unique individual ID number to get the new name.\nFor example, something like EP_A_HEBE_GL_8937 is the gill tissue (GL) from an Alpheus hebes (A-HEBE), individual ID 8937, collected in the Eastern Pacific (EP).\nWe use a similar convention for environmental samples, but instead of species name, we use E_SAMP to delineate environmental samples. For example, WA_E_SAMP_WT_3075 is a water sample (WT) collected from the Western Atlantic (WA), unique id 3075.\n\nClick to see the the shortcodes for each variable.tmp_ocean &lt;- c(\n  \"Control\" = \"CON\",\n  \"Eastern_Pacific\" = \"EP\",\n  \"Western_Atlantic\" = \"WA\"\n)\n\ntmp_tissue &lt;- c(\n  \"Control\" = \"CON\", \n  \"Egg\" = \"EG\", \n  \"Gill\" = \"GL\", \n  \"Hepatopancreas\" = \"HP\", \n  \"Midgut\" = \"MG\", \n  \"Stomach\" = \"ST\", \n  \"Mud\" = \"MD\", \n  \"Rubble\" = \"RB\", \n  \"Sediment\" = \"SD\", \n  \"Water\" = \"WT\"\n)\n\ntmp_species &lt;- c(\n  \"Control\" = \"CON\", \n  \"Alpheus_arenensis\" = \"A_AREN\", \n  \"Alpheus_bahamensis\" = \"A_BAHA\", \n  \"Alpheus_bouvieri\" = \"A_BOUV\", \n  \"Alpheus_cristulifrons\" = \"A_CRIS\", \n  \"Alpheus_fasciatus\" = \"A_FASC\", \n  \"Alpheus_floridanus\" = \"A_FLOR\", \n  \"Alpheus_formosus\" = \"A_FORM\", \n  \"Alpheus_galapagensis\" = \"A_GALA\", \n  \"Alpheus_hebes\" = \"A_HEBE\", \n  \"Alpheus_hephaestus\" = \"A_HEPH\", \n  \"Alpheus_hyeyoungae\" = \"A_HYEY\", \n  \"Alpheus_javieri\" = \"A_JAVI\", \n  \"Alpheus_millsae\" = \"A_MILL\", \n  \"Alpheus_nuttingi\" = \"A_NUTT\", \n  \"Alpheus_panamensis\" = \"A_PANA\", \n  \"Alpheus_paracrinitus_no_spot\" = \"A_PCNS\", \n  \"Alpheus_paracrinitus_with_spot\" = \"A_PCWS\", \n  \"Alpheus_paraformosus\" = \"A_PARA\", \n  \"Alpheus_platycheirus\" = \"A_PLAT\", \n  \"Alpheus_rostratus\" = \"A_ROST\", \n  \"Alpheus_saxidomus\" = \"A_SAXI\", \n  \"Alpheus_simus\" = \"A_SIMU\", \n  \"Alpheus_thomasi\" = \"A_THOM\", \n  \"Alpheus_umbo\" = \"A_UMBO\", \n  \"Alpheus_utriensis\" = \"A_UTRI\", \n  \"Alpheus_verrilli\" = \"A_VERR\", \n  \"Alpheus_websteri\" = \"A_WEBS\", \n  \"Environmental\" = \"E_SAMP\", \n  \"Unknown\" = \"UNKN\"\n)\n\n\nWith these shortcodes in hand we can match each original sample names (from the fastq file names) to the shortcodes.\n\ntmp_shortcode &lt;- tmp_meta\ntmp_shortcode &lt;- tmp_shortcode %&gt;% select(1:4, 8)\n\ntmp_shortcode &lt;- tmp_shortcode %&gt;%\n  mutate(\n    Ocean_code   = Ocean,\n    Species_code = Morphospecies,\n    Tissue_code  = Tissue\n  )\n\ntmp_shortcode &lt;- tmp_shortcode %&gt;%\n  mutate(\n    Ocean_code   = str_replace_all(Ocean, tmp_ocean),\n    Species_code = str_replace_all(Morphospecies, tmp_species),\n    Tissue_code  = str_replace_all(Tissue, tmp_tissue)\n  )\ntmp_shortcode$Ocean_code &lt;- tmp_shortcode$Ocean\ntmp_shortcode &lt;- tmp_shortcode %&gt;%\n  mutate(Ocean_code = str_replace_all(Ocean_code, tmp_ocean)) \n\ntmp_shortcode$Species_code &lt;- tmp_shortcode$Morphospecies\ntmp_shortcode &lt;- tmp_shortcode %&gt;%\n  mutate(Species_code = str_replace_all(Species_code, tmp_species)) \n\ntmp_shortcode$Tissue_code &lt;- tmp_shortcode$Tissue\ntmp_shortcode &lt;- tmp_shortcode %&gt;%\n  mutate(Tissue_code = str_replace_all(Tissue_code, tmp_tissue)) \n\ntmp_shortcode[2:4] &lt;- NULL\n\ntmp_shortcode &lt;- tmp_shortcode %&gt;% \n  dplyr::relocate(\"Taxon\", .after = \"Tissue_code\")\n\n\n\n\n---------------------------------------------------------------------------\n Original_name   Ocean_code   Species_code   Tissue_code        Taxon      \n--------------- ------------ -------------- ------------- -----------------\n    7322-M           EP          A_PANA          MG        Snapping_shrimp \n\n    7322-H           EP          A_PANA          HP        Snapping_shrimp \n\n    7322-S           EP          A_PANA          ST        Snapping_shrimp \n\n    7322-G           EP          A_PANA          GL        Snapping_shrimp \n\n    7326-M           EP          A_PANA          MG        Snapping_shrimp \n\n    7326-G           EP          A_PANA          GL        Snapping_shrimp \n---------------------------------------------------------------------------\n\n\nStill with me? Even though we have the pieces in place to standardize the sample names, we cannot do that yet because different sample types have different initial naming conventions. For examples, Control samples are named like Control-1, Control-2, etc., while the snapping shrimp are named like 7322-M, 7322-H, etc. Environmental samples have totally different names, for example Machete_scrap2, ML2670, etc. This makes it a litle difficult to parse out meaningful information (specifically ID numbers) from the original sample names and apply our short codes to generate the new names.\nWhen we generated the short codes above we also included a column called Taxon. These data tell us whether a the entry is a snapping shrimp, a control, or an environmental sample.\n\nunique(tmp_shortcode$Taxon)\n\n[1] \"Snapping_shrimp\" \"Control\"  \"Environmental\"  \nWe can use the base R command split to separate the dataset based on the Taxon type so that we can process each category separately.\n\ntmp_split_dfs &lt;- split(tmp_shortcode, tmp_shortcode$Taxon)\n\nGenerating three separate data frames.\ntmp_split_dfs$Control\n\ntmp_split_dfs$Environmental\n\ntmp_split_dfs$Snapping_shrimp\nControl samples\n\ntmp_control &lt;- tmp_split_dfs$Control\ntmp_control$tmp &lt;- tmp_control$Original_name\n\ntmp_control &lt;- tmp_control %&gt;%\n  separate_wider_delim(tmp, delim = \"-\", names = c(\"tmp1\", \"ID\"))\ntmp_control$tmp1 &lt;- NULL\n\nEnvironmental samples\n\ntmp_envr &lt;- tmp_split_dfs$Environmental\ntmp_envr$tmp &lt;- tmp_envr$Original_name\n\ntmp_envr &lt;- tmp_envr %&gt;%\n  separate_wider_delim(tmp,\n                       delim = stringr::regex(\"(_)|(ML)\"),\n                       too_few = \"align_start\",\n                       names_sep = \"\",\n                       names_repair = ~ sub(\"value\", \"X\", .x))\n\ntmp_envr$tmp2 &lt;- str_replace(tmp_envr$tmp2, \"(\\\\d+).*\", \"\\\\1\")\ntmp_envr$tmp2 &lt;- str_replace(tmp_envr$tmp2, \"Sed\", \"sed\")\ntmp_envr$tmp1 &lt;- NULL\ntmp_envr &lt;- tmp_envr %&gt;% dplyr::rename(ID = tmp2)\n\nShrimp samples\n\ntmp_shrmp &lt;- tmp_split_dfs$Snapping_shrimp\n\ntmp_shrmp$tmp &lt;- tmp_shrmp$Original_name\ntmp_shrmp %&gt;%\n  filter(Original_name == \"7332-H-M\")\ntmp_shrmp$tmp &lt;- gsub(\"7332-H-M\", \"7332M-H\", tmp_shrmp$tmp)\n\ntmp_shrmp &lt;- tmp_shrmp %&gt;%\n  separate_wider_delim(tmp,\n                       delim = \"-\",\n                       names = c(\"tmp1\", \"tmp2\")\n                       )\ntmp_shrmp %&gt;%\n  filter(Original_name == \"7332-H-M\")\ntmp_shrmp$tmp2 &lt;- NULL\ntmp_shrmp &lt;- tmp_shrmp %&gt;% dplyr::rename(ID = tmp1)\n\nSweet. At this point we can generate the new unique name for each sample based on the criteria listed above.\n\ntmp_control$SampleID &lt;- tmp_control$Original_name\ntmp_control$SampleID &lt;- gsub(\"-\", \"_\", tmp_control$SampleID)\n\ntmp_envr &lt;- tmp_envr %&gt;%\n  mutate(SampleID = paste(Ocean_code, \n                          Species_code, \n                          Tissue_code, \n                          ID, \n                          sep = \"_\"))\ntmp_shrmp &lt;- tmp_shrmp %&gt;%\n  mutate(SampleID = paste(Ocean_code, \n                          Species_code, \n                          Tissue_code, \n                          ID, \n                          sep = \"_\"))\n\nAnd combine the three modified data frames.\n\ntmp_combo &lt;- rbind(tmp_control, tmp_envr, tmp_shrmp)\ntmp_combo$Taxon &lt;- NULL\nshortcodes &lt;- tmp_combo\n\nAnd generate a modified metadata file containing the new sample ID plus the original and modified fastq file names. We have a four column tab delimited text file called fastq_info.txt that contains the original sample names, the run ID, plus the original forward and reverse fastq file names.\n\ntmp_combo[2:5] &lt;- NULL\ntmp_fastq &lt;- read_delim(here(work_here, \"fastq_info.txt\"))\n\nall_metadata &lt;- dplyr::left_join(tmp_meta, tmp_combo, by = \"Original_name\") %&gt;%\n  dplyr::left_join(., tmp_fastq, by = \"Original_name\")\n\nall_metadata &lt;- all_metadata %&gt;% dplyr::relocate(\"SampleID\", \n                                         .before = \"Original_name\")\nall_metadata &lt;- all_metadata %&gt;% \n  dplyr::relocate(\"Run\", .before = \"Plate\")\n\nall_metadata &lt;- all_metadata %&gt;%\n  mutate(Fastq_ID_forward_rename = paste0(SampleID, \"_R1.fastq.gz\")) %&gt;%\n  relocate(Fastq_ID_forward_rename, .after = Fastq_ID_forward_original) %&gt;%\n  mutate(Fastq_ID_reverse_rename = paste0(SampleID, \"_R2.fastq.gz\")) %&gt;%\n  relocate(Fastq_ID_reverse_rename, .after = Fastq_ID_reverse_original)",
    "crumbs": [
      "Home",
      "16S rRNA",
      "1. Sample Metadata"
    ]
  },
  {
    "objectID": "workflows/ssu/sampledata/index.html#modified-metadata",
    "href": "workflows/ssu/sampledata/index.html#modified-metadata",
    "title": "1. Sample Metadata",
    "section": "Modified Metadata",
    "text": "Modified Metadata\nAnd here is the modified metadata.\n\n\n\n\n\n\nNoteExpand to column descriptions for sample table\n\n\n\n\n\n\n\n\n\n\n\nHeader\nDescription\n\n\n\nSampleID\nSample ID based on Ocean, species, tissue, & unique ID\n\n\nOriginal_name\nOriginal sample ID\n\n\nOcean\nSampling ocean\n\n\nMorphospecies\nHost shrimp species\n\n\nTissue\nShrimp tissue type\n\n\nHabitat\nSampling habitat\n\n\nSite\nSampling Site 1\n\n\nSite_2\nSampling Site 2\n\n\nTaxon\nShrimp, environmental samples, or Controls\n\n\nLength\nLength of individual\n\n\nStation_no\nASK MATT\n\n\nSpecies_pair\nASK MATT\n\n\nSpecies_group\nASK MATT\n\n\nSpecies_complex\nASK MATT\n\n\nRun\nSequencing run ID\n\n\nPlate\nSequencing plate ID\n\n\nRow\nSequencing plate row number\n\n\nColumn\nSequencing plate column number\n\n\nFastq_ID_forward_original\nOriginal fastq ID (F)\n\n\nFastq_ID_forward_rename\nNew fastq ID (F)\n\n\nFastq_ID_reverse_original\nOriginal fastq ID (R)\n\n\nFastq_ID_reverse_rename\nNew fastq ID (R)\n\n\n\n\n\n\nWe sequenced a total of 1909 samples, including 1797 shrimp samples, 52 environmental samples, and 60 control samples. 884 total samples came from the Eastern Pacific and 965 from the Western Atlantic.",
    "crumbs": [
      "Home",
      "16S rRNA",
      "1. Sample Metadata"
    ]
  },
  {
    "objectID": "workflows/ssu/sampledata/index.html#rename-fastq-files",
    "href": "workflows/ssu/sampledata/index.html#rename-fastq-files",
    "title": "1. Sample Metadata",
    "section": "Rename FastQ Files",
    "text": "Rename FastQ Files\nUsing the modified metadata, specifically the new sample names, we can rename all fastq file prior to processing the data. To batch rename samples we created tab-delimited lookup tables, where the first column contains the original name of each fastq file while the second column corresponds to the new name. We did this for each sequencing run. Here is an example of the first few samples from the lookup table for run BCS_34, which we call BCS_34.rename.txt\n8937-G_R1_001.trimmed.fastq EP_A_HEBE_GL_8937_R1_001.trimmed.fastq\n8937-H_R1_001.trimmed.fastq EP_A_HEBE_HP_8937_R1_001.trimmed.fastq\n8937-M_R1_001.trimmed.fastq EP_A_HEBE_MG_8937_R1_001.trimmed.fastq\n8937-S_R1_001.trimmed.fastq EP_A_HEBE_ST_8937_R1_001.trimmed.fastq\nWe use the new metadata file and the base R command split to generate initial lookup tables for each of the six sequencing runs.\n\ntmp_split_dfs &lt;- all_metadata\ntmp_split_dfs &lt;- tmp_split_dfs %&gt;% \n  dplyr::select(Run, \n                Fastq_ID_forward_original, \n                Fastq_ID_forward_rename, \n                Fastq_ID_reverse_original, \n                Fastq_ID_reverse_rename)\nsplit_dfs &lt;- split(tmp_split_dfs, tmp_split_dfs$Run)\n\nAgain, a partial example from run BCS_34.\n\n\n\n-------------------------------------------------------------------------\n  Run       Fastq_ID_forward_original         Fastq_ID_forward_rename    \n-------- -------------------------------- -------------------------------\n BCS_34   9123-G_R1_001.trimmed.fastq.gz   EP_A_ROST_GL_9123_R1.fastq.gz \n\n BCS_34   9123-H_R1_001.trimmed.fastq.gz   EP_A_ROST_HP_9123_R1.fastq.gz \n\n BCS_34   9123-M_R1_001.trimmed.fastq.gz   EP_A_ROST_MG_9123_R1.fastq.gz \n-------------------------------------------------------------------------\n\n\nFinally, a little wrangling and then save each lookup table.\n\nfor (i in split_dfs) {\n  tmp_ds &lt;- data.frame(i)\n  tmp_name &lt;- as.character(i[1,1])\n  \n  tmp_runF &lt;- tmp_ds %&gt;% \n    dplyr::select(Run, \n                  Fastq_ID_forward_original, \n                  Fastq_ID_forward_rename)\n  tmp_runF$Run &lt;- NULL\n  tmp_runF &lt;- tmp_runF %&gt;% dplyr::rename(\"X1\" = 1, \"X2\" = 2)\n  \n  tmp_runR &lt;- tmp_ds %&gt;% \n    dplyr::select(Run,\n                  Fastq_ID_reverse_original, \n                  Fastq_ID_reverse_rename)\n  tmp_runR$Run &lt;- NULL\n  tmp_runR &lt;- tmp_runR %&gt;% dplyr::rename(\"X1\" = 1, \"X2\" = 2)\n  \n  tmp_run &lt;- rbind(tmp_runF, tmp_runR)\n  assign(tmp_name, tmp_run)  \n  write.table(\n    tmp_run, \n    paste(here(share_here, \"fastq_rename_lookup\", tmp_name), \n          \".rename.txt\", sep = \"\"), \n    sep = \"\\t\", \n    quote = FALSE, \n    row.names = FALSE, \n    col.names = FALSE)\n  rm(list = ls(pattern = \"tmp_\"))\n}\n\nOnce we have lookup tables, we can run the bash script called rename.sh. This code will take the lookup table and go through each fastq file in the run directory and assign the new name. It will also output a timestamped rename results file showing the name changes. The script is run like so:\n\nbash rename.sh /path/to/files/ rename_file.txt [--dry-run]\n\nPass the script a path to the fatsq files and a lookup table. If you just want to see what changes will be made without actually making the changes you can include the flag --dry-run. Here is an example from this dataset:\n\nbash rename.sh 01_TRIMMED_DATA/BCS_34_Istmo_S1-2_trimmed/ BCS_34.rename.txt\n\nAnd you should see something like this in the results file:\n8937-G_R1_001.trimmed.fastq.gz -&gt; EP_A_HEBE_GL_8937_R1.fastq.gz\n8937-H_R1_001.trimmed.fastq.gz -&gt; EP_A_HEBE_HP_8937_R1.fastq.gz\n8937-M_R1_001.trimmed.fastq.gz -&gt; EP_A_HEBE_MG_8937_R1.fastq.gz\n8937-S_R1_001.trimmed.fastq.gz -&gt; EP_A_HEBE_ST_8937_R1.fastq.gz\n8938-G_R1_001.trimmed.fastq.gz -&gt; EP_A_HEBE_GL_8938_R1.fastq.gz\n8938-H_R1_001.trimmed.fastq.gz -&gt; EP_A_HEBE_HP_8938_R1.fastq.gz",
    "crumbs": [
      "Home",
      "16S rRNA",
      "1. Sample Metadata"
    ]
  },
  {
    "objectID": "workflows/ssu/sampledata/index.html#save-sample-data",
    "href": "workflows/ssu/sampledata/index.html#save-sample-data",
    "title": "1. Sample Metadata",
    "section": "Save Sample Data",
    "text": "Save Sample Data\nThe last thing to do is define a sample data frame that contains only the most relevant metadata. This table will be used in all downstream analyses to define samples.\n\ntmp_shortcodes &lt;- shortcodes[order(shortcodes$SampleID), ]\ntmp_metadata &lt;- all_metadata[order(all_metadata$SampleID), ]\nidentical(tmp_shortcodes$SampleID, tmp_metadata$SampleID)\n\nsamptab &lt;- dplyr::left_join(tmp_metadata, tmp_shortcodes, \n                            by = c(\"SampleID\", \"Original_name\"))\n\n\nsample_df &lt;- samptab %&gt;%\n  dplyr::select(\n    SampleID = SampleID,\n    OCEAN    = Ocean_code,\n    SPECIES  = Species_code,\n    TISSUE   = Tissue_code,\n    ID       = ID,\n    SITE     = Site,\n    SITE2    = Site_2,\n    HABITAT  = Habitat,\n    TAXON    = Taxon,\n    PAIR     = Species_pair,\n    GROUP    = Species_group,\n    COMPLEX  = Species_complex,\n    RUN      = Run,\n    PLATE    = Plate\n  )\n\nMoving on.",
    "crumbs": [
      "Home",
      "16S rRNA",
      "1. Sample Metadata"
    ]
  },
  {
    "objectID": "workflows/ssu/med/index.html",
    "href": "workflows/ssu/med/index.html",
    "title": "5. MED Workflow",
    "section": "",
    "text": "NoteData & Scripts\n\n\n\nFastq sequence files, scripts, and other assets for running this workflow can be found on the MED Data Portal page.\nThe Data Portal page also contains a link to the curated output of this pipeline–-feature-sample count table, taxonomy table, sample metadata, & representative fasta sequences. Data is available as stand-alone text files, OR bundled in microtable and phyloseq objects. Archive also includes a table tracking sample read changes.\n\n\n\nTo run this workflow you need to install mothur and the Oligotyping/MED. You will need the tidyverse package.",
    "crumbs": [
      "Home",
      "16S rRNA",
      "5. MED Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/med/index.html#workflow-input",
    "href": "workflows/ssu/med/index.html#workflow-input",
    "title": "5. MED Workflow",
    "section": "",
    "text": "NoteData & Scripts\n\n\n\nFastq sequence files, scripts, and other assets for running this workflow can be found on the MED Data Portal page.\nThe Data Portal page also contains a link to the curated output of this pipeline–-feature-sample count table, taxonomy table, sample metadata, & representative fasta sequences. Data is available as stand-alone text files, OR bundled in microtable and phyloseq objects. Archive also includes a table tracking sample read changes.\n\n\n\nTo run this workflow you need to install mothur and the Oligotyping/MED. You will need the tidyverse package.",
    "crumbs": [
      "Home",
      "16S rRNA",
      "5. MED Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/med/index.html#overview",
    "href": "workflows/ssu/med/index.html#overview",
    "title": "5. MED Workflow",
    "section": "Overview",
    "text": "Overview\nWith the mothur pipeline finished, we can turn our attention to Minimum Entropy Decomposition (MED) (Eren et al. 2015). MED is a novel, information theory-based clustering algorithm for sensitive partitioning of high-throughput marker gene sequences.\n(From the Meren Lab website) MED:\n\nDoes not perform pairwise sequence comparison,\n\nDoes not rely on arbitrary sequence similarity thresholds,\n\nDoes not require user supervision,\n\nDoes not require preliminary classification or clustering results,\n\nIs agnostic to sampling strategy or how long your sequences are,\n\nGives you 1 nucleotide resolution over any sequencing length with computational efficiency and minimal computational heuristics.\n\nMED needs a redundant alignment file of read data. This means all identical sequences need to be included. We again use mothur but this pipeline starts with the currated output of the align.seqs portion of our mothur OTU pipeline.\nWe set up our run in the same way as the mothur pipeline.\n\n\n\n\n\nflowchart LR\n  A(Start with curated&lt;/br&gt;mothur alignment)\n  B(\"&lt;b&gt;&lt;a href='https://mothur.org/wiki/get.groups/'&gt;get.groups&lt;/a&gt;&lt;/b&gt;&lt;br/&gt;(NC Samples)\")\n  A --&gt; C(Remove&lt;/br&gt;NC Samples&lt;br/&gt;files needed by mothur &lt;/br&gt;were generated in R)\n  B --&gt; C\n  C --&gt; D(&lt;b&gt;&lt;a href='https://mothur.org/wiki/remove.seqs/'&gt;remove.seqs&lt;/a&gt;&lt;/b&gt;)\n  C --&gt; E(&lt;b&gt;&lt;a href='https://mothur.org/wiki/remove.groups/'&gt;remove.groups&lt;/a&gt;&lt;/b&gt;)\n  E --&gt; H\n  D --&gt; H(&lt;b&gt;&lt;a href='https://mothur.org/wiki/chimera.vsearch/'&gt;chimera.vsearch&lt;/a&gt;&lt;/b&gt;)\n  H --&gt; END:::hidden\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n  BEGIN:::hidden --&gt; I\n  I(&lt;b&gt;&lt;a href='https://mothur.org/wiki/classify.seqs/'&gt;classify.seqs&lt;/a&gt;&lt;/b&gt;)\n  I --&gt; J(&lt;b&gt;&lt;a href='https://mothur.org/wiki/remove.lineage/'&gt;remove.lineage&lt;/a&gt;&lt;/b&gt;) \n  J --&gt; K(to &lt;b&gt;&lt;a href='https://merenlab.org/2014/11/04/med/'&gt;MED pipeline&lt;/a&gt;&lt;/b&gt;)\n\n  K --&gt; END:::hidden",
    "crumbs": [
      "Home",
      "16S rRNA",
      "5. MED Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/med/index.html#getting-started",
    "href": "workflows/ssu/med/index.html#getting-started",
    "title": "5. MED Workflow",
    "section": "Getting Started",
    "text": "Getting Started\nThe first thing to do is copy the output of the align.seqs portion of the mothur workflow to a new working directory.\nset.dir(output=pipelineFiles_med/)\nMothur's directories:\noutputDir=pipelineFiles_med/\nsystem(cp pipelineFiles/shrimp.trim.contigs.good.unique.good.filter.unique.fasta pipelineFiles_med/)\n\nsystem(cp pipelineFiles/shrimp.trim.contigs.good.unique.good.filter.count_table pipelineFiles_med/)",
    "crumbs": [
      "Home",
      "16S rRNA",
      "5. MED Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/med/index.html#remove-negative-controls",
    "href": "workflows/ssu/med/index.html#remove-negative-controls",
    "title": "5. MED Workflow",
    "section": "Remove Negative Controls",
    "text": "Remove Negative Controls\nAs with the OTU workflow, we remove NC samples, but in this case we skip the pre.cluster step. Our goal here to remove all NC samples from the dataset, as well as any reads that are predominantly found in NC samples. More on what that means in a minute. Before we do anything we need to identify all reads present in NC samples.\nHere is what we are going to do:\n\nSubset the NC samples (and associated reads) from the fasta and count.table. To do this in mothur we need all of the NC sample names collected in an .accnos file, which is a text file used in mothur that contains a single column of names–these can be sequences, OTU labels, or sample names. This list is used to select or remove specific items from other mothur files. That way we can process subsets of the data without having to retype long lists of names.\n\nTo generate the .accnos file of NC samples we can use the shrimp.files file generated at the beginning of the mothur pipeline.\n\ntmp_accnos &lt;- readr::read_delim(here(work_here, \"nc_screen/shrimp.files\"), \n                                delim = \"\\t\", col_names = FALSE)\ntmp_accnos[, 2:3] &lt;- NULL\ntmp_accnos &lt;- tmp_accnos[grepl(\"Control_\", tmp_accnos$X1), ]\nreadr::write_delim(tmp_accnos, file = here(work_here, \"nc_screen/nc_samples.accnos\"), \n                   col_names = FALSE)\n\n\nNow we have a list of all NC sample names. The mothur command get.groups in conjunction with accnos file allows us to subset the full fasta and count_table\n\n\nget.groups(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.fasta, count=shrimp.trim.contigs.good.unique.good.filter.count_table, accnos=nc_samples.accnos)\nSelected 192842 sequences from your count file.\nSelected 34262 sequences from your fasta file.\n\nOutput File names:\nshrimp.trim.contigs.good.unique.good.filter.pick.count_table\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.fasta\n\nNext we rename the new files to something more informative (and shorter).\n\nrename.file(input=shrimp.trim.contigs.good.unique.good.filter.unique.pick.fasta, new=nc.fasta)\nrename.file(input=shrimp.trim.contigs.good.unique.good.filter.pick.count_table, new=nc.count_table)\n\nAnd a quick summary of the NC subset.\n\nsummary.seqs(fasta=nc.fasta, count=nc.count_table, processors=$PROC)\n\n\n\n\n\n\nNoteExpand to see negative control summary\n\n\n\n\n\n\n\n\nStart\nEnd\nNBases\nAmbigs\nPolymer\nNumSeqs\n\n\n\nMinimum\n1\n554\n248\n0\n3\n1\n\n\n2.5%-tile:\n1\n554\n252\n0\n4\n4822\n\n\n25%-tile:\n1\n554\n253\n0\n4\n48211\n\n\nMedian:\n1\n554\n253\n0\n5\n96422\n\n\n75%-tile:\n1\n554\n253\n0\n5\n144632\n\n\n97.5%-tile:\n1\n554\n254\n0\n6\n188021\n\n\nMaximum:\n1\n554\n254\n0\n6\n192842\n\n\nMean:\n1\n554\n253\n0\n4\n\n\n\n\n# of unique seqs:   34262\ntotal # of seqs:    192842\n\nIt took 0 secs to summarize 192842 sequences.\n\nOutput File Names:\nnc.summary\n\n\n\n\nSweet. We use the command list.seqs to get a complete list of all repseq names in the NC subset.\n\nlist.seqs(count=nc.count_table)\nOutput File Names: \nnc.accnos\nThis gives us all repseq IDs in the NC samples.\n\nWe could simply use the nc.accnos file from the list.seqs command to remove repseqs found in negative control (NC) samples from the main dataset. This seems reasonable enough. Except mothur will remove all repseqs found in a NC sample, in other words, any repseq in the accnos file. For example, let’s consider the following scenario where we have two repseqs:\n\nrepseq01 is abundant in many NC samples but not found in any other samples.repseq02 on the other hand is represented by say one read in a single NC sample but very abundant in other samples.\nIt makes sense to remove repseq01 but not necessarily repseq02. Essentially, for each repseq in the nc.accnos file we want to calculate:\n\nThe total number of reads in NC samples.\n\nThe total number of reads in non-NC samples.\n\nThe percent of reads in the NC samples.\nThe total number of NC samples containing at least 1 read.\n\nThe total number of non-NC samples containing at least 1 read.\n\nThe percent of NC samples containing reads.\n\nWhere a final data table might look something like this\n\n\n\n\n\n\n\n\n\n\n\nrepseq\nrc_nc\nrc_samps\n%in_nc\nnc_samp\nno_nc_samp\n%_in_nc_samp\n\n\n\nrepseq001\n3\n5\n37.5\n1\n2\n33.31\n\n\nrepseq002\n196\n308\n38.9\n17\n38\n30.7\n\n\nrepseq003\n3\n23\n11.1\n3\n18\n14.5\n\n\n\nTo accomplish this we will parse out relevant data from the .count_table files. We got the idea on how best to do this from a discussion on the mothur forum.\nTo save space and minimize file size, mothur formats the .count_table using a sparse format by storing only non zero sample counts. However, we need the full format which lists each repseq and its abundance counts for each sample. Using the command count.seqs in conjunction with the .count_table will return a full format table.\ncount.seqs(count=nc.count_table, compress=f)\nOutput File Names:\nnc.full.count_table\nThen we use the accnos file (nc.accnos)–containing repseqs found in NC samples, to generate a subset of the complete dataset containing only the reseqs found in NC samples.\nget.seqs(accnos=nc.accnos, fasta=shrimp.trim.contigs.good.unique.good.filter.unique.fasta, count=shrimp.trim.contigs.good.unique.good.filter.count_table)\nrename.file(input=shrimp.trim.contigs.good.unique.good.filter.unique.pick.fasta, new=subset.fasta)\nrename.file(input=shrimp.trim.contigs.good.unique.good.filter.pick.count_table, new=subset.count_table)\nAnd again run count.seqs to get a full format table of the complete dataset, which we subsetted earlier to contain only repseqs found in NC samples.\ncount.seqs(count=subset.count_table, compress=f)\nOutput File Names:\nsubset.full.count_table\nFinally we can parse out read count data from the two subset.full.count_table files.\n\nfull_count_tab &lt;- readr::read_delim(here(work_here, \"nc_screen/subset.full.count_table\"), \n                                    delim = \"\\t\", col_names = TRUE)\n# figure out which columns to use\ncontrol_cols     &lt;- grep(\"^Control_\", names(full_count_tab), value = TRUE)\nnoncontrol_cols  &lt;- setdiff(names(full_count_tab)[-(1:2)], control_cols)\n\n# now do the rowwise sums\nread_totals &lt;- full_count_tab %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    total_reads_nc   = sum(c_across(all_of(control_cols)), na.rm = TRUE),\n    total_reads_non_nc = sum(c_across(all_of(noncontrol_cols)), na.rm = TRUE)\n  ) %&gt;%\n  ungroup() %&gt;%\n  select(1, 2, total_reads_nc, total_reads_non_nc)\n\nread_totals &lt;- read_totals %&gt;% dplyr::rename(\"total_reads\" = 2)\n\nAnd here is what the new dataframe looks like. Three columns where the first is the repseq name, the second the total number of reads in NC samples, and the third the total number of reads in the entire dataset (this includes NC samples).\n\n\n\n-------------------------------------------------------------------------------------------------\n           Representative_Sequence             total_reads   total_reads_nc   total_reads_non_nc \n--------------------------------------------- ------------- ---------------- --------------------\n M06508_12_000000000-CJG44_1_1101_16846_2451      33614          19913              13701        \n\n M06508_12_000000000-CJG44_1_1101_9357_2876        571            342                229         \n\n M06508_18_000000000-CNPPR_1_1101_15534_2103     201974            2                201972       \n\n M06508_12_000000000-CJG44_1_1101_15015_3135      6446            703                5743        \n\n M06508_12_000000000-CJG44_1_1101_15072_3643      26928           2907              24021        \n\n M06508_9_000000000-JTBW3_1_1106_8860_16108      368528            2                368526       \n-------------------------------------------------------------------------------------------------\n\n\nIn total there are 34262 repseqs that were potential contaminants.\nNow we add in a column that calculates the percent of reads in the NC samples.\n\ntmp_read_totals &lt;- read_totals %&gt;%\n  dplyr::mutate(perc_reads_in_nc = 100*(\n    total_reads_nc / (total_reads_nc + total_reads_non_nc)),\n                .after = \"total_reads_non_nc\")\ntmp_read_totals$perc_reads_in_nc &lt;- \n  round(tmp_read_totals$perc_reads_in_nc, digits = 6)\n\nAnd then we calculate row sums to get the number of NC and non-NC samples containing these reads. For this we can use the subset.full.count_table we read in above.\n\ncontrol_cols     &lt;- grep(\"^Control_\", names(full_count_tab), value = TRUE)\nnoncontrol_cols  &lt;- setdiff(names(full_count_tab)[-(1:2)], control_cols)\n# rowwise tally of non-zero columns\nsamp_totals &lt;- full_count_tab %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    num_nc_samp     = sum(c_across(all_of(control_cols)) != 0, na.rm = TRUE),\n    num_non_nc_samp = sum(c_across(all_of(noncontrol_cols)) != 0, na.rm = TRUE)\n  ) %&gt;%\n  ungroup() %&gt;%\n  select(1, num_nc_samp, num_non_nc_samp)\n\nFinally add a column with the total number of samples and calculate the percent of NC samples containing these reads.\n\nsamp_totals$total_samp &lt;- samp_totals$num_nc_samp + samp_totals$num_non_nc_samp\nsamp_totals &lt;- samp_totals %&gt;%  \n  dplyr::relocate(\"total_samp\", .after = \"Representative_Sequence\")\nsamp_totals &lt;- samp_totals %&gt;%\n  dplyr::mutate(perc_nc_samp = \n                  100*( num_nc_samp / (num_nc_samp + num_non_nc_samp)),\n                  .after = \"num_non_nc_samp\")\n\nAfter all of this wrangling here is a snippet of the table showing the distribution of NC reads across samples.\nNow we remove any repseqs where:\n\nThe number of reads found in NC samples accounted for more than 10% of total reads OR\nThe percent of NC samples containing the repseq was greater than 10% of total samples.\n\n\nnc_remove &lt;- nc_check %&gt;% \n  filter(perc_reads_in_nc &gt; 10 | perc_nc_samp &gt; 10)\n\n\n\n\n\n\n\n\n\n\n\nTotal rep seqs\nNC reads\nnon NC reads\n% NC reads\n\n\n\nRemoved\n32438\n166614\n212292\n43.972\n\n\nRetained\n1824\n26228\n10280853\n0.254\n\n\n\nWe identified a total of 34262 representative sequences (repseqs) that were present in at least 1 NC sample by at least 1 read. We removed any repseq where more than 10% of total reads were found in NC samples OR any repseq found in more than 10% of NC samples. Based on these criteria we removed 32438 repseqs from the data set, which accounted for 166614 total reads in NC samples and 212292 total reads in non-NC samples. Of the total reads removed 43.972% came from NC samples. Of all repseqs identified in NC samples, 1824 were retained because they fell below the threshold criteria. These repseqs accounted for 26228 reads in NC samples and 10280853 reads in non-NC samples. NC samples accounted for 0.254% of these reads.\nOK, now we can create a new neg_control.accnos containing only repseqs abundant in NC samples.\n\nwrite_delim(\n  data.frame(nc_remove$Representative_Sequence), \n  here(work_here, \"nc_screen/nc_repseq_remove.accnos\"), \n  col_names = FALSE)\n\nAnd then use this file in conjunction with the mothur command remove.seqs.\nremove.seqs(accnos=nc_repseq_remove.accnos, fasta=shrimp.trim.contigs.good.unique.good.filter.unique.fasta, count=shrimp.trim.contigs.good.unique.good.filter.count_table)\nRemoved 32438 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.fasta.\nRemoved 378906 sequences from shrimp.trim.contigs.good.unique.good.filter.count_table.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.fasta\nshrimp.trim.contigs.good.unique.good.filter.pick.count_table\ncount.groups(count=shrimp.trim.contigs.good.unique.good.filter.pick.count_table)\nSize of smallest group: 1.\n\nTotal seqs: 35944156.\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.good.filter.pick.count.summary\nBefore we remove the NC samples we need to check whether some NC samples were already removed. When mothur runs the remove.seqs command it will automatically remove any samples where the read count has fallen to zero. If mothur did remove samples and we try to remove all NC samples, we will get an error. To check we can compare the count.summary files before and after the previous remove.seqs command.\n\ntmp_before &lt;- read_tsv(\n  here(work_here, \"nc_screen/shrimp.trim.contigs.good.unique.good.filter.count.summary\"),\n  col_names = FALSE,\n  col_select = 1\n)\n\ntmp_after &lt;- read_tsv(\n  here(work_here, \"nc_screen/shrimp.trim.contigs.good.unique.good.filter.pick.count.summary\"),\n  col_names = FALSE,\n  col_select = 1\n)\ntmp_nc_lost &lt;- anti_join(tmp_before, tmp_after)\ntmp_nc_lost$X1\n\nThese are the samples that were removed when we ran remove.seqs. We need to eliminate these sample IDs from our list of NC samples to remove or mothur will throw an error.\n[1] \"Control_15\" \"Control_18\" \"Control_21\" \"Control_5\"  \nAs before, we can generate a list of NC samples to use in conjunction with the remove.groups command to eliminate all NC samples.\n\nnc_to_remove &lt;- semi_join(tmp_before, tmp_after)\nnc_to_remove &lt;- nc_to_remove %&gt;%\n  dplyr::filter(\n    stringr::str_starts(X1, \"Control\")\n    )\nreadr::write_delim(nc_to_remove, \n                   file = here(work_here, \"nc_screen/nc_samples_remove.accnos\"), \n                   col_names = FALSE)\n\nIn total the following mothur command should remove 56 negative control samples.\nremove.groups(count=shrimp.trim.contigs.good.unique.good.filter.pick.count_table, fasta=shrimp.trim.contigs.good.unique.good.filter.unique.pick.fasta, accnos=nc_samples_remove.accnos)\nRemoved 26228 sequences from your count file.\nRemoved 0 sequences from your fasta file.\n\nOutput File names: \nshrimp.trim.contigs.good.unique.good.filter.pick.pick.count_table\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.fasta\nsummary.seqs(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.fasta, count=shrimp.trim.contigs.good.unique.good.filter.pick.pick.count_table, processors=30)\n\n\n\n\n\n\nNoteExpand to see data set summary\n\n\n\n\n\n\n\n\nStart\nEnd\nNBases\nAmbigs\nPolymer\nNumSeqs\n\n\n\nMinimum\n1\n554\n208\n0\n3\n1\n\n\n2.5%-tile:\n1\n554\n252\n0\n3\n897949\n\n\n25%-tile:\n1\n554\n253\n0\n4\n8979483\n\n\nMedian:\n1\n554\n253\n0\n4\n17958965\n\n\n75%-tile:\n1\n554\n253\n0\n5\n26938447\n\n\n97.5%-tile:\n1\n554\n253\n0\n6\n35019980\n\n\nMaximum:\n1\n554\n254\n0\n6\n35917928\n\n\nMean:\n1\n554\n252\n0\n4\n\n\n\n\n# of unique seqs:   4146230\ntotal # of seqs:    35917928\n\nIt took 74 secs to summarize 35917928 sequences.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.summary\n\n\n\ncount.groups(count=shrimp.trim.contigs.good.unique.good.filter.pick.pick.count_table)\nSize of smallest group: 49.\n\nTotal seqs: 35917928.\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.good.filter.pick.pick.count.summary",
    "crumbs": [
      "Home",
      "16S rRNA",
      "5. MED Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/med/index.html#remove-chimeras",
    "href": "workflows/ssu/med/index.html#remove-chimeras",
    "title": "5. MED Workflow",
    "section": "Remove Chimeras",
    "text": "Remove Chimeras\nchimera.vsearch(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.fasta, count=shrimp.trim.contigs.good.unique.good.filter.pick.pick.count_table, dereplicate=t, processors=30)\nUsing vsearch version v2.30.0.\nChecking sequences from shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.fasta ...\n...\n\n/******************************************/\nSplitting by sample: \n\n...\n\nRemoving chimeras from your input files:\n/******************************************/\nRunning command: remove.seqs(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.fasta,\naccnos=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.accnos)\nRemoved 710630 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.fasta.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.pick.fasta\n\n/******************************************/\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.count_table\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.chimeras\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.accnos\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.fasta\n\n/******************************************/\nsummary.seqs(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.fasta, count=shrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.count_table, processors=30)\n\n\n\n\n\n\nNoteExpand to see data set summary\n\n\n\n\n\n\n\n\nStart\nEnd\nNBases\nAmbigs\nPolymer\nNumSeqs\n\n\n\nMinimum\n1\n554\n208\n0\n3\n1\n\n\n2.5%-tile:\n1\n554\n252\n0\n3\n865884\n\n\n25%-tile:\n1\n554\n253\n0\n4\n8658836\n\n\nMedian:\n1\n554\n253\n0\n4\n17317672\n\n\n75%-tile:\n1\n554\n253\n0\n5\n25976508\n\n\n97.5%-tile:\n1\n554\n253\n0\n6\n33769460\n\n\nMaximum:\n1\n554\n254\n0\n6\n34635343\n\n\nMean:\n1\n554\n252\n0\n4\n\n\n\n\n# of unique seqs:   3435600\ntotal # of seqs:    34635343\n\nIt took 64 secs to summarize 34635343 sequences.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.summary\n\n/******************************************/\n\n\n\ncount.groups(count=shrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.count_table)\nSize of smallest group: 49.\n\nTotal seqs: 34635343.\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.count.summary\n\n/******************************************/",
    "crumbs": [
      "Home",
      "16S rRNA",
      "5. MED Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/med/index.html#repseq-taxonomy",
    "href": "workflows/ssu/med/index.html#repseq-taxonomy",
    "title": "5. MED Workflow",
    "section": "Repseq Taxonomy",
    "text": "Repseq Taxonomy\nThe classify.seqs command requires properly formatted reference and taxonomy databases. For taxonomic assignment, we are using the GSR database (Molano, Vega-Abellaneda, and Manichanh 2024). The developers of mothur maintain formatted versions of popular databases, however the GSR-DB has not been formatted by the developers yet.\n\n\n\n\n\n\nNote\n\n\n\nYou can visit the GSR database download page to find a database suitable to your data.\n\n\nTo create a mothur formatted version GSR-DB1, we perform the following steps.\nDownload a data base\nHere we are using the GSR V4 database.\n\nwget https://manichanh.vhir.org/gsrdb/GSR-DB_V4_cluster-1.tar.gz\ntar -xvzf GSR-DB_V4_cluster-1.tar.gz\n\nFirst (in the command line) we remove first line of the taxonomy file.\n\ncp GSR-DB_V4_cluster-1_taxa.txt tmp0.txt\nsed '1d' tmp0.txt &gt; tmp1.txt\n\nNext, delete species and remove leading [a-z]__ from taxa names\n\nsed -E 's/s__.*//g' tmp1.txt &gt; tmp2.txt\nsed -E 's/[a-zA-Z]__//g' tmp2.txt &gt; gsrdb.tax\ncp GSR-DB_V4_cluster-1_seqs.fasta gsrdb.fasta\nrm tmp*\n\nclassify.seqs(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.fasta, count=shrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.count_table, reference=reference_dbs/gsrdb.fasta, taxonomy=reference_dbs/gsrdb.tax, processors=30)\nReading template taxonomy...     DONE.\nReading template probabilities...     DONE.\nIt took 4 seconds get probabilities.\nClassifying sequences from \nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.fasta ...\n[WARNING]: M06508_12_000000000-CJG44_1_2103_6654_25682 could not be classified. \nYou can use the remove.lineage command with taxon=unknown; to remove such sequences.\n\n...\n\nIt took 839 secs to classify 3435600 sequences.\n\nIt took 1697 secs to create the summary file for 3435600 sequences.\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.gsrdb.wang.taxonomy\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.gsrdb.wang.tax.summary\n\n/******************************************/",
    "crumbs": [
      "Home",
      "16S rRNA",
      "5. MED Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/med/index.html#remove-contaminants",
    "href": "workflows/ssu/med/index.html#remove-contaminants",
    "title": "5. MED Workflow",
    "section": "Remove Contaminants",
    "text": "Remove Contaminants\nremove.lineage(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.fasta, count=shrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.count_table, taxonomy=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.gsrdb.wang.taxonomy, taxon=Chloroplast-Mitochondria-unknown-Eukaryota)\nRunning command: \nremove.seqs(accnos=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.gsrdb.wang.accnos, count=shrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.count_table, fasta=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.fasta)\nRemoved 2160 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.fasta.\nRemoved 7262 sequences from shrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.count_table.\n\n/******************************************/\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.gsrdb.wang.pick.taxonomy\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.gsrdb.wang.accnos\nshrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.pick.count_table\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.pick.fasta\n\n/******************************************/\nsummary.seqs(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.pick.fasta, count=shrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.pick.count_table, processors=30)\n\n\n\n\n\n\nNoteExpand to see data set summary\n\n\n\n\n\n\n\n\nStart\nEnd\nNBases\nAmbigs\nPolymer\nNumSeqs\n\n\n\nMinimum\n1\n554\n208\n0\n3\n1\n\n\n2.5%-tile:\n1\n554\n252\n0\n3\n865703\n\n\n25%-tile:\n1\n554\n253\n0\n4\n8657021\n\n\nMedian:\n1\n554\n253\n0\n4\n17314041\n\n\n75%-tile:\n1\n554\n253\n0\n5\n25971061\n\n\n97.5%-tile:\n1\n554\n253\n0\n6\n33762379\n\n\nMaximum:\n1\n554\n254\n0\n6\n34628081\n\n\nMean:\n1\n554\n252\n0\n4\n\n\n\n\n# of unique seqs:   3433440\ntotal # of seqs:    34628081\n\nIt took 62 secs to summarize 34628081 sequences.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.pick.summary\n\n/******************************************/\n\n\n\nsummary.tax(taxonomy=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.gsrdb.wang.pick.taxonomy, count=shrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.pick.count_table)\nUsing shrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.pick.count_table as input file for the count parameter.\nUsing shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.gsrdb.wang.pick.taxonomy as input file for the taxonomy parameter.\nIt took 1580 secs to create the summary file for 34628081 sequences.\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.gsrdb.wang.pick.tax.summary\n\n/******************************************/\ncount.groups(count=shrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.pick.count_table)\nSize of smallest group: 49.\n\nTotal seqs: 34628081.\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.pick.count.summary\n\n/******************************************/",
    "crumbs": [
      "Home",
      "16S rRNA",
      "5. MED Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/med/index.html#track-reads-through-workflow",
    "href": "workflows/ssu/med/index.html#track-reads-through-workflow",
    "title": "5. MED Workflow",
    "section": "Track Reads through Workflow",
    "text": "Track Reads through Workflow\nAt this point we can look at the number of reads that made it through each step of the workflow for every sample.\n\nread_change &lt;- read_tsv(\n  here(work_here, \"mothur_med_pipeline_read_changes.txt\"),\n  col_names = TRUE\n)",
    "crumbs": [
      "Home",
      "16S rRNA",
      "5. MED Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/med/index.html#preparing-for-analysis",
    "href": "workflows/ssu/med/index.html#preparing-for-analysis",
    "title": "5. MED Workflow",
    "section": "Preparing for analysis",
    "text": "Preparing for analysis\nrename.file(fasta=current, count=current, taxonomy=current, prefix=final_med)",
    "crumbs": [
      "Home",
      "16S rRNA",
      "5. MED Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/med/index.html#assign-taxonomy",
    "href": "workflows/ssu/med/index.html#assign-taxonomy",
    "title": "5. MED Workflow",
    "section": "Assign Taxonomy",
    "text": "Assign Taxonomy\nOur first step is to classify the node representatives from the MED output. The classify.seqs command requires properly formatted reference and taxonomy databases. For taxonomic assignment, we are using the GSR database (Molano, Vega-Abellaneda, and Manichanh 2024). The developers of mothur maintain formatted versions of popular databases, however the GSR-DB has not been formatted by the developers yet.\n\n\n\n\n\n\nNote\n\n\n\nHere you can download an appropriate version of the GSR database.\n\n\nTo create a mothur formatted version GSR-DB, we perform the following steps (we went through this process above prior to removing contaminants but will repeat here for posterity).\nDownload a data base\nHere we are using the GSR V4 database.\n\nwget https://manichanh.vhir.org/gsrdb/GSR-DB_V4_cluster-1.tar.gz\ntar -xvzf GSR-DB_V4_cluster-1.tar.gz\n\nFirst (in the command line) we remove first line of the taxonomy file.\n\ncp GSR-DB_V4_cluster-1_taxa.txt tmp0.txt\nsed '1d' tmp0.txt &gt; tmp1.txt\n\nNext, delete species and remove leading [a-z]__ from taxa names\n\nsed -E 's/s__.*//g' tmp1.txt &gt; tmp2.txt\nsed -E 's/[a-zA-Z]__//g' tmp2.txt &gt; gsrdb.tax\ncp GSR-DB_V4_cluster-1_seqs.fasta gsrdb.fasta\n\nThe next thing we need to do is grab the node-representatives.fa.txt from the MED output so that we can classify these sequences. Of course, proper formatting is required.\n\nseqkit replace --pattern ^ --replacement MED node-representatives.fa.txt &gt; tmp1.fa\nseqkit replace --pattern \"\\|.*\" --replacement '' tmp1.fa &gt; med_nodes.fasta\nrm tmp1.fa\n\nGreat, the reference database is formatted. Now we need to make a few files that mimics the normal mothur output files because the MED pipeline does not exactly generate the files we need to create a microtable object. First we use the matrix_counts.txt file from the MED analysis to create a mothur-styled count.table.\n\ntmp_med_counts &lt;- read_tsv(\n  here(work_here, \"med_results/matrix_counts.txt\"),\n    col_names = TRUE)\n\ntmp_med_counts &lt;- tmp_med_counts %&gt;% \n  dplyr::rename_with( ~ paste0(\"MED\", .x)) \n\ntmp_med_counts &lt;- tmp_med_counts %&gt;%\n  tidyr::pivot_longer(cols = c(-1), names_to = \"tmp\") %&gt;%\n  tidyr::pivot_wider(names_from = c(1))\n\ntmp_med_counts &lt;- tibble::column_to_rownames(tmp_med_counts, \"tmp\")\n\ntmp_med_counts &lt;- tmp_med_counts %&gt;%\n                  mutate(total = rowSums(.), .before = 1)\n\ntmp_med_counts &lt;- tmp_med_counts %&gt;% \n     tibble::rownames_to_column(\"Representative_Sequence\")\nmed_counts &lt;- tmp_med_counts\n\nNow we can actually classify the representative sequences.\nclassify.seqs(fasta=med_nodes.fasta, count=med_nodes.count.table, reference=reference_dbs/gsrdb.fasta, taxonomy=reference_dbs/gsrdb.tax)\nNow we make a mothur styled shared file.\n\ntmp_med_counts &lt;- read_tsv(\n  here(work_here, \"med_results/matrix_counts.txt\"),\n    col_names = TRUE)\n\ntmp_n_meds &lt;- ncol(tmp_med_counts) - 1\ntmp_med_counts &lt;- tmp_med_counts %&gt;% \n  dplyr::rename_with( ~ paste0(\"MED\", .x)) %&gt;% \n  dplyr::rename(\"Group\" = \"MEDsamples\")\n\ntmp_med_counts &lt;- tmp_med_counts %&gt;% \n  tibble::add_column(label = 0.03, .before = \"Group\") %&gt;% \n  tibble::add_column(numOtus = tmp_n_meds, .after = \"Group\")\n\nmed_shared &lt;- tmp_med_counts\n\nA. Taxonomy Table\nHere is what the taxonomy table looks like in the microeco mock data.\n\n\n\n---------------------------------------------------------------------\n  &nbsp;      Kingdom           Phylum                 Class         \n---------- ------------- -------------------- -----------------------\n  OTU_50    k__Bacteria   p__Proteobacteria    c__Betaproteobacteria \n\n OTU_8058   k__Bacteria   p__Actinobacteria      c__Actinobacteria   \n\n OTU_7152   k__Bacteria   p__Verrucomicrobia    c__OPB35 soil group  \n---------------------------------------------------------------------\n\n\nOur taxonomy file (below) needs a little wrangling to be properly formatted.\n\ntmp_tax &lt;- read_delim(\n             here(work_here, \"node_taxonomy/med_nodes.cons.taxonomy\"),\n             delim = \"\\t\")\n\n\n\n\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n     OTU         Size                                                                                Taxonomy                                                                              \n-------------- -------- -------------------------------------------------------------------------------------------------------------------------------------------------------------------\n MED000013674   347929                     Bacteria(100);Proteobacteria(100);Gammaproteobacteria(100);Vibrionales(100);Vibrionaceae(100);Vibrionaceae_unclassified(100);                   \n\n MED000009391   314434         Bacteria(100);Bacteria_unclassified(100);Bacteria_unclassified(100);Bacteria_unclassified(100);Bacteria_unclassified(100);Bacteria_unclassified(100);       \n\n MED000010996   305834   Bacteria(100);Proteobacteria(90);Proteobacteria_unclassified(90);Proteobacteria_unclassified(90);Proteobacteria_unclassified(90);Proteobacteria_unclassified(90); \n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n\nSome fancy string manipulation…\n\ntmp_tax &lt;- data.frame(sapply(tmp_tax, \n                             gsub, \n                             pattern = \"\\\\(\\\\d+\\\\)\", \n                             replacement = \"\"))\ntmp_tax &lt;- data.frame(sapply(tmp_tax, \n                             gsub, \n                             pattern = \";$\", \n                             replacement = \"\"))\ntmp_tax &lt;- separate_wider_delim(tmp_tax, \n                              cols = Taxonomy, \n                              delim = \";\", names = c(\n                                \"Kingdom\", \"Phylum\", \n                                \"Class\", \"Order\", \n                                \"Family\", \"Genus\"))\ntmp_tax &lt;- data.frame(sapply(tmp_tax, gsub, \n                           pattern = \"^.*_unclassified$\", \n                           replacement = \"\"))\ntmp_tax$Size &lt;- NULL\ntmp_tax &lt;- tibble::column_to_rownames(tmp_tax, \"OTU\")\n\nAnd we get this …\n\n\n\n------------------------------------------------------------------------------\n    &nbsp;      Kingdom        Phylum              Class             Order    \n-------------- ---------- ---------------- --------------------- -------------\n MED000013674   Bacteria   Proteobacteria   Gammaproteobacteria   Vibrionales \n\n MED000009391   Bacteria                                                      \n\n MED000010996   Bacteria   Proteobacteria                                     \n\n MED000011677   Bacteria   Proteobacteria   Gammaproteobacteria               \n------------------------------------------------------------------------------\n\n\n\nrank_prefixes &lt;- c(\n  Kingdom = \"k\", \n  Phylum  = \"p\", \n  Class   = \"c\", \n  Order   = \"o\", \n  Family  = \"f\", \n  Genus   = \"g\"\n)\n\ntmp_tax &lt;- tmp_tax %&gt;%\n  mutate(across(everything(), ~replace_na(., \"\"))) %&gt;% \n  mutate(across(names(rank_prefixes), \n                ~ paste0(rank_prefixes[cur_column()], \"__\", .))) %&gt;%\ntidy_taxonomy()\n\nAnd then this. Exactly like the mock data.\n\n\n\n------------------------------------------------------------------------------------------\n    &nbsp;        Kingdom          Phylum                 Class                Order      \n-------------- ------------- ------------------- ------------------------ ----------------\n MED000013674   k__Bacteria   p__Proteobacteria   c__Gammaproteobacteria   o__Vibrionales \n\n MED000009391   k__Bacteria          p__                   c__                  o__       \n\n MED000010996   k__Bacteria   p__Proteobacteria            c__                  o__       \n\n MED000011677   k__Bacteria   p__Proteobacteria   c__Gammaproteobacteria        o__       \n------------------------------------------------------------------------------------------\n\n\nB. Sequence Table\nHere is what the sequence table looks like in the mock data.\n\n\n\n--------------------------------------------------------------------\n  &nbsp;    S1   S2   S3   S4   S5   S6   S7   S9   S10   S11   S12 \n---------- ---- ---- ---- ---- ---- ---- ---- ---- ----- ----- -----\n OTU_4272   1    0    1    1    0    0    1    1     0     1     1  \n\n OTU_236    1    4    0    2    35   5    94   0    177   14    27  \n\n OTU_399    9    2    2    4    4    0    3    6     0     1     2  \n--------------------------------------------------------------------\n\n\nThese code block will return a properly formatted sequence table.\n\ntmp_st &lt;- readr::read_delim(\n  here(work_here, \"node_taxonomy/med_nodes.shared\"),\n  delim = \"\\t\")\n\n\ntmp_st$numOtus &lt;- NULL\ntmp_st$label &lt;- NULL\ntmp_st &lt;- tmp_st %&gt;%\n  tidyr::pivot_longer(cols = c(-1), names_to = \"tmp\") %&gt;%\n  tidyr::pivot_wider(names_from = c(1))\n\ntmp_st &lt;- tibble::column_to_rownames(tmp_st, \"tmp\")\n\n\n\n\n--------------------------------------------------------------------------\n    &nbsp;      EP_A_AREN_EG_8651   EP_A_AREN_EG_8654   EP_A_AREN_EG_8698 \n-------------- ------------------- ------------------- -------------------\n MED000011539           0                   4                   1         \n\n MED000013720           7                  410                 25         \n\n MED000009147           0                   0                   0         \n\n MED000009218           0                   0                   0         \n--------------------------------------------------------------------------\n\n\nC. Sample Table\nHere is what the sample table looks like in the mock data.\n\n\n\n----------------------------------------------------\n &nbsp;   SampleID   Group   Type       Saline      \n-------- ---------- ------- ------ -----------------\n   S1        S1       IW      NE    Non-saline soil \n\n   S2        S2       IW      NE    Non-saline soil \n\n   S3        S3       IW      NE    Non-saline soil \n----------------------------------------------------\n\n\nNo problem.\n\nsamdf &lt;- read.table(\n  here(\"working_files/ssu/sampledata\", \"sample_data.txt\"),\n  header = TRUE, sep = \"\\t\")\n\nsamdf &lt;- samdf %&gt;% tibble::column_to_rownames(\"SampleID\")\nsamdf$SampleID &lt;- rownames(samdf)\nsamdf &lt;- samdf %&gt;% relocate(SampleID)\n\nsamdf &lt;- samdf %&gt;%\n  dplyr::filter(\n    stringr::str_starts(SampleID, \"Control\", negate = TRUE))\n\n\n\n\n-------------------------------------------------------------------------\n      &nbsp;             SampleID        OCEAN   SPECIES   TISSUE    ID  \n------------------- ------------------- ------- --------- -------- ------\n EP_A_AREN_GL_8625   EP_A_AREN_GL_8625    EP     A_AREN      GL     8625 \n\n EP_A_AREN_GL_8626   EP_A_AREN_GL_8626    EP     A_AREN      GL     8626 \n\n EP_A_AREN_GL_8627   EP_A_AREN_GL_8627    EP     A_AREN      GL     8627 \n-------------------------------------------------------------------------",
    "crumbs": [
      "Home",
      "16S rRNA",
      "5. MED Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/med/index.html#experiment-level-objects",
    "href": "workflows/ssu/med/index.html#experiment-level-objects",
    "title": "5. MED Workflow",
    "section": "Experiment-level Objects",
    "text": "Experiment-level Objects\nIn the following section we create microtable and phyloseq objects. These are single, consolidated experiment-level data objects that organize various data types, such as OTU tables (taxa abundance), sample metadata, taxonomic classifications, as well as phylogenetic trees and reference sequences, into one structured format. This unified approach facilitates reproducible, interactive analysis and visualization by bringing all related information together.\nWe begin by creating the microtable and then use the function meco2phyloseq from the file2meco package to create the phyloseq object. This way all of the underlying data is identical across the two objects.\n\n\n\n\n\n\nNote\n\n\n\nThese objects contain an OTU table (taxa abundances), sample metadata, and taxonomy table (mapping between OTUs and higher-level taxonomic classifications).\n\n\n\nsample_info &lt;- samdf\ntax_tab &lt;- tmp_tax\notu_tab &lt;- tmp_st\n\n\ntmp_me &lt;- microtable$new(sample_table = sample_info, \n                         otu_table = otu_tab, \n                         tax_table = tax_tab)\ntmp_me\n\nmicrotable-class object:\nsample_table have 1849 rows and 14 columns\notu_table have 436 rows and 1849 columns\ntax_table have 436 rows and 6 columns\nAdd Representative Sequence\nThe fasta file returned by MED needs a little T.L.C. For that we use a tool called SeqKit (Shen, Sipos, and Zhao 2024) for fasta defline manipulation.\n\nseqkit replace -p ^ -r MED node-representatives.fa.txt &gt; tmp1.fa\nseqkit replace -p \"\\|.*\" -r '' tmp1.fa &gt; tmp2.fa \nseqkit replace -p \"-\" -r '$1' -s -w 0 tmp2.fa &gt; med_rep.fasta\nrm tmp1.fa\nrm tmp2.fa\n\n\nrep_fasta &lt;- Biostrings::readDNAStringSet(here(work_here, \"med_rep.fasta\"))\n\n\ntmp_me$rep_fasta &lt;- rep_fasta\ntmp_me$tidy_dataset()\ntmp_me\nme_raw &lt;- microeco::clone(tmp_me)",
    "crumbs": [
      "Home",
      "16S rRNA",
      "5. MED Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/med/index.html#curate-the-data-set",
    "href": "workflows/ssu/med/index.html#curate-the-data-set",
    "title": "5. MED Workflow",
    "section": "Curate the Data Set",
    "text": "Curate the Data Set\nPretty much the last thing to do is remove low-count samples.\nRemove Low-Count Samples\n\nthreshold &lt;- 1000\ntmp_no_low &lt;- microeco::clone(me_raw)\ntmp_no_low$otu_table &lt;- me_raw$otu_table %&gt;%\n          dplyr::select(where(~ is.numeric(.) && sum(.) &gt;= threshold))\ntmp_no_low$tidy_dataset()\ntmp_no_low\n\n\n\nmicrotable-class object:\nsample_table have 1799 rows and 14 columns\notu_table have 436 rows and 1799 columns\ntax_table have 436 rows and 6 columns\nrep_fasta have 436 sequences\n\n\n\nme_final &lt;- microeco::clone(tmp_no_low)\n\nLastly, we can use the package file2meco to generate a phyloseq object.\n\nps_final &lt;- file2meco::meco2phyloseq(me_final)",
    "crumbs": [
      "Home",
      "16S rRNA",
      "5. MED Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/med/index.html#summary",
    "href": "workflows/ssu/med/index.html#summary",
    "title": "5. MED Workflow",
    "section": "Summary",
    "text": "Summary\nNow time to summarize the data. For this we use the R package miaverse (Borman et al. 2024).\n\n\n\n\nme_dataset\ntotal_asvs\ntotal_reads\ntotal_samples\n\n\n\noriginal\n436\n24817013\n1849\n\n\nno low count samps\n436\n24785483\n1799\n\n\n\n\n\n\n\n\nDataset metrics before and after curation.\n\nMetric\nStart\nEnd\n\n\n\nMin. no. of reads\n1\n1010\n\n\nMax. no. of reads\n185675\n185675\n\n\nTotal no. of reads\n24817013\n24785483\n\n\nAvg. no. of reads\n13422\n13777\n\n\nMedian no. of reads\n10360\n10566\n\n\nTotal ASVs\n436\n436\n\n\nNo. of singleton ASVs\nNA\nNA\n\n\n% of singleton ASVs\nNA\nNA\n\n\nSparsity\n0.846\n0.845\n\n\n\n\n\nWe started off with 436 ASVs and 1849 samples. After removing low-count samples, there were 436 ASVs and 1799 samples remaining.\nWe lost a total of 110 samples after curating the dataset. This includes 60 NC samples and 50 non-NC samples.\nHere is a list of non-NC samples that were removed.\n\n\n [1] \"EP_A_HEBE_ST_8937\" \"EP_A_UMBO_EG_9126\" \"EP_A_UTRI_MG_9071\"\n [4] \"EP_A_UTRI_ST_9071\" \"EP_E_SAMP_MD_9290\" \"EP_E_SAMP_MD_9293\"\n [7] \"EP_E_SAMP_SD_9285\" \"EP_E_SAMP_SD_9286\" \"EP_E_SAMP_SD_sed\" \n[10] \"WA_A_BAHA_GL_7474\" \"WA_A_BOUV_GL_7580\" \"WA_A_BOUV_ST_7585\"\n[13] \"WA_A_CRIS_GL_9624\" \"WA_A_CRIS_HP_7536\" \"WA_A_CRIS_ST_7539\"\n[16] \"WA_A_CRIS_ST_9471\" \"WA_A_FLOR_EG_7685\" \"WA_A_FLOR_EG_9662\"\n[19] \"WA_A_FLOR_GL_9659\" \"WA_A_FLOR_HP_9659\" \"WA_A_FLOR_ST_9655\"\n[22] \"WA_A_FORM_EG_7457\" \"WA_A_FORM_MG_7752\" \"WA_A_NUTT_EG_9410\"\n[25] \"WA_A_NUTT_GL_9405\" \"WA_A_NUTT_ST_7430\" \"WA_A_NUTT_ST_9405\"\n[28] \"WA_A_PARA_GL_9474\" \"WA_A_PARA_HP_9364\" \"WA_A_PARA_MG_9364\"\n[31] \"WA_A_PARA_ST_7460\" \"WA_A_PARA_ST_7708\" \"WA_A_PARA_ST_9477\"\n[34] \"WA_A_PCNS_HP_9513\" \"WA_A_PCNS_MG_9513\" \"WA_A_PCWS_EG_9438\"\n[37] \"WA_A_PCWS_ST_9438\" \"WA_A_THOM_HP_7392\" \"WA_A_THOM_ST_7392\"\n[40] \"WA_A_THOM_ST_9385\" \"WA_A_THOM_ST_9389\" \"WA_A_VERR_EG_9501\"\n[43] \"WA_A_VERR_MG_9501\" \"WA_A_WEBS_HP_7727\" \"WA_A_WEBS_HP_9411\"\n[46] \"WA_A_WEBS_MG_7612\" \"WA_A_WEBS_ST_9411\" \"WA_E_SAMP_RB_7531\"\n[49] \"WA_E_SAMP_RB_7533\" \"WA_E_SAMP_RB_7535\"",
    "crumbs": [
      "Home",
      "16S rRNA",
      "5. MED Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/med/index.html#footnotes",
    "href": "workflows/ssu/med/index.html#footnotes",
    "title": "5. MED Workflow",
    "section": "Footnotes",
    "text": "Footnotes\n\nFrom the developers: GSR database (Greengenes, SILVA, and RDP database) is an integrated and manually curated database for bacterial and archaeal 16S amplicon taxonomy analysis. Unlike previous integration approaches, this database creation pipeline includes a taxonomy unification step to ensure consistency in taxonomical annotations. The database was validated with three mock communities and two real datasets and compared with existing 16S databases such as Greengenes, GTDB, ITGDB, SILVA, RDP, and MetaSquare. Results showed that the GSR database enhances taxonomical annotations of 16S sequences, outperforming current 16S databases at the species level. The GSR database is available for full-length 16S sequences and the most commonly used hypervariable regions: V4, V1-V3, V3-V4, and V3-V5.↩︎",
    "crumbs": [
      "Home",
      "16S rRNA",
      "5. MED Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/dada2/index.html",
    "href": "workflows/ssu/dada2/index.html",
    "title": "2. DADA2 ASV Workflow",
    "section": "",
    "text": "NoteData & Scripts\n\n\n\nFastq sequence files, scripts, and other assets for running this workflow can be found on the Dada2 Data Portal page.\nThe Data Portal page also contains a link to the curated output of this pipeline–-feature-sample count table, taxonomy table, sample metadata, & representative fasta sequences. Data is available as stand-alone text files, OR bundled in microtable and phyloseq objects. Archive also includes a table tracking sample read changes.\n\n\n\nThere are several R packages you need to run this workflow.\n\nClick here for workflow library information.#!/usr/bin/env Rscript\nset.seed(919191)\npacman::p_load(tidyverse, gridExtra, grid, phyloseq,\n               formatR, gdata, ff, decontam, dada2, \n               ShortRead, Biostrings, DECIPHER, \n               install = FALSE, update = FALSE)",
    "crumbs": [
      "Home",
      "16S rRNA",
      "2. DADA2 ASV Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/dada2/index.html#workflow-input",
    "href": "workflows/ssu/dada2/index.html#workflow-input",
    "title": "2. DADA2 ASV Workflow",
    "section": "",
    "text": "NoteData & Scripts\n\n\n\nFastq sequence files, scripts, and other assets for running this workflow can be found on the Dada2 Data Portal page.\nThe Data Portal page also contains a link to the curated output of this pipeline–-feature-sample count table, taxonomy table, sample metadata, & representative fasta sequences. Data is available as stand-alone text files, OR bundled in microtable and phyloseq objects. Archive also includes a table tracking sample read changes.\n\n\n\nThere are several R packages you need to run this workflow.\n\nClick here for workflow library information.#!/usr/bin/env Rscript\nset.seed(919191)\npacman::p_load(tidyverse, gridExtra, grid, phyloseq,\n               formatR, gdata, ff, decontam, dada2, \n               ShortRead, Biostrings, DECIPHER, \n               install = FALSE, update = FALSE)",
    "crumbs": [
      "Home",
      "16S rRNA",
      "2. DADA2 ASV Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/dada2/index.html#overview",
    "href": "workflows/ssu/dada2/index.html#overview",
    "title": "2. DADA2 ASV Workflow",
    "section": "Overview",
    "text": "Overview\nThis workflow contains the code we used to process the 16S rRNA data sets using DADA2 (Callahan et al. 2016). Workflow construction is based on the DADA2 Pipeline Tutorial (1.8) and the primer identification section of the DADA2 ITS Pipeline Workflow (1.8). In the first part of the pipeline, we process the individual sequencing runs separately. Next we combine the sequence tables from each run into one merged sequences table and continue with processing.",
    "crumbs": [
      "Home",
      "16S rRNA",
      "2. DADA2 ASV Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/dada2/index.html#individual-run-workflows",
    "href": "workflows/ssu/dada2/index.html#individual-run-workflows",
    "title": "2. DADA2 ASV Workflow",
    "section": "Individual Run Workflows",
    "text": "Individual Run Workflows\nThe first part of the workflow consists of the following steps for each of the runs:\n\n\n\n\n\n\n\nStep\nCommand\nWhat we’re doing\n\n\n\n1\nmultiple\nprepare input file names & paths\n\n\n2\nmultiple\nDefine primers (all orientations)\n\n\n3\ncutadapt\nRemove primers\n\n\n4\nplotQualityProfile()\nPlot quality scores.\n\n\n5\nfilterAndTrim()\nAssess quality & filter reads\n\n\n6\nlearnErrors()\nGenerate an error model for the data\n\n\n7\nderepFastq()\nDereplicate sequences\n\n\n8\ndada()\nInfer ASVs (forward & reverse reads).\n\n\n9\n\nmergePairs().\nMerge denoised forward & reverse reads\n\n\n10\nmakeSequenceTable()\nGenerate count table for each run\n\n\n11\n\nTrack reads through workflow\n\n\n\n\n\n\n\n\nflowchart LR\n    A(Start with raw&lt;/br&gt;sequence data)\n    A --&gt; B(plotQualityProfile)\n    B --&gt; C(filterAndTrim)\n    C --&gt; D(plotErrors)\n    C --&gt; E(learnErrors)\n    E --&gt; END:::hidden\n    \n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n    BEGIN:::hidden --&gt; F(derepFastq)\n    F --&gt; G(dada)\n    G --&gt; I(mergePairs) \n    I --&gt; J(makeSequenceTable)\n    J --&gt; K(to Merged Runs&lt;/br&gt;Workflow)\n\n\n\n\n\n\n\n\n\nBCS_26\nBCS_28\nBCS_29\nBCS_30\nBCS_34\nBCS_35\n\n\n\n1. Set Working Environment\n\n\n\n\n\n\nNote\n\n\n\nThe following plate were sequenced in this run: ISTHMO S5, S5, S7, S8. Access the source code for BCS_26 processing.\n\n\nFirst, we setup the working environment by defining a path for the working directory.\n\npath &lt;- \"BCS_26/\"\nhead(list.files(path)) \n\n\n\n[1] \"Control_10_R1_001.trimmed.fastq\" \"Control_10_R2_001.trimmed.fastq\"\n[3] \"Control_11_R1_001.trimmed.fastq\" \"Control_11_R2_001.trimmed.fastq\"\n[5] \"Control_12_R1_001.trimmed.fastq\" \"Control_12_R2_001.trimmed.fastq\"\n\n\nThen, we generate matched lists of the forward and reverse read files. We also parse out the sample name.\n\nfnFs &lt;- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names &lt;- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs &lt;- file.path(path, fnFs)\nfnRs &lt;- file.path(path, fnRs)\n\n2. Plot quality scores\nAnd next we plot a visual summary of the distribution of quality scores as a function of sequence position for the input fastq files. Here we set the number of records to sample from each fastq file (n) to 20000 reads.\n\nqprofile_fwd &lt;- plotQualityProfile(fnFs[1:x], \n                                   aggregate = TRUE, \n                                   n = 20000)\nqprofile_rev &lt;- plotQualityProfile(fnRs[1:x], \n                                   aggregate = TRUE, \n                                   n = 20000)\nqprofile &lt;- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\n\n\n\n\n\nAggregated quality score plots for forward (left) & reverse (right) reads.\n\n\n\n3. Filtering\nWe again make some path variables and setup a new directory of filtered reads.\n\nfiltFs &lt;- file.path(path, \"filtered\", \n                    paste0(sample.names, \"_F_filt.fastq\")\n                    )\nfiltRs &lt;- file.path(path, \"filtered\", \n                    paste0(sample.names, \"_R_filt.fastq\")\n                    )\nnames(filtFs) &lt;- sample.names\nnames(filtRs) &lt;- sample.names\n\n\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, \n                     truncLen=c(220,180), \n                     maxN = 0, maxEE = 2, truncQ = 2, \n                     rm.phix = TRUE, compress = TRUE, \n                     multithread = 20)\n\n\n\n\n\n\n\nNote\n\n\n\nThese parameters should be set based on the anticipated length of the amplicon and the read quality.\n\n\nAnd here is a table of how the filtering step affected the number of reads in each sample.\n4. Learn Error Rates\nNow it is time to assess the error rate of the data. The DADA2 algorithm uses a parametric error model. Every amplicon data set has a different set of error rates and the learnErrors method learns this error model from the data. It does this by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. The algorithm begins with an initial guess, for which the maximum possible error rates in the data are used.\nForward Reads\n\nerrF &lt;- learnErrors(filtFs, multithread = TRUE)\n\n112559480 total bases in 511634 reads from 28 samples \nwill be used for learning the error rates.\nWe can then plot the error rates for the forward reads.\n\np3 &lt;- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_26_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_26_plot_errorF_2.png\", p3)\n\n\n\n\n\nForward reads: Observed frequency of each transition (e.g., T -&gt; G) as a function of the associated quality score.\n\n\n\nReverse Reads\nAnd now we can process the reverse read error rates.\n\nerrR &lt;- learnErrors(filtRs, multithread = TRUE)\n\n100501560 total bases in 558342 reads from 31 samples \nwill be used for learning the error rates.\n\np4 &lt;- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_26_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_26_plot_errorR_2.png\", p4)\n\n\n\n\n\nReverse reads: Observed frequency of each transition (e.g., T -&gt; G) as a function of the associated quality score.\n\n\n\nThe error rates for each possible transition (A to C, A to G, etc.) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. Here the estimated error rates (black line) are a good fit to the observed rates (points), and the error rates drop with increased quality as expected.\n5. Dereplicate Reads\nNow we can use derepFastq to identify the unique sequences in the forward and reverse fastq files.\n\nsam.names &lt;- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs &lt;- derepFastq(filtFs)\nnames(derepFs) &lt;- sam.names\nderepRs &lt;- derepFastq(filtRs)\nnames(derepRs) &lt;- sam.names\n\n6. DADA2 & ASV Inference\nAt this point we are ready to apply the core sample inference algorithm (dada) to the filtered and trimmed sequence data. DADA2 offers three options for whether and how to pool samples for ASV inference.\nIf pool = TRUE, the algorithm will pool together all samples prior to sample inference.\nIf pool = FALSE, sample inference is performed on each sample individually.\nIf pool = \"pseudo\", the algorithm will perform pseudo-pooling between individually processed samples.\nFor our final analysis, we chose pool = pseudo for this data set.\n\ndadaFs &lt;- dada(derepFs, err = errF, pool = \"pseudo\", \n               multithread = TRUE)\ndadaRs &lt;- dada(derepRs, err = errR, pool = \"pseudo\",\n               multithread = TRUE)\n\nAs an example, we can inspect the returned dada-class object for the forward reads from the sample #1:\n\ndadaFs[[1]]\n\ndada-class: object describing DADA2 denoising results\n15 sequence variants were inferred from 813 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\nAnd the corresponding reverse reads.\n\ndadaRs[[1]]\n\ndada-class: object describing DADA2 denoising results\n13 sequence variants were inferred from 683 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\nThese outputs tell us how many true sequence variants the DADA2 algorithm inferred from the unique sequences, in this case the sample 1.\n7. Merge Paired Reads\nWe now merge the forward and reverse reads together to obtain the full denoised sequence dataset. Merging is performed by aligning the denoised forward reads with the reverse-complement of the corresponding denoised reverse reads, and then constructing the merged “contig” sequences. By default, merged sequences are only output if the forward and reverse reads overlap by at least 12 bases, and are identical to each other in the overlap region (but these conditions can be changed via function arguments).\n\nmergers &lt;- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\n\nThe mergers objects are lists of data.frames from each sample. Each data.frame contains the merged sequence, its abundance, and the indices of the forward and reverse sequence variants that were merged. Paired reads that did not exactly overlap were removed by mergePairs, further reducing spurious output.\n8. Construct Sequence Table\nNow we construct an amplicon sequence variant (ASV) table.\n\nBCS_26 &lt;- makeSequenceTable(mergers)\ndim(BCS_26)\n\nLooks like we have 29430 sequence variants from 384 samples.\n\ntable(nchar(getSequences(BCS_26)))\n\n  220   221   222   223   224   225   226   227   228   229   230   231   234 \n   32    30     6    13     3     2     1     9     5     2     1     3     2 \n  235   236   237   238   239   240   241   242   243   244   245   246   247 \n    2   479    83     3     4    45    31   218    72    20     3     3     7 \n  248   249   250   251   252   253   254   255   256   257   258   259   260 \n    8    10    14    47   985 25810  1025    80    27    15     6     2     3 \n  261   262   263   268   270   272   273   275   276   278   279   280   286 \n   10     3     2     1     1     2     1     1     4     1     2     1     1 \n  289   293   294   295   296   298   300   311   316   318   319   321   323 \n    1     3     3     1     2     1     1     1     2     1     1     1     1 \n  324   333   334   335   337   338   339   340   341   342   343   344   345 \n    1     1     7     5     9     1    10     5     3     7     1     3     2 \n  346   347   348   349   350   351   352   355   356   357   359   360   361 \n    3     2     5     8     5     1     7     7     2     2     4     2    22 \n  362   363   364   365   366   367   368   369   370   372   373   374   377 \n   28     4    30    50     5     3     3     2     1     3    10     1     3 \n  378 \n    2 \nThe sequence table is a matrix with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants. We have 29430 sequence variants but also a range of sequence lengths. Since many of these sequence variants are singletons or doubletons, we can just select a range that corresponds to the expected amplicon length and eliminate the spurious reads. But we will do this step after merging sequence tables from all 6 runs.\n\n\n\n\nDistribution of read length by total ASVs before removing extreme length variants.\n\n\n\n9. Tracking Reads\nIf you are interested how reads changed up to this point in the pipeline please see the Dada2 Data Portal page for more details.\nAnd save the sequence table to an RDS file.\n\nsaveRDS(BCS_26, \"BCS_26.rds\")\n\n\n\n\n1. Set Working Environment\n\n\n\n\n\n\nNote\n\n\n\nThe following plate were sequenced in this run: ISTHMO S3, S4. Access the source code for BCS_28 processing.\n\n\nFirst, we setup the working environment by defining a path for the working directory.\n\npath &lt;- \"BCS_28/\"\nhead(list.files(path)) \n\n\n\n[1] \"Control_31_R1_001.trimmed.fastq\" \"Control_31_R2_001.trimmed.fastq\"\n[3] \"Control_32_R1_001.trimmed.fastq\" \"Control_32_R2_001.trimmed.fastq\"\n[5] \"Control_33_R1_001.trimmed.fastq\" \"Control_33_R2_001.trimmed.fastq\"\n\n\nThen, we generate matched lists of the forward and reverse read files. We also parse out the sample name.\n\nfnFs &lt;- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names &lt;- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs &lt;- file.path(path, fnFs)\nfnRs &lt;- file.path(path, fnRs)\n\n2. Plot quality scores\nAnd next we plot a visual summary of the distribution of quality scores as a function of sequence position for the input fastq files. Here we set the number of records to sample from each fastq file (n) to 20000 reads.\n\nqprofile_fwd &lt;- plotQualityProfile(fnFs[1:x], \n                                   aggregate = TRUE, \n                                   n = 20000)\nqprofile_rev &lt;- plotQualityProfile(fnRs[1:x], \n                                   aggregate = TRUE, \n                                   n = 20000)\nqprofile &lt;- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\n\n\n\n\n\nAggregated quality score plots for forward (left) & reverse (right) reads.\n\n\n\n3. Filtering\nWe again make some path variables and setup a new directory of filtered reads.\n\nfiltFs &lt;- file.path(path, \"filtered\", \n                    paste0(sample.names, \"_F_filt.fastq\")\n                    )\nfiltRs &lt;- file.path(path, \"filtered\", \n                    paste0(sample.names, \"_R_filt.fastq\")\n                    )\nnames(filtFs) &lt;- sample.names\nnames(filtRs) &lt;- sample.names\n\n\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, \n                     truncLen=c(220,180), \n                     maxN = 0, maxEE = 2, truncQ = 2, \n                     rm.phix = TRUE, compress = TRUE, \n                     multithread = 20)\n\n\n\n\n\n\n\nNote\n\n\n\nThese parameters should be set based on the anticipated length of the amplicon and the read quality.\n\n\nAnd here is a table of how the filtering step affected the number of reads in each sample.\n4. Learn Error Rates\nNow it is time to assess the error rate of the data. The DADA2 algorithm uses a parametric error model. Every amplicon data set has a different set of error rates and the learnErrors method learns this error model from the data. It does this by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. The algorithm begins with an initial guess, for which the maximum possible error rates in the data are used.\nForward Reads\n\nerrF &lt;- learnErrors(filtFs, multithread = TRUE)\n\n100307240 total bases in 455942 reads from 31 samples \nwill be used for learning the error rates.\nWe can then plot the error rates for the forward reads.\n\np3 &lt;- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_28_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_28_plot_errorF_2.png\", p3)\n\n\n\n\n\nForward reads: Observed frequency of each transition (e.g., T -&gt; G) as a function of the associated quality score.\n\n\n\nReverse Reads\nAnd now we can process the reverse read error rates.\n\nerrR &lt;- learnErrors(filtRs, multithread = TRUE)\n\n103203360 total bases in 573352 reads from 36 samples \nwill be used for learning the error rates.\n\np4 &lt;- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_28_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_28_plot_errorR_2.png\", p4)\n\n\n\n\n\nReverse reads: Observed frequency of each transition (e.g., T -&gt; G) as a function of the associated quality score.\n\n\n\nThe error rates for each possible transition (A to C, A to G, etc.) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. Here the estimated error rates (black line) are a good fit to the observed rates (points), and the error rates drop with increased quality as expected.\n5. Dereplicate Reads\nNow we can use derepFastq to identify the unique sequences in the forward and reverse fastq files.\n\nsam.names &lt;- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs &lt;- derepFastq(filtFs)\nnames(derepFs) &lt;- sam.names\nderepRs &lt;- derepFastq(filtRs)\nnames(derepRs) &lt;- sam.names\n\n6. DADA2 & ASV Inference\nAt this point we are ready to apply the core sample inference algorithm (dada) to the filtered and trimmed sequence data. DADA2 offers three options for whether and how to pool samples for ASV inference.\nIf pool = TRUE, the algorithm will pool together all samples prior to sample inference.\nIf pool = FALSE, sample inference is performed on each sample individually.\nIf pool = \"pseudo\", the algorithm will perform pseudo-pooling between individually processed samples.\nFor our final analysis, we chose pool = pseudo for this data set.\n\ndadaFs &lt;- dada(derepFs, err = errF, pool = \"pseudo\", \n               multithread = TRUE)\ndadaRs &lt;- dada(derepRs, err = errR, pool = \"pseudo\",\n               multithread = TRUE)\n\nAs an example, we can inspect the returned dada-class object for the forward reads from the sample #1:\n\ndadaFs[[1]]\n\ndada-class: object describing DADA2 denoising results\n76 sequence variants were inferred from 8771 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\nAnd the corresponding reverse reads.\n\ndadaRs[[1]]\n\ndada-class: object describing DADA2 denoising results\n74 sequence variants were inferred from 6979 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\nThese outputs tell us how many true sequence variants the DADA2 algorithm inferred from the unique sequences, in this case the sample 1.\n7. Merge Paired Reads\nWe now merge the forward and reverse reads together to obtain the full denoised sequence dataset. Merging is performed by aligning the denoised forward reads with the reverse-complement of the corresponding denoised reverse reads, and then constructing the merged “contig” sequences. By default, merged sequences are only output if the forward and reverse reads overlap by at least 12 bases, and are identical to each other in the overlap region (but these conditions can be changed via function arguments).\n\nmergers &lt;- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\n\nThe mergers objects are lists of data.frames from each sample. Each data.frame contains the merged sequence, its abundance, and the indices of the forward and reverse sequence variants that were merged. Paired reads that did not exactly overlap were removed by mergePairs, further reducing spurious output.\n8. Construct Sequence Table\nNow we construct an amplicon sequence variant (ASV) table.\n\nBCS_28 &lt;- makeSequenceTable(mergers)\ndim(BCS_28)\n\nLooks like we have 9461 sequence variants from 192 samples.\n\ntable(nchar(getSequences(BCS_28)))\n\n 220  221  222  223  224  227  230  231  236  237  238  239  241  242  243  244 \n   3    8    2    1    1    4    1    1    1    2    1    1    2    5    1    1 \n 245  246  247  248  249  250  251  252  253  254  255  256  257  258  259  268 \n   1    1    2    1    1    4    8  328 8699  252   21    9    6    1    1    1 \n 270  292  318  335  336  337  338  339  341  342  344  345  348  357  359  361 \n   1    1    1    1    2    2    1    1    2    3    1    1    3    1    1    5 \n 362  363  364  365  366  367  368  372  373  374  376  378  385  388 \n   8    3   22   13    1    1    8    1    1    1    1    1    1    1 \nThe sequence table is a matrix with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants. We have 9461 sequence variants but also a range of sequence lengths. Since many of these sequence variants are singletons or doubletons, we can just select a range that corresponds to the expected amplicon length and eliminate the spurious reads. But we will do this step after merging sequence tables from all 6 runs.\n\n\n\n\nDistribution of read length by total ASVs before removing extreme length variants.\n\n\n\n9. Tracking Reads\nIf you are interested how reads changed up to this point in the pipeline please see the Dada2 Data Portal page for more details.\nAnd save the sequence table to an RDS file.\n\nsaveRDS(BCS_28, \"BCS_28.rds\")\n\n\n\n\n1. Set Working Environment\n\n\n\n\n\n\nNote\n\n\n\nThe following plate were sequenced in this run: ISTHMO S13, S14, S15, S16. Access the source code for BCS_29 processing.\n\n\nFirst, we setup the working environment by defining a path for the working directory.\n\npath &lt;- \"BCS_29/\"\nhead(list.files(path)) \n\n\n\n[1] \"Control_37_R1_001.trimmed.fastq\" \"Control_37_R2_001.trimmed.fastq\"\n[3] \"Control_38_R1_001.trimmed.fastq\" \"Control_38_R2_001.trimmed.fastq\"\n[5] \"Control_39_R1_001.trimmed.fastq\" \"Control_39_R2_001.trimmed.fastq\"\n\n\nThen, we generate matched lists of the forward and reverse read files. We also parse out the sample name.\n\nfnFs &lt;- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names &lt;- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs &lt;- file.path(path, fnFs)\nfnRs &lt;- file.path(path, fnRs)\n\n2. Plot quality scores\nAnd next we plot a visual summary of the distribution of quality scores as a function of sequence position for the input fastq files. Here we set the number of records to sample from each fastq file (n) to 20000 reads.\n\nqprofile_fwd &lt;- plotQualityProfile(fnFs[1:x], \n                                   aggregate = TRUE, \n                                   n = 20000)\nqprofile_rev &lt;- plotQualityProfile(fnRs[1:x], \n                                   aggregate = TRUE, \n                                   n = 20000)\nqprofile &lt;- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\n\n\n\n\n\nAggregated quality score plots for forward (left) & reverse (right) reads.\n\n\n\n3. Filtering\nWe again make some path variables and setup a new directory of filtered reads.\n\nfiltFs &lt;- file.path(path, \"filtered\", \n                    paste0(sample.names, \"_F_filt.fastq\")\n                    )\nfiltRs &lt;- file.path(path, \"filtered\", \n                    paste0(sample.names, \"_R_filt.fastq\")\n                    )\nnames(filtFs) &lt;- sample.names\nnames(filtRs) &lt;- sample.names\n\n\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, \n                     truncLen=c(220,180), \n                     maxN = 0, maxEE = 2, truncQ = 2, \n                     rm.phix = TRUE, compress = TRUE, \n                     multithread = 20)\n\n\n\n\n\n\n\nNote\n\n\n\nThese parameters should be set based on the anticipated length of the amplicon and the read quality.\n\n\nAnd here is a table of how the filtering step affected the number of reads in each sample.\n4. Learn Error Rates\nNow it is time to assess the error rate of the data. The DADA2 algorithm uses a parametric error model. Every amplicon data set has a different set of error rates and the learnErrors method learns this error model from the data. It does this by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. The algorithm begins with an initial guess, for which the maximum possible error rates in the data are used.\nForward Reads\n\nerrF &lt;- learnErrors(filtFs, multithread = TRUE)\n\n104777640 total bases in 476262 reads from 36 samples \nwill be used for learning the error rates.\nWe can then plot the error rates for the forward reads.\n\np3 &lt;- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_29_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_29_plot_errorF_2.png\", p3)\n\n\n\n\n\nForward reads: Observed frequency of each transition (e.g., T -&gt; G) as a function of the associated quality score.\n\n\n\nReverse Reads\nAnd now we can process the reverse read error rates.\n\nerrR &lt;- learnErrors(filtRs, multithread = TRUE)\n\n103229460 total bases in 573497 reads from 43 samples \nwill be used for learning the error rates.\n\np4 &lt;- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_29_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_29_plot_errorR_2.png\", p4)\n\n\n\n\n\nReverse reads: Observed frequency of each transition (e.g., T -&gt; G) as a function of the associated quality score.\n\n\n\nThe error rates for each possible transition (A to C, A to G, etc.) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. Here the estimated error rates (black line) are a good fit to the observed rates (points), and the error rates drop with increased quality as expected.\n5. Dereplicate Reads\nNow we can use derepFastq to identify the unique sequences in the forward and reverse fastq files.\n\nsam.names &lt;- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs &lt;- derepFastq(filtFs)\nnames(derepFs) &lt;- sam.names\nderepRs &lt;- derepFastq(filtRs)\nnames(derepRs) &lt;- sam.names\n\n6. DADA2 & ASV Inference\nAt this point we are ready to apply the core sample inference algorithm (dada) to the filtered and trimmed sequence data. DADA2 offers three options for whether and how to pool samples for ASV inference.\nIf pool = TRUE, the algorithm will pool together all samples prior to sample inference.\nIf pool = FALSE, sample inference is performed on each sample individually.\nIf pool = \"pseudo\", the algorithm will perform pseudo-pooling between individually processed samples.\nFor our final analysis, we chose pool = pseudo for this data set.\n\ndadaFs &lt;- dada(derepFs, err = errF, pool = \"pseudo\", \n               multithread = TRUE)\ndadaRs &lt;- dada(derepRs, err = errR, pool = \"pseudo\",\n               multithread = TRUE)\n\nAs an example, we can inspect the returned dada-class object for the forward reads from the sample #1:\n\ndadaFs[[1]]\n\ndada-class: object describing DADA2 denoising results\n46 sequence variants were inferred from 3108 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\nAnd the corresponding reverse reads.\n\ndadaRs[[1]]\n\ndada-class: object describing DADA2 denoising results\n44 sequence variants were inferred from 2744 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\nThese outputs tell us how many true sequence variants the DADA2 algorithm inferred from the unique sequences, in this case the sample 1.\n7. Merge Paired Reads\nWe now merge the forward and reverse reads together to obtain the full denoised sequence dataset. Merging is performed by aligning the denoised forward reads with the reverse-complement of the corresponding denoised reverse reads, and then constructing the merged “contig” sequences. By default, merged sequences are only output if the forward and reverse reads overlap by at least 12 bases, and are identical to each other in the overlap region (but these conditions can be changed via function arguments).\n\nmergers &lt;- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\n\nThe mergers objects are lists of data.frames from each sample. Each data.frame contains the merged sequence, its abundance, and the indices of the forward and reverse sequence variants that were merged. Paired reads that did not exactly overlap were removed by mergePairs, further reducing spurious output.\n8. Construct Sequence Table\nNow we construct an amplicon sequence variant (ASV) table.\n\nBCS_29 &lt;- makeSequenceTable(mergers)\ndim(BCS_29)\n\nLooks like we have 21105 sequence variants from 384 samples.\n\ntable(nchar(getSequences(BCS_29)))\n\n   220   221   222   223   224   225   226   227   228   229   231   234   236 \n   16    22     1    10     5     5     2    10     1     2    11     3   518 \n  237   238   240   241   242   243   244   245   246   247   248   249   250 \n    3     7     8   155    11    81     1     2     4     4     1     2     8 \n  251   252   253   254   255   256   257   258   260   262   269   270   272 \n   28   690 18300   857    42    17    18     1     1     2     1     1     1 \n  273   274   275   277   278   281   286   288   292   293   294   297   300 \n    1     1     1     1     3     1     1     1     1     2     2     1     1 \n  303   304   305   308   313   315   318   319   329   330   334   335   336 \n    2     1     3     1     2     1     1     1     3     2     3     1     1 \n  337   339   340   341   342   343   344   347   348   349   350   352   356 \n    2     4     5    21     7     1     1     1     2     3     1     1     2 \n  357   358   359   360   361   362   363   364   365   366   367   368   370 \n   14     1     2     6    14    65     4    32    13     4     2     2     2 \n  379   384 \n    1     1  \nThe sequence table is a matrix with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants. We have 21105 sequence variants but also a range of sequence lengths. Since many of these sequence variants are singletons or doubletons, we can just select a range that corresponds to the expected amplicon length and eliminate the spurious reads. But we will do this step after merging sequence tables from all 6 runs.\n\n\n\n\nDistribution of read length by total ASVs before removing extreme length variants.\n\n\n\n9. Tracking Reads\nIf you are interested how reads changed up to this point in the pipeline please see the Dada2 Data Portal page for more details.\nAnd save the sequence table to an RDS file.\n\nsaveRDS(BCS_29, \"BCS_29.rds\")\n\n\n\n\n1. Set Working Environment\n\n\n\n\n\n\nNote\n\n\n\nThe following plate were sequenced in this run: ISTHMO S17, S18, S19, S20. Access the source code for BCS_30 processing.\n\n\nFirst, we setup the working environment by defining a path for the working directory.\n\npath &lt;- \"BCS_30/\"\nhead(list.files(path)) \n\n\n\n[1] \"Control_49_R1_001.trimmed.fastq\" \"Control_49_R2_001.trimmed.fastq\"\n[3] \"Control_50_R1_001.trimmed.fastq\" \"Control_50_R2_001.trimmed.fastq\"\n[5] \"Control_51_R1_001.trimmed.fastq\" \"Control_51_R2_001.trimmed.fastq\"\n\n\nThen, we generate matched lists of the forward and reverse read files. We also parse out the sample name.\n\nfnFs &lt;- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names &lt;- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs &lt;- file.path(path, fnFs)\nfnRs &lt;- file.path(path, fnRs)\n\n2. Plot quality scores\nAnd next we plot a visual summary of the distribution of quality scores as a function of sequence position for the input fastq files. Here we set the number of records to sample from each fastq file (n) to 20000 reads.\n\nqprofile_fwd &lt;- plotQualityProfile(fnFs[1:x], \n                                   aggregate = TRUE, \n                                   n = 20000)\nqprofile_rev &lt;- plotQualityProfile(fnRs[1:x], \n                                   aggregate = TRUE, \n                                   n = 20000)\nqprofile &lt;- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\n\n\n\n\n\nAggregated quality score plots for forward (left) & reverse (right) reads.\n\n\n\n3. Filtering\nWe again make some path variables and setup a new directory of filtered reads.\n\nfiltFs &lt;- file.path(path, \"filtered\", \n                    paste0(sample.names, \"_F_filt.fastq\")\n                    )\nfiltRs &lt;- file.path(path, \"filtered\", \n                    paste0(sample.names, \"_R_filt.fastq\")\n                    )\nnames(filtFs) &lt;- sample.names\nnames(filtRs) &lt;- sample.names\n\n\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, \n                     truncLen=c(220,180), \n                     maxN = 0, maxEE = 2, truncQ = 2, \n                     rm.phix = TRUE, compress = TRUE, \n                     multithread = 20)\n\n\n\n\n\n\n\nNote\n\n\n\nThese parameters should be set based on the anticipated length of the amplicon and the read quality.\n\n\nAnd here is a table of how the filtering step affected the number of reads in each sample.\n4. Learn Error Rates\nNow it is time to assess the error rate of the data. The DADA2 algorithm uses a parametric error model. Every amplicon data set has a different set of error rates and the learnErrors method learns this error model from the data. It does this by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. The algorithm begins with an initial guess, for which the maximum possible error rates in the data are used.\nForward Reads\n\nerrF &lt;- learnErrors(filtFs, multithread = TRUE)\n\n101887500 total bases in 463125 reads from 26 samples \nwill be used for learning the error rates.\nWe can then plot the error rates for the forward reads.\n\np3 &lt;- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_30_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_30_plot_errorF_2.png\", p3)\n\n\n\n\n\nForward reads: Observed frequency of each transition (e.g., T -&gt; G) as a function of the associated quality score.\n\n\n\nReverse Reads\nAnd now we can process the reverse read error rates.\n\nerrR &lt;- learnErrors(filtRs, multithread = TRUE)\n\n105320340 total bases in 585113 reads from 30 samples \nwill be used for learning the error rates.\n\np4 &lt;- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_30_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_30_plot_errorR_2.png\", p4)\n\n\n\n\n\nReverse reads: Observed frequency of each transition (e.g., T -&gt; G) as a function of the associated quality score.\n\n\n\nThe error rates for each possible transition (A to C, A to G, etc.) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. Here the estimated error rates (black line) are a good fit to the observed rates (points), and the error rates drop with increased quality as expected.\n5. Dereplicate Reads\nNow we can use derepFastq to identify the unique sequences in the forward and reverse fastq files.\n\nsam.names &lt;- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs &lt;- derepFastq(filtFs)\nnames(derepFs) &lt;- sam.names\nderepRs &lt;- derepFastq(filtRs)\nnames(derepRs) &lt;- sam.names\n\n6. DADA2 & ASV Inference\nAt this point we are ready to apply the core sample inference algorithm (dada) to the filtered and trimmed sequence data. DADA2 offers three options for whether and how to pool samples for ASV inference.\nIf pool = TRUE, the algorithm will pool together all samples prior to sample inference.\nIf pool = FALSE, sample inference is performed on each sample individually.\nIf pool = \"pseudo\", the algorithm will perform pseudo-pooling between individually processed samples.\nFor our final analysis, we chose pool = pseudo for this data set.\n\ndadaFs &lt;- dada(derepFs, err = errF, pool = \"pseudo\", \n               multithread = TRUE)\ndadaRs &lt;- dada(derepRs, err = errR, pool = \"pseudo\",\n               multithread = TRUE)\n\nAs an example, we can inspect the returned dada-class object for the forward reads from the sample #1:\n\ndadaFs[[1]]\n\ndada-class: object describing DADA2 denoising results\n9 sequence variants were inferred from 25 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\nAnd the corresponding reverse reads.\n\ndadaRs[[1]]\n\ndada-class: object describing DADA2 denoising results\n9 sequence variants were inferred from 21 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\nThese outputs tell us how many true sequence variants the DADA2 algorithm inferred from the unique sequences, in this case the sample 1.\n7. Merge Paired Reads\nWe now merge the forward and reverse reads together to obtain the full denoised sequence dataset. Merging is performed by aligning the denoised forward reads with the reverse-complement of the corresponding denoised reverse reads, and then constructing the merged “contig” sequences. By default, merged sequences are only output if the forward and reverse reads overlap by at least 12 bases, and are identical to each other in the overlap region (but these conditions can be changed via function arguments).\n\nmergers &lt;- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\n\nThe mergers objects are lists of data.frames from each sample. Each data.frame contains the merged sequence, its abundance, and the indices of the forward and reverse sequence variants that were merged. Paired reads that did not exactly overlap were removed by mergePairs, further reducing spurious output.\n8. Construct Sequence Table\nNow we construct an amplicon sequence variant (ASV) table.\n\nBCS_30 &lt;- makeSequenceTable(mergers)\ndim(BCS_30)\n\nLooks like we have 36401 sequence variants from 380 samples.\n\ntable(nchar(getSequences(BCS_30)))\n\n  220   221   222   223   224   225   226   227   228   229   231   232   234 \n   45    24     5    12    10     3     2    13     2     1    15     2     3 \n  235   236   237   238   240   241   242   243   244   245   246   247   248 \n    5   142     2    29    33   128    26   266     5     3     4    10     8 \n  249   250   251   252   253   254   255   256   257   258   259   260   261 \n    7    16    62  1119 31980  1708   149    56    47     7     6     2     4 \n  262   263   264   267   268   269   270   271   272   273   274   276   278 \n    3     1     2     1     1     1     4     7     1     2     2     1     5 \n  279   282   284   285   286   288   289   291   292   293   294   295   296 \n    1     1     1     3     1     1     2     1     1     5     1     1     1 \n  297   298   303   305   307   308   311   313   316   317   319   320   321 \n    1     1     1     2     1     1     2     2     2     1     2     1     1 \n  322   323   325   326   328   332   333   334   335   336   337   338   339 \n    2     1     2     1     2     1     1     2     1     6     3     3     5 \n  340   341   342   343   344   345   347   348   349   350   351   352   353 \n    2    21    23     4     2     4     8     6     6     3     1     2     1 \n  354   355   356   357   358   359   360   361   362   363   364   365   366 \n    1     1     3     6     2     8     9    76    58    18    33    22     5 \n  368   371   372   373   379   380   387 \n    8     1     1     1     2     1     2 \nThe sequence table is a matrix with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants. We have 36401 sequence variants but also a range of sequence lengths. Since many of these sequence variants are singletons or doubletons, we can just select a range that corresponds to the expected amplicon length and eliminate the spurious reads. But we will do this step after merging sequence tables from all 6 runs.\n\n\n\n\nDistribution of read length by total ASVs before removing extreme length variants.\n\n\n\n9. Tracking Reads\nIf you are interested how reads changed up to this point in the pipeline please see the Dada2 Data Portal page for more details.\nAnd save the sequence table to an RDS file.\n\nsaveRDS(BCS_30, \"BCS_30.rds\")\n\n\n\n\n1. Set Working Environment\n\n\n\n\n\n\nNote\n\n\n\nThe following plate were sequenced in this run: ISTHMO S01, S02. Access the source code for BCS_34 processing.\n\n\nFirst, we setup the working environment by defining a path for the working directory.\n\npath &lt;- \"BCS_34/\"\nhead(list.files(path)) \n\n\n\n[1] \"Control_25_R1_001.trimmed.fastq\" \"Control_25_R2_001.trimmed.fastq\"\n[3] \"Control_26_R1_001.trimmed.fastq\" \"Control_26_R2_001.trimmed.fastq\"\n[5] \"Control_27_R1_001.trimmed.fastq\" \"Control_27_R2_001.trimmed.fastq\"\n\n\nThen, we generate matched lists of the forward and reverse read files. We also parse out the sample name.\n\nfnFs &lt;- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names &lt;- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs &lt;- file.path(path, fnFs)\nfnRs &lt;- file.path(path, fnRs)\n\n2. Plot quality scores\nAnd next we plot a visual summary of the distribution of quality scores as a function of sequence position for the input fastq files. Here we set the number of records to sample from each fastq file (n) to 20000 reads.\n\nqprofile_fwd &lt;- plotQualityProfile(fnFs[1:x], \n                                   aggregate = TRUE, \n                                   n = 20000)\nqprofile_rev &lt;- plotQualityProfile(fnRs[1:x], \n                                   aggregate = TRUE, \n                                   n = 20000)\nqprofile &lt;- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\n\n\n\n\n\nAggregated quality score plots for forward (left) & reverse (right) reads.\n\n\n\n3. Filtering\nWe again make some path variables and setup a new directory of filtered reads.\n\nfiltFs &lt;- file.path(path, \"filtered\", \n                    paste0(sample.names, \"_F_filt.fastq\")\n                    )\nfiltRs &lt;- file.path(path, \"filtered\", \n                    paste0(sample.names, \"_R_filt.fastq\")\n                    )\nnames(filtFs) &lt;- sample.names\nnames(filtRs) &lt;- sample.names\n\n\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, \n                     truncLen=c(220,180), \n                     maxN = 0, maxEE = 2, truncQ = 2, \n                     rm.phix = TRUE, compress = TRUE, \n                     multithread = 20)\n\n\n\n\n\n\n\nNote\n\n\n\nThese parameters should be set based on the anticipated length of the amplicon and the read quality.\n\n\nAnd here is a table of how the filtering step affected the number of reads in each sample.\n4. Learn Error Rates\nNow it is time to assess the error rate of the data. The DADA2 algorithm uses a parametric error model. Every amplicon data set has a different set of error rates and the learnErrors method learns this error model from the data. It does this by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. The algorithm begins with an initial guess, for which the maximum possible error rates in the data are used.\nForward Reads\n\nerrF &lt;- learnErrors(filtFs, multithread = TRUE)\n\n100938640 total bases in 458812 reads from 22 samples \nwill be used for learning the error rates.\nWe can then plot the error rates for the forward reads.\n\np3 &lt;- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_34_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_34_plot_errorF_2.png\", p3)\n\n\n\n\n\nForward reads: Observed frequency of each transition (e.g., T -&gt; G) as a function of the associated quality score.\n\n\n\nReverse Reads\nAnd now we can process the reverse read error rates.\n\nerrR &lt;- learnErrors(filtRs, multithread = TRUE)\n\n104387760 total bases in 579932 reads from 25 samples \nwill be used for learning the error rates.\n\np4 &lt;- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_34_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_34_plot_errorR_2.png\", p4)\n\n\n\n\n\nReverse reads: Observed frequency of each transition (e.g., T -&gt; G) as a function of the associated quality score.\n\n\n\nThe error rates for each possible transition (A to C, A to G, etc.) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. Here the estimated error rates (black line) are a good fit to the observed rates (points), and the error rates drop with increased quality as expected.\n5. Dereplicate Reads\nNow we can use derepFastq to identify the unique sequences in the forward and reverse fastq files.\n\nsam.names &lt;- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs &lt;- derepFastq(filtFs)\nnames(derepFs) &lt;- sam.names\nderepRs &lt;- derepFastq(filtRs)\nnames(derepRs) &lt;- sam.names\n\n6. DADA2 & ASV Inference\nAt this point we are ready to apply the core sample inference algorithm (dada) to the filtered and trimmed sequence data. DADA2 offers three options for whether and how to pool samples for ASV inference.\nIf pool = TRUE, the algorithm will pool together all samples prior to sample inference.\nIf pool = FALSE, sample inference is performed on each sample individually.\nIf pool = \"pseudo\", the algorithm will perform pseudo-pooling between individually processed samples.\nFor our final analysis, we chose pool = pseudo for this data set.\n\ndadaFs &lt;- dada(derepFs, err = errF, pool = \"pseudo\", \n               multithread = TRUE)\ndadaRs &lt;- dada(derepRs, err = errR, pool = \"pseudo\",\n               multithread = TRUE)\n\nAs an example, we can inspect the returned dada-class object for the forward reads from the sample #1:\n\ndadaFs[[1]]\n\ndada-class: object describing DADA2 denoising results\n9 sequence variants were inferred from 327 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\nAnd the corresponding reverse reads.\n\ndadaRs[[1]]\n\ndada-class: object describing DADA2 denoising results\n8 sequence variants were inferred from 255 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\nThese outputs tell us how many true sequence variants the DADA2 algorithm inferred from the unique sequences, in this case the sample 1.\n7. Merge Paired Reads\nWe now merge the forward and reverse reads together to obtain the full denoised sequence dataset. Merging is performed by aligning the denoised forward reads with the reverse-complement of the corresponding denoised reverse reads, and then constructing the merged “contig” sequences. By default, merged sequences are only output if the forward and reverse reads overlap by at least 12 bases, and are identical to each other in the overlap region (but these conditions can be changed via function arguments).\n\nmergers &lt;- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\n\nThe mergers objects are lists of data.frames from each sample. Each data.frame contains the merged sequence, its abundance, and the indices of the forward and reverse sequence variants that were merged. Paired reads that did not exactly overlap were removed by mergePairs, further reducing spurious output.\n8. Construct Sequence Table\nNow we construct an amplicon sequence variant (ASV) table.\n\nBCS_34 &lt;- makeSequenceTable(mergers)\ndim(BCS_34)\n\nLooks like we have 18373 sequence variants from 190 samples.\n\ntable(nchar(getSequences(BCS_34)))\n\n  220   221   222   223   224   225   226   227   229   230   231   234   236 \n   14    21     2     6     1     2     2     9     1     1     2     2   101 \n  237   238   239   241   242   243   244   245   246   247   248   249   250 \n   62     1     1    11    16     6     4     4     4     2     4     5     6 \n  251   252   253   254   255   256   257   258   259   261   265   270   271 \n   20   697 16533   584    52    15    11     5     1     2     1     2     1 \n  274   278   279   285   286   290   291   295   308   309   322   325   328 \n    1     1     1     1     1     1     1     1     1     1     1     1     1 \n  335   336   337   338   339   340   341   342   343   344   345   347   348 \n    2     1     4     1     5     1     4     8     2     3     2     1     5 \n  349   356   357   358   359   360   361   362   363   364   365   366   367 \n    1     1     1     3     2     1    14    15     8    25    16     2     1 \n  368   371   372   373   376   377   378   385   386 \n    3     2     2     4     2     1     1     1     1  \nThe sequence table is a matrix with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants. We have 18373 sequence variants but also a range of sequence lengths. Since many of these sequence variants are singletons or doubletons, we can just select a range that corresponds to the expected amplicon length and eliminate the spurious reads. But we will do this step after merging sequence tables from all 6 runs.\n\n\n\n\nDistribution of read length by total ASVs before removing extreme length variants.\n\n\n\n9. Tracking Reads\nIf you are interested how reads changed up to this point in the pipeline please see the Dada2 Data Portal page for more details.\nAnd save the sequence table to an RDS file.\n\nsaveRDS(BCS_34, \"BCS_34.rds\")\n\n\n\n\n1. Set Working Environment\n\n\n\n\n\n\nNote\n\n\n\nThe following plate were sequenced in this run: ISTHMO S9, S10, S11, S12. Access the source code for BCS_35 processing.\n\n\nFirst, we setup the working environment by defining a path for the working directory.\n\npath &lt;- \"BCS_35/\"\nhead(list.files(path)) \n\n\n\n[1] \"Control_13_R1_001.trimmed.fastq\" \"Control_13_R2_001.trimmed.fastq\"\n[3] \"Control_14_R1_001.trimmed.fastq\" \"Control_14_R2_001.trimmed.fastq\"\n[5] \"Control_15_R1_001.trimmed.fastq\" \"Control_15_R2_001.trimmed.fastq\"\n\n\nThen, we generate matched lists of the forward and reverse read files. We also parse out the sample name.\n\nfnFs &lt;- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names &lt;- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs &lt;- file.path(path, fnFs)\nfnRs &lt;- file.path(path, fnRs)\n\n2. Plot quality scores\nAnd next we plot a visual summary of the distribution of quality scores as a function of sequence position for the input fastq files. Here we set the number of records to sample from each fastq file (n) to 20000 reads.\n\nqprofile_fwd &lt;- plotQualityProfile(fnFs[1:x], \n                                   aggregate = TRUE, \n                                   n = 20000)\nqprofile_rev &lt;- plotQualityProfile(fnRs[1:x], \n                                   aggregate = TRUE, \n                                   n = 20000)\nqprofile &lt;- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\n\n\n\n\n\nAggregated quality score plots for forward (left) & reverse (right) reads.\n\n\n\n3. Filtering\nWe again make some path variables and setup a new directory of filtered reads.\n\nfiltFs &lt;- file.path(path, \"filtered\", \n                    paste0(sample.names, \"_F_filt.fastq\")\n                    )\nfiltRs &lt;- file.path(path, \"filtered\", \n                    paste0(sample.names, \"_R_filt.fastq\")\n                    )\nnames(filtFs) &lt;- sample.names\nnames(filtRs) &lt;- sample.names\n\n\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, \n                     truncLen=c(220,180), \n                     maxN = 0, maxEE = 2, truncQ = 2, \n                     rm.phix = TRUE, compress = TRUE, \n                     multithread = 20)\n\n\n\n\n\n\n\nNote\n\n\n\nThese parameters should be set based on the anticipated length of the amplicon and the read quality.\n\n\nAnd here is a table of how the filtering step affected the number of reads in each sample.\n4. Learn Error Rates\nNow it is time to assess the error rate of the data. The DADA2 algorithm uses a parametric error model. Every amplicon data set has a different set of error rates and the learnErrors method learns this error model from the data. It does this by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. The algorithm begins with an initial guess, for which the maximum possible error rates in the data are used.\nForward Reads\n\nerrF &lt;- learnErrors(filtFs, multithread = TRUE)\n\n101017840 total bases in 459172 reads from 48 samples \nwill be used for learning the error rates.\nWe can then plot the error rates for the forward reads.\n\np3 &lt;- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_35_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_35_plot_errorF_2.png\", p3)\n\n\n\n\n\nForward reads: Observed frequency of each transition (e.g., T -&gt; G) as a function of the associated quality score.\n\n\n\nReverse Reads\nAnd now we can process the reverse read error rates.\n\nerrR &lt;- learnErrors(filtRs, multithread = TRUE)\n\n100695240 total bases in 559418 reads from 59 samples \nwill be used for learning the error rates.\n\np4 &lt;- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_35_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_35_plot_errorR_2.png\", p4)\n\n\n\n\n\nReverse reads: Observed frequency of each transition (e.g., T -&gt; G) as a function of the associated quality score.\n\n\n\nThe error rates for each possible transition (A to C, A to G, etc.) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. Here the estimated error rates (black line) are a good fit to the observed rates (points), and the error rates drop with increased quality as expected.\n5. Dereplicate Reads\nNow we can use derepFastq to identify the unique sequences in the forward and reverse fastq files.\n\nsam.names &lt;- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs &lt;- derepFastq(filtFs)\nnames(derepFs) &lt;- sam.names\nderepRs &lt;- derepFastq(filtRs)\nnames(derepRs) &lt;- sam.names\n\n6. DADA2 & ASV Inference\nAt this point we are ready to apply the core sample inference algorithm (dada) to the filtered and trimmed sequence data. DADA2 offers three options for whether and how to pool samples for ASV inference.\nIf pool = TRUE, the algorithm will pool together all samples prior to sample inference.\nIf pool = FALSE, sample inference is performed on each sample individually.\nIf pool = \"pseudo\", the algorithm will perform pseudo-pooling between individually processed samples.\nFor our final analysis, we chose pool = pseudo for this data set.\n\ndadaFs &lt;- dada(derepFs, err = errF, pool = \"pseudo\", \n               multithread = TRUE)\ndadaRs &lt;- dada(derepRs, err = errR, pool = \"pseudo\",\n               multithread = TRUE)\n\nAs an example, we can inspect the returned dada-class object for the forward reads from the sample #1:\n\ndadaFs[[1]]\n\ndada-class: object describing DADA2 denoising results\n15 sequence variants were inferred from 432 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\nAnd the corresponding reverse reads.\n\ndadaRs[[1]]\n\ndada-class: object describing DADA2 denoising results\n19 sequence variants were inferred from 465 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\nThese outputs tell us how many true sequence variants the DADA2 algorithm inferred from the unique sequences, in this case the sample 1.\n7. Merge Paired Reads\nWe now merge the forward and reverse reads together to obtain the full denoised sequence dataset. Merging is performed by aligning the denoised forward reads with the reverse-complement of the corresponding denoised reverse reads, and then constructing the merged “contig” sequences. By default, merged sequences are only output if the forward and reverse reads overlap by at least 12 bases, and are identical to each other in the overlap region (but these conditions can be changed via function arguments).\n\nmergers &lt;- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\n\nThe mergers objects are lists of data.frames from each sample. Each data.frame contains the merged sequence, its abundance, and the indices of the forward and reverse sequence variants that were merged. Paired reads that did not exactly overlap were removed by mergePairs, further reducing spurious output.\n8. Construct Sequence Table\nNow we construct an amplicon sequence variant (ASV) table.\n\nBCS_35 &lt;- makeSequenceTable(mergers)\ndim(BCS_35)\n\nLooks like we have 23367 sequence variants from 379 samples.\n\ntable(nchar(getSequences(BCS_35)))\n\n  220   221   222   223   224   225   226   227   228   229   230   234   235 \n   16    28     5     9     2     2     4     4     1     1     1     1     2 \n  236   237   240   241   242   243   245   246   247   248   249   250   251 \n  147     4    10    76    16    27     5     1     2     3     2     8    31 \n  252   253   254   255   256   257   258   260   262   263   266   268   273 \n 1004 20727   767    44     9    22     1     3     3     1     1     1     3 \n  282   292   293   294   295   303   307   309   310   312   333   334   335 \n    1     1     3     1     1     1     1     1     1     1     1     4     1 \n  336   337   339   340   341   342   344   346   347   348   349   350   352 \n    1     6     6     3    39    23     4     1    19     5     5     3     3 \n  357   359   360   361   362   363   364   365   366   367   368   369   372 \n   13     1     8    64    46    18    33    26     4     4    14     3     1 \n  376   377 \n    1     1  \nThe sequence table is a matrix with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants. We have 23367 sequence variants but also a range of sequence lengths. Since many of these sequence variants are singletons or doubletons, we can just select a range that corresponds to the expected amplicon length and eliminate the spurious reads. But we will do this step after merging sequence tables from all 6 runs.\n\n\n\n\nDistribution of read length by total ASVs before removing extreme length variants.\n\n\n\n9. Tracking Reads\nIf you are interested how reads changed up to this point in the pipeline please see the Dada2 Data Portal page for more details.\nAnd save the sequence table to an RDS file.\n\nsaveRDS(BCS_35, \"BCS_35.rds\")",
    "crumbs": [
      "Home",
      "16S rRNA",
      "2. DADA2 ASV Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/dada2/index.html#merged-runs-workflow",
    "href": "workflows/ssu/dada2/index.html#merged-runs-workflow",
    "title": "2. DADA2 ASV Workflow",
    "section": "Merged Runs Workflow",
    "text": "Merged Runs Workflow\nNow it is time to combine the sequence tables from each run together into one merged sequences table.\nWe start by reading in each sequence table.\n\n\n\n\n\n\nNote\n\n\n\nClick here to access the source code for the Merge Runs section of the workflow.\n\n\n\n\nStep\nCommand\nWhat we’re doing\n\n\n\n10\nmergeSequenceTables()\nmerge seqtabs from all runs.\n\n\n11\nremoveBimeraDenovo()\nscreen for & remove chimeras\n\n\n12\n\ntrack reads through workflow\n\n\n13\nassignTaxonomy()\nassign taxonomy & finish workflow\n\n\n\n\n\n\n\n\nflowchart LR\n    D(BCS_26&lt;br/&gt;BCS_28&lt;br/&gt;BCS_29&lt;br/&gt;BCS_30&lt;br/&gt;BCS_34&lt;br/&gt;BCS_35&lt;br/&gt;) --&gt; M\n    D --&gt; M\n    D --&gt; M\n    D --&gt; M\n    D --&gt; M\n    D --&gt; M\n    M(mergeSequenceTables) --&gt; N(removeBimeraDenovo)\n    N --&gt; O(assignTaxonomy)\n\n\n\n\n\n\n\nBCS_26 &lt;- readRDS(\"`BCS_26.rds\")\nBCS_28 &lt;- readRDS(\"`BCS_28.rds\")\nBCS_29 &lt;- readRDS(\"`BCS_29.rds\")\nBCS_30 &lt;- readRDS(\"`BCS_30.rds\")\nBCS_34 &lt;- readRDS(\"`BCS_34.rds\")\nBCS_35 &lt;- readRDS(\"`BCS_35.rds\")\n\n1. Merge Sequencing Tables\n\nseqtab.merge &lt;- mergeSequenceTables(BCS_26, BCS_28, BCS_29, \n                                    BCS_30, BCS_34, BCS_35)\ndim(seqtab.merge)\n\n\n\n[1]  1909 96680\n\n\nSo our count is 96680 ASVs across 1909 samples.\n\ntable(nchar(getSequences(seqtab.merge)))\n\n  220   221   222   223   224   225   226   227   228   229   230   231   232 \n  124    67    14    36    20    13    10    25     8     6     4    30     2 \n  234   235   236   237   238   239   240   241   242   243   244   245   246 \n    9     8  1371   151    41     6    96   401   291   443    31    14    13 \n  247   248   249   250   251   252   253   254   255   256   257   258   259 \n   26    23    19    48   159  3606 83887  3756   315   121    95    20    10 \n  260   261   262   263   264   265   266   267   268   269   270   271   272 \n    8    16     9     4     2     1     1     1     4     2     9     8     4 \n  273   274   275   276   277   278   279   280   281   282   284   285   286 \n    7     3     2     5     1     7     4     1     1     2     1     4     4 \n  288   289   290   291   292   293   294   295   296   297   298   300   303 \n    1     3     1     2     4     8     7     2     3     2     2     2     3 \n  304   305   307   308   309   310   311   312   313   315   316   317   318 \n    1     5     2     3     2     1     3     1     3     1     4     1     3 \n  319   320   321   322   323   324   325   326   328   329   330   332   333 \n    3     1     2     3     2     1     3     1     3     3     2     1     3 \n  334   335   336   337   338   339   340   341   342   343   344   345   346 \n   13     6     7    17     5    25    16    70    51     8     7     8     4 \n  347   348   349   350   351   352   353   354   355   356   357   358   359 \n   28    17    21    10     2    11     1     1     7     6    31     6    15 \n  360   361   362   363   364   365   366   367   368   369   370   371   372 \n   21   161   186    43   135   107    19     9    26     5     3     3     8 \n  373   374   376   377   378   379   380   384   385   386   387   388 \n   11     2     3     5     2     3     1     1     1     1     2     1 \n\nread_length_all &lt;-  data.frame(nchar(getSequences(seqtab.merge)))\ncolnames(read_length_all) &lt;- \"length\"\nplot_all &lt;- qplot(length, data = read_length_all, geom = \"histogram\", \n                  binwidth = 1, xlab = \"read length\", \n                  ylab = \"total variants\", xlim = c(200,400)) \n\n\n\n\n\nDistribution of read length by total ASVs after merging & before removing extreme length variants.\n\n\n\nThen we remove length variants.\n\nseqtab.trim &lt;- seqtab.merge[,nchar(colnames(seqtab.merge)) %in% \n                              seq(252, 254)]\ndim(seqtab.trim)\n\n\n\n[1]  1909 91249\n\n\nAnd now our count is 91249 ASVs across 1909 samples.\n\ntable(nchar(getSequences(seqtab.trim)))\n\n 252   253   254 \n 3606 83887  3756 \n2. Remove Chimeras\nEven though the dada method corrects substitution and indel errors, chimeric sequences remain. According to the DADA2 documentation, the accuracy of sequence variants after denoising makes identifying chimeric ASVs simpler than when dealing with fuzzy OTUs. Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant parent sequences.\n\nseqtab.trim.nochim.consensus &lt;- \n  removeBimeraDenovo(seqtab.trim, \n                     method = \"consensus\", \n                     multithread = 20,  \n                     verbose = TRUE)\ndim(seqtab.trim.nochim.consensus)\n\nIdentified 18398 bimeras out of 91249 input sequences.\n\n\n[1]  1909 72851\n\n\n\nsum(seqtab.nochim)/sum(seqtab.2)\n\n[1] 0.9669996\nChimera checking removed an additional 18398 sequence variants however, when we account for the abundances of each variant, we see chimeras accounts for about 3.30004% of the merged sequence reads. Not bad.\n3. Track Reads through Workflow\nAt this point we can look at the number of reads that made it through each step of the workflow for every sample.\n\ngetN &lt;- function(x) sum(getUniques(x))\ntrack &lt;- cbind(rowSums(seqtab), \n               rowSums(seqtab.trim), \n               rowSums(seqtab.trim.nochim.pool), \n               rowSums(seqtab.trim.nochim.consensus))\n\ncolnames(track) &lt;- c(\"merged\", \"trim\", \n                     \"chimera_pool\", \n                     \"chimera_concensus\")\n\n4. Assign Taxonomy\nThe assignTaxonomy command implements the naive Bayesian classifier, so for reproducible results you need to set a random number seed (see issue #538). We did this at the beginning of the workflow. For taxonomic assignment, we are using the GSR database (Molano, Vega-Abellaneda, and Manichanh 2024). The developers of DADA2 maintain formatted versions of popular databases, however the GSR-DB has not been formatted by the developers yet.\n\n\n\n\n\n\nNote\n\n\n\nClick the link to can download an appropriate version of the GSR database.\n\n\nTo create a DADA2 formatted version GSR-DB1, we perform the following steps.\nDownload a data base\nHere we are using the GSR V4 database.\n\nwget https://manichanh.vhir.org/gsrdb/GSR-DB_V4_cluster-1.tar.gz\ntar -xvzf GSR-DB_V4_cluster-1.tar.gz\n\nOnce you uncompress the tar file you should see four files, two .qza files (which you can ignore), a _taxa.txt file and a _seqs.fasta file. We are interested in the latter two files. These are the files we need to format for DADA2. How about we have a look at each file?\nFirst the taxonomy file.\n\nhead GSR-DB_V4_cluster-1_taxa.txt\n\nFeature ID  Taxon\nAY999846    k__Bacteria; p__Actinobacteria; c__Actinomycetia; o__Actinomycetales-Streptomycetales-Unknown; f__Streptomycetaceae-Unknown; g__Streptomyces-Unknown; s__Unknown\nJN885187.1.1362 k__Bacteria; p__Actinobacteria; c__Actinomycetia; o__Actinomycetales-Streptomycetales-Unknown; f__Streptomycetaceae-Unknown; g__Kitasatospora-Streptomyces-Unknown; s__Unknown\nAnd next the fasta file.\n\nhead GSR-DB_V4_cluster-1_seqs.fasta\n\n&gt;AY999846\nTACGTAGGGCGCAAGCGTTGTCCGGAATTATTGGGCGTAAAGAGCTCGTAGGCGGCTTGT\nCACGTCGGTTGTGAAAGCCCGGGGCTTAACCCCGGGTCTGCAGTCGATACGGGCAGGCTA\nGAGTTCGGTAGGGGAGATCGGAATTCCTGGTGTAGCGGTGAAATGCGCAGATATCAGGAG\nGAACACCGGTGGCGAAGGCGGATCTCTGGGCCGATACTGACGCTGAGGAGCGAAAGCGTG\nGGGAGCGAACAGG\n&gt;JN885187.1.1362\nTACGTAGGGCGCGAGCGTTGTCCGGAATTATTGGGCGTAAAGAGCTCGTAGGCGGCTTGT\nCACGTCGGTTGTGAAAGCCCGGGGCTTAACCCCGGGTCTGCAGTCGATACGGGCAGGCTA\nGAGTTCGGTAGGGGAGATCGGAATTCCTGGTGTAGCGGTGAAATGCGCAGATATCAGGAG\nDADA2 requires a very specific format for classification. For the next few step we use a tool called SeqKit (Shen, Sipos, and Zhao 2024) for fasta defline manipulation.\nThis first step replaces the original fasta defline with the correspoding lineage information.\n\nconda activate seqkit\nseqkit replace -w 0  -p \"(.+)\" -r '{kv}' -k GSR-DB_V4_cluster-1_taxa.txt GSR-DB_V4_cluster-1_seqs.fasta &gt; tmp_1.fa\n\n[INFO] read key-value file: GSR-DB_V4_cluster-1_taxa.txt\n[INFO] 38802 pairs of key-value loaded\nHere is what the first few entries look like.\n&gt;k__Bacteria; p__Actinobacteria; c__Actinomycetia; o__Actinomycetales-Streptomycetales-Unknown; f__Streptomycetaceae-Unknown; g__Streptomyces-Unknown; s__Unknown\nTACGTAGGGCGCAAGCGTTGTCCGGAATTATTGGGCGTAAAGAGCTCGTAGGCGGCTTGTCACGTCGGTTGTGAAAGCCCGGGGCTTAACCCCGGGTCTGCAGTCGATACGGGCAGGCTAGAGTTCGGTAGGGGAGATCGGAATTCCTGGTGTAGCGGTGAAATGCGCAGATATCAGGAGGAACACCGGTGGCGAAGGCGGATCTCTGGGCCGATACTGACGCTGAGGAGCGAAAGCGTGGGGAGCGAACAGG\n&gt;k__Bacteria; p__Actinobacteria; c__Actinomycetia; o__Actinomycetales-Streptomycetales-Unknown; f__Streptomycetaceae-Unknown; g__Kitasatospora-Streptomyces-Unknown; s__Unknown\nTACGTAGGGCGCGAGCGTTGTCCGGAATTATTGGGCGTAAAGAGCTCGTAGGCGGCTTGTCACGTCGGTTGTGAAAGCCCGGGGCTTAACCCCGGGTCTGCAGTCGATACGGGCAGGCTAGAGTTCGGTAGGGGAGATCGGAATTCCTGGTGTAGCGGTGAAATGCGCAGATATCAGGAGGAACACCGGTGGCGAAGGCGGATCTCTGGGCCGATACTGACGCTGAGGAGCGAAAGCGTGGGGAGCGAACAGGATTAGATACCCTGGTAGTCCACGCCGTAAACGGTGGGCACTAGGTGTAGG\nNext we tidy up the deflines to remove spaces and leading taxonomic rank designations.\n\nseqkit replace -w 0  -p \" s__.*\" -r ''  tmp_1.fa &gt; tmp_2.fa\nseqkit replace -w 0  -p \"\\s\" -r ''  tmp_2.fa &gt; tmp_3.fa\nseqkit replace -w 0  -p \"\\w__\" -r ''  tmp_3.fa &gt; gsrdb_dada2.fa\nrm tmp_*\n\nAnd here are the first few lines of the final formatted GSR-DB fasta file.\n&gt;Bacteria;Actinobacteria;Actinomycetia;Actinomycetales-Streptomycetales-Unknown;Streptomycetaceae-Unknown;Streptomyces-Unknown;\nTACGTAGGGCGCAAGCGTTGTCCGGAATTATTGGGCGTAAAGAGCTCGTAGGCGGCTTGTCACGTCGGTTGTGAAAGCCCGGGGCTTAACCCCGGGTCTGCAGTCGATACGGGCAGGCTAGAGTTCGGTAGGGGAGATCGGAATTCCTGGTGTAGCGGTGAAATGCGCAGATATCAGGAGGAACACCGGTGGCGAAGGCGGATCTCTGGGCCGATACTGACGCTGAGGAGCGAAAGCGTGGGGAGCGAACAGG\n&gt;Bacteria;Actinobacteria;Actinomycetia;Actinomycetales-Streptomycetales-Unknown;Streptomycetaceae-Unknown;Kitasatospora-Streptomyces-Unknown;\nTACGTAGGGCGCGAGCGTTGTCCGGAATTATTGGGCGTAAAGAGCTCGTAGGCGGCTTGTCACGTCGGTTGTGAAAGCCCGGGGCTTAACCCCGGGTCTGCAGTCGATACGGGCAGGCTAGAGTTCGGTAGGGGAGATCGGAATTCCTGGTGTAGCGGTGAAATGCGCAGATATCAGGAGGAACACCGGTGGCGAAGGCGGATCTCTGGGCCGATACTGACGCTGAGGAGCGAAAGCGTGGGGAGCGAACAGGATTAGATACCCTGGTAGTCCACGCCGTAAACGGTGGGCACTAGGTGTAGG\nNow we can run the classification step.\n\nseqtab.consensus &lt;- seqtab.trim.nochim.consensus\ntax_gsrdb.consensus &lt;- \n  assignTaxonomy(seqtab.consensus, \n                 \"TAXONOMY_FILES/gsrdb_dada2.fa\",\n                 multithread = TRUE, \n                 verbose = TRUE)\nsaveRDS(tax_gsrdb.consensus, \"4.tax_gsrdb.consensus.rds\")",
    "crumbs": [
      "Home",
      "16S rRNA",
      "2. DADA2 ASV Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/dada2/index.html#r-session-information-code",
    "href": "workflows/ssu/dada2/index.html#r-session-information-code",
    "title": "2. DADA2 ASV Workflow",
    "section": "R Session Information & Code",
    "text": "R Session Information & Code\nThis workflow was run on the Smithsonian High Performance Cluster (SI/HPC), Smithsonian Institution. Below are the specific packages and versions used in this workflow using both sessionInfo() and devtools::session_info(). Click the arrow to see the details.\n\nExpand to see R Session Info\nsessionInfo()\nR version 4.3.2 (2023-10-31)\nPlatform: x86_64-conda-linux-gnu (64-bit)\nRunning under: Rocky Linux 8.9 (Green Obsidian)\n\nMatrix products: default\nBLAS/LAPACK: /home/scottjj/miniconda3/envs/R/lib/libopenblasp-r0.3.25.so;  LAPACK version 3.11.0\n\nlocale:\n[1] C\n\ntime zone: America/New_York\ntzcode source: system (glibc)\n\nattached base packages:\n [1] parallel  stats4    grid      stats     graphics  grDevices utils    \n [8] datasets  methods   base     \n\nother attached packages:\n [1] DECIPHER_2.30.0             RSQLite_2.3.4              \n [3] ShortRead_1.60.0            GenomicAlignments_1.38.0   \n [5] SummarizedExperiment_1.32.0 Biobase_2.62.0             \n [7] MatrixGenerics_1.14.0       matrixStats_1.2.0          \n [9] Rsamtools_2.18.0            GenomicRanges_1.54.1       \n[11] Biostrings_2.70.1           GenomeInfoDb_1.38.2        \n[13] XVector_0.42.0              IRanges_2.36.0             \n[15] S4Vectors_0.40.2            BiocParallel_1.36.0        \n[17] BiocGenerics_0.48.1         decontam_1.22.0            \n[19] dplyr_1.1.4                 gridExtra_2.3              \n[21] phyloseq_1.46.0             ff_4.0.9                   \n[23] bit_4.0.5                   ggplot2_3.4.4              \n[25] dada2_1.30.0                Rcpp_1.0.11                \n\nloaded via a namespace (and not attached):\n [1] DBI_1.2.0               bitops_1.0-7            deldir_2.0-2           \n [4] permute_0.9-7           rlang_1.1.2             magrittr_2.0.3         \n [7] ade4_1.7-22             compiler_4.3.2          mgcv_1.9-1             \n[10] systemfonts_1.0.5       png_0.1-8               vctrs_0.6.5            \n[13] reshape2_1.4.4          stringr_1.5.1           pkgconfig_2.0.3        \n[16] crayon_1.5.2            fastmap_1.1.1           labeling_0.4.3         \n[19] utf8_1.2.4              ragg_1.2.7              zlibbioc_1.48.0        \n[22] cachem_1.0.8            jsonlite_1.8.8          biomformat_1.30.0      \n[25] blob_1.2.4              rhdf5filters_1.14.1     DelayedArray_0.28.0    \n[28] Rhdf5lib_1.24.1         jpeg_0.1-10             cluster_2.1.6          \n[31] R6_2.5.1                stringi_1.8.3           RColorBrewer_1.1-3     \n[34] iterators_1.0.14        Matrix_1.6-4            splines_4.3.2          \n[37] igraph_1.6.0            tidyselect_1.2.0        abind_1.4-5            \n[40] vegan_2.6-4             codetools_0.2-19        hwriter_1.3.2.1        \n[43] lattice_0.22-5          tibble_3.2.1            plyr_1.8.9             \n[46] withr_2.5.2             survival_3.5-7          RcppParallel_5.1.7     \n[49] pillar_1.9.0            foreach_1.5.2           generics_0.1.3         \n[52] RCurl_1.98-1.13         munsell_0.5.0           scales_1.3.0           \n[55] glue_1.6.2              tools_4.3.2             interp_1.1-5           \n[58] data.table_1.14.10      rhdf5_2.46.1            ape_5.7-1              \n[61] latticeExtra_0.6-30     colorspace_2.1-0        nlme_3.1-164           \n[64] GenomeInfoDbData_1.2.11 cli_3.6.2               textshaping_0.3.7      \n[67] fansi_1.0.6             S4Arrays_1.2.0          gtable_0.3.4           \n[70] digest_0.6.33           SparseArray_1.2.2       farver_2.1.1           \n[73] memoise_2.0.1           multtest_2.58.0         lifecycle_1.0.4        \n[76] bit64_4.0.5             MASS_7.3-60            \n\ndevtools::session_info()\n- Session info ---------------------------------------------------------------\n setting  value\n version  R version 4.3.2 (2023-10-31)\n os       Rocky Linux 8.9 (Green Obsidian)\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  C\n ctype    C\n tz       America/New_York\n date     2024-08-15\n pandoc   3.1.3 @ /home/scottjj/miniconda3/envs/R/bin/pandoc\n\n- Packages -------------------------------------------------------------------\n package              * version   date (UTC) lib source\n abind                  1.4-5     2016-07-21 [1] CRAN (R 4.3.2)\n ade4                   1.7-22    2023-02-06 [1] CRAN (R 4.3.2)\n ape                    5.7-1     2023-03-13 [1] CRAN (R 4.3.2)\n Biobase              * 2.62.0    2023-10-24 [1] Bioconductor\n BiocGenerics         * 0.48.1    2023-11-01 [1] Bioconductor\n BiocParallel         * 1.36.0    2023-10-24 [1] Bioconductor\n biomformat             1.30.0    2023-10-24 [1] Bioconductor\n Biostrings           * 2.70.1    2023-10-25 [1] Bioconductor\n bit                  * 4.0.5     2022-11-15 [1] CRAN (R 4.3.0)\n bit64                  4.0.5     2020-08-30 [1] CRAN (R 4.3.0)\n bitops                 1.0-7     2021-04-24 [1] CRAN (R 4.3.2)\n blob                   1.2.4     2023-03-17 [1] CRAN (R 4.3.0)\n cachem                 1.0.8     2023-05-01 [1] CRAN (R 4.3.0)\n cli                    3.6.2     2023-12-11 [1] CRAN (R 4.3.2)\n cluster                2.1.6     2023-12-01 [1] CRAN (R 4.3.2)\n codetools              0.2-19    2023-02-01 [1] CRAN (R 4.3.0)\n colorspace             2.1-0     2023-01-23 [1] CRAN (R 4.3.0)\n crayon                 1.5.2     2022-09-29 [1] CRAN (R 4.3.0)\n dada2                * 1.30.0    2023-10-24 [1] Bioconductor\n data.table             1.14.10   2023-12-08 [1] CRAN (R 4.3.2)\n DBI                    1.2.0     2023-12-21 [1] CRAN (R 4.3.2)\n DECIPHER             * 2.30.0    2023-10-24 [1] Bioconductor\n decontam             * 1.22.0    2023-10-24 [1] Bioconductor\n DelayedArray           0.28.0    2023-10-24 [1] Bioconductor\n deldir                 2.0-2     2023-11-23 [1] CRAN (R 4.3.2)\n devtools               2.4.5     2022-10-11 [1] CRAN (R 4.3.2)\n digest                 0.6.33    2023-07-07 [1] CRAN (R 4.3.0)\n dplyr                * 1.1.4     2023-11-17 [1] CRAN (R 4.3.2)\n ellipsis               0.3.2     2021-04-29 [1] CRAN (R 4.3.0)\n fansi                  1.0.6     2023-12-08 [1] CRAN (R 4.3.2)\n farver                 2.1.1     2022-07-06 [1] CRAN (R 4.3.0)\n fastmap                1.1.1     2023-02-24 [1] CRAN (R 4.3.0)\n ff                   * 4.0.9     2023-01-25 [1] CRAN (R 4.3.2)\n foreach                1.5.2     2022-02-02 [1] CRAN (R 4.3.0)\n fs                     1.6.3     2023-07-20 [1] CRAN (R 4.3.1)\n generics               0.1.3     2022-07-05 [1] CRAN (R 4.3.0)\n GenomeInfoDb         * 1.38.2    2023-12-13 [1] Bioconductor 3.18 (R 4.3.2)\n GenomeInfoDbData       1.2.11    2023-12-26 [1] Bioconductor\n GenomicAlignments    * 1.38.0    2023-10-24 [1] Bioconductor\n GenomicRanges        * 1.54.1    2023-10-29 [1] Bioconductor\n ggplot2              * 3.4.4     2023-10-12 [1] CRAN (R 4.3.1)\n glue                   1.6.2     2022-02-24 [1] CRAN (R 4.3.0)\n gridExtra            * 2.3       2017-09-09 [1] CRAN (R 4.3.2)\n gtable                 0.3.4     2023-08-21 [1] CRAN (R 4.3.1)\n htmltools              0.5.7     2023-11-03 [1] CRAN (R 4.3.2)\n htmlwidgets            1.6.4     2023-12-06 [1] CRAN (R 4.3.2)\n httpuv                 1.6.13    2023-12-06 [1] CRAN (R 4.3.2)\n hwriter                1.3.2.1   2022-04-08 [1] CRAN (R 4.3.2)\n igraph                 1.6.0     2023-12-11 [1] CRAN (R 4.3.2)\n interp                 1.1-5     2023-11-27 [1] CRAN (R 4.3.2)\n IRanges              * 2.36.0    2023-10-24 [1] Bioconductor\n iterators              1.0.14    2022-02-05 [1] CRAN (R 4.3.0)\n jpeg                   0.1-10    2022-11-29 [1] CRAN (R 4.3.2)\n jsonlite               1.8.8     2023-12-04 [1] CRAN (R 4.3.2)\n labeling               0.4.3     2023-08-29 [1] CRAN (R 4.3.1)\n later                  1.3.2     2023-12-06 [1] CRAN (R 4.3.2)\n lattice                0.22-5    2023-10-24 [1] CRAN (R 4.3.1)\n latticeExtra           0.6-30    2022-07-04 [1] CRAN (R 4.3.2)\n lifecycle              1.0.4     2023-11-07 [1] CRAN (R 4.3.2)\n magrittr               2.0.3     2022-03-30 [1] CRAN (R 4.3.0)\n MASS                   7.3-60    2023-05-04 [1] CRAN (R 4.3.0)\n Matrix                 1.6-4     2023-11-30 [1] CRAN (R 4.3.2)\n MatrixGenerics       * 1.14.0    2023-10-24 [1] Bioconductor\n matrixStats          * 1.2.0     2023-12-11 [1] CRAN (R 4.3.2)\n memoise                2.0.1     2021-11-26 [1] CRAN (R 4.3.0)\n mgcv                   1.9-1     2023-12-21 [1] CRAN (R 4.3.2)\n mime                   0.12      2021-09-28 [1] CRAN (R 4.3.0)\n miniUI                 0.1.1.1   2018-05-18 [1] CRAN (R 4.3.2)\n multtest               2.58.0    2023-10-24 [1] Bioconductor\n munsell                0.5.0     2018-06-12 [1] CRAN (R 4.3.0)\n nlme                   3.1-164   2023-11-27 [1] CRAN (R 4.3.2)\n permute                0.9-7     2022-01-27 [1] CRAN (R 4.3.2)\n phyloseq             * 1.46.0    2023-10-24 [1] Bioconductor\n pillar                 1.9.0     2023-03-22 [1] CRAN (R 4.3.0)\n pkgbuild               1.4.3     2023-12-10 [1] CRAN (R 4.3.2)\n pkgconfig              2.0.3     2019-09-22 [1] CRAN (R 4.3.0)\n pkgload                1.3.3     2023-09-22 [1] CRAN (R 4.3.2)\n plyr                   1.8.9     2023-10-02 [1] CRAN (R 4.3.1)\n png                    0.1-8     2022-11-29 [1] CRAN (R 4.3.2)\n profvis                0.3.8     2023-05-02 [1] CRAN (R 4.3.2)\n promises               1.2.1     2023-08-10 [1] CRAN (R 4.3.1)\n purrr                  1.0.2     2023-08-10 [1] CRAN (R 4.3.1)\n R6                     2.5.1     2021-08-19 [1] CRAN (R 4.3.0)\n ragg                   1.2.7     2023-12-11 [1] CRAN (R 4.3.2)\n RColorBrewer           1.1-3     2022-04-03 [1] CRAN (R 4.3.0)\n Rcpp                 * 1.0.11    2023-07-06 [1] CRAN (R 4.3.0)\n RcppParallel           5.1.7     2023-02-27 [1] CRAN (R 4.3.2)\n RCurl                  1.98-1.13 2023-11-02 [1] CRAN (R 4.3.2)\n remotes                2.4.2.1   2023-07-18 [1] CRAN (R 4.3.2)\n reshape2               1.4.4     2020-04-09 [1] CRAN (R 4.3.0)\n rhdf5                  2.46.1    2023-11-29 [1] Bioconductor 3.18 (R 4.3.2)\n rhdf5filters           1.14.1    2023-11-06 [1] Bioconductor\n Rhdf5lib               1.24.1    2023-12-11 [1] Bioconductor 3.18 (R 4.3.2)\n rlang                  1.1.2     2023-11-04 [1] CRAN (R 4.3.2)\n Rsamtools            * 2.18.0    2023-10-24 [1] Bioconductor\n RSQLite              * 2.3.4     2023-12-08 [1] CRAN (R 4.3.2)\n S4Arrays               1.2.0     2023-10-24 [1] Bioconductor\n S4Vectors            * 0.40.2    2023-11-23 [1] Bioconductor 3.18 (R 4.3.2)\n scales                 1.3.0     2023-11-28 [1] CRAN (R 4.3.2)\n sessioninfo            1.2.2     2021-12-06 [1] CRAN (R 4.3.2)\n shiny                  1.8.0     2023-11-17 [1] CRAN (R 4.3.2)\n ShortRead            * 1.60.0    2023-10-24 [1] Bioconductor\n SparseArray            1.2.2     2023-11-07 [1] Bioconductor\n stringi                1.8.3     2023-12-11 [1] CRAN (R 4.3.2)\n stringr                1.5.1     2023-11-14 [1] CRAN (R 4.3.2)\n SummarizedExperiment * 1.32.0    2023-10-24 [1] Bioconductor\n survival               3.5-7     2023-08-14 [1] CRAN (R 4.3.1)\n systemfonts            1.0.5     2023-10-09 [1] CRAN (R 4.3.1)\n textshaping            0.3.7     2023-10-09 [1] CRAN (R 4.3.1)\n tibble                 3.2.1     2023-03-20 [1] CRAN (R 4.3.0)\n tidyselect             1.2.0     2022-10-10 [1] CRAN (R 4.3.0)\n urlchecker             1.0.1     2021-11-30 [1] CRAN (R 4.3.2)\n usethis                2.2.2     2023-07-06 [1] CRAN (R 4.3.0)\n utf8                   1.2.4     2023-10-22 [1] CRAN (R 4.3.1)\n vctrs                  0.6.5     2023-12-01 [1] CRAN (R 4.3.2)\n vegan                  2.6-4     2022-10-11 [1] CRAN (R 4.3.2)\n withr                  2.5.2     2023-10-30 [1] CRAN (R 4.3.1)\n xtable                 1.8-4     2019-04-21 [1] CRAN (R 4.3.0)\n XVector              * 0.42.0    2023-10-24 [1] Bioconductor\n zlibbioc               1.48.0    2023-10-24 [1] Bioconductor",
    "crumbs": [
      "Home",
      "16S rRNA",
      "2. DADA2 ASV Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/dada2/index.html#read-counts-assessment",
    "href": "workflows/ssu/dada2/index.html#read-counts-assessment",
    "title": "2. DADA2 ASV Workflow",
    "section": "Read Counts Assessment",
    "text": "Read Counts Assessment\nBefore we begin, let’s create a summary table containing some basic sample metadata and the read count data from the Sample Data section of the workflow. We want to inspect how total reads changed through the workflow. Table headers are as follows:\n\n\n\n\n\n\nHeader\nDescription\n\n\n\nSample ID\nNew sample ID based on Ocean, species, tissue, & unique ID\n\n\ninput rc\nNo. of raw reads\n\n\nfinal rc\nFinal read count after removing chimeras\n\n\nper reads retain\nPercent of reads remaining from input to final rc\n\n\n\ntotal ASVs\nNo. of ASVs\n\n\nOcean\nSampling ocean\n\n\nMorphospecies\nHost shrimp species\n\n\nTissue\nShrimp tissue type\n\n\nHabitat\nSampling habitat\n\n\nSite\nSampling site\n\n\nTaxon\nShrimp, environmental samples, Controls\n\n\nSpecies_Pair\nASK MATT\n\n\nSpecies_group\nASK MATT\n\n\nSpecies_complex\nASK MATT\n\n\nRun\nSequencing Run\n\n\nPlate\nPlate name",
    "crumbs": [
      "Home",
      "16S rRNA",
      "2. DADA2 ASV Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/dada2/index.html#prep-data-for-microeco",
    "href": "workflows/ssu/dada2/index.html#prep-data-for-microeco",
    "title": "2. DADA2 ASV Workflow",
    "section": "Prep Data for microeco\n",
    "text": "Prep Data for microeco\n\nLike any tool, the microeco package needs the data in a specific form. I formatted our data to match the mock data in the microeco tutorial, specifically this section.\nA. Taxonomy Table\nHere is what the taxonomy table looks like in the mock data.\n\n\n\n---------------------------------------------------------------------\n  &nbsp;      Kingdom           Phylum                 Class         \n---------- ------------- -------------------- -----------------------\n  OTU_50    k__Bacteria   p__Proteobacteria    c__Betaproteobacteria \n\n OTU_8058   k__Bacteria   p__Actinobacteria      c__Actinobacteria   \n\n OTU_7152   k__Bacteria   p__Verrucomicrobia    c__OPB35 soil group  \n---------------------------------------------------------------------\n\n\nAnd here is the taxonomy table from the dada2 workflow. The name dada2 gives to each ASV is the actual sequence of the ASV. We need to change this for downstream analyses.\n\n\n\n\n\n\nNoteExpand to see the partial content of the dada2 taxonomy table\n\n\n\n\n\n\n\n\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n                                                                                                                            &nbsp;                                                                                                                               Kingdom  \n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ----------\n TACGGAGGGTGCAAGCGTTAATCGGAATTACTGGGCGTAAAGCGCTCGTAGGCGGCACGCTAAGTCGGATGTGAAATCCCAGGGCTTAACCTTGGAACTGCATTCGATACTGGCGAGCTAGAGTGTGTGAGAGGGTAGTGGAATTCCAGGTGTAGCGGTGAAATGCGTAGATATCTGGAGGAACATCAGTGGCGAAGGCGACTACCTGGCACAACACTGACGCTGAGGAGCGAAAGCGTGGGGAGCAAACAGG   Bacteria \n\n TACGGAGGGTGCGAGCGTTAATCGGAATTACTGGGCGTAAAGCGCATGCAGGTGGTTTGTTAAGTCAGATGTGAAAGCCCGGGGCTCAACCTCGGAATAGCATTTGAAACTGGCAGACTAGAGTACTGTAGAGGGGGGTAGAATTTCAGGTGTAGCGGTGAAATGCGTAGAGATCTGAAGGAATACCGGTGGCGAAGGCGGCCCCCTGGACAGATACTGACACTCAGATGCGAAAGCGTGGGGAGCAAACAGG   Bacteria \n\n TACATAGGTGTCAAGCGTTATCCGGATTTATTGGGCGTAAAGAGTGCGCAGATGGTTTAATAAGTTTGGGGTTAAATGCTAAAGCTTAACTTTAGTATGCCTTGAAAACTGTTTTACTAGAGTGTGGTAGAAGTTGATGGAATTTCATGTGTAGCGGTGGAATGCGTAGATATATGAAGGAACACCAATGGCGAAGGCAATCAACTATGCCATTACTGACATTCATGCACGAAAGCGTGGGGAGCGAATAGG    Bacteria \n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nTable: Table continues below\n\n \n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n                                                                                                                            &nbsp;                                                                                                                                   Phylum     \n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ----------------\n TACGGAGGGTGCAAGCGTTAATCGGAATTACTGGGCGTAAAGCGCTCGTAGGCGGCACGCTAAGTCGGATGTGAAATCCCAGGGCTTAACCTTGGAACTGCATTCGATACTGGCGAGCTAGAGTGTGTGAGAGGGTAGTGGAATTCCAGGTGTAGCGGTGAAATGCGTAGATATCTGGAGGAACATCAGTGGCGAAGGCGACTACCTGGCACAACACTGACGCTGAGGAGCGAAAGCGTGGGGAGCAAACAGG   Proteobacteria \n\n TACGGAGGGTGCGAGCGTTAATCGGAATTACTGGGCGTAAAGCGCATGCAGGTGGTTTGTTAAGTCAGATGTGAAAGCCCGGGGCTCAACCTCGGAATAGCATTTGAAACTGGCAGACTAGAGTACTGTAGAGGGGGGTAGAATTTCAGGTGTAGCGGTGAAATGCGTAGAGATCTGAAGGAATACCGGTGGCGAAGGCGGCCCCCTGGACAGATACTGACACTCAGATGCGAAAGCGTGGGGAGCAAACAGG   Proteobacteria \n\n TACATAGGTGTCAAGCGTTATCCGGATTTATTGGGCGTAAAGAGTGCGCAGATGGTTTAATAAGTTTGGGGTTAAATGCTAAAGCTTAACTTTAGTATGCCTTGAAAACTGTTTTACTAGAGTGTGGTAGAAGTTGATGGAATTTCATGTGTAGCGGTGGAATGCGTAGATATATGAAGGAACACCAATGGCGAAGGCAATCAACTATGCCATTACTGACATTCATGCACGAAAGCGTGGGGAGCGAATAGG          NA       \n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nTable: Table continues below\n\n \n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n                                                                                                                            &nbsp;                                                                                                                                      Class        \n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ---------------------\n TACGGAGGGTGCAAGCGTTAATCGGAATTACTGGGCGTAAAGCGCTCGTAGGCGGCACGCTAAGTCGGATGTGAAATCCCAGGGCTTAACCTTGGAACTGCATTCGATACTGGCGAGCTAGAGTGTGTGAGAGGGTAGTGGAATTCCAGGTGTAGCGGTGAAATGCGTAGATATCTGGAGGAACATCAGTGGCGAAGGCGACTACCTGGCACAACACTGACGCTGAGGAGCGAAAGCGTGGGGAGCAAACAGG   Gammaproteobacteria \n\n TACGGAGGGTGCGAGCGTTAATCGGAATTACTGGGCGTAAAGCGCATGCAGGTGGTTTGTTAAGTCAGATGTGAAAGCCCGGGGCTCAACCTCGGAATAGCATTTGAAACTGGCAGACTAGAGTACTGTAGAGGGGGGTAGAATTTCAGGTGTAGCGGTGAAATGCGTAGAGATCTGAAGGAATACCGGTGGCGAAGGCGGCCCCCTGGACAGATACTGACACTCAGATGCGAAAGCGTGGGGAGCAAACAGG   Gammaproteobacteria \n\n TACATAGGTGTCAAGCGTTATCCGGATTTATTGGGCGTAAAGAGTGCGCAGATGGTTTAATAAGTTTGGGGTTAAATGCTAAAGCTTAACTTTAGTATGCCTTGAAAACTGTTTTACTAGAGTGTGGTAGAAGTTGATGGAATTTCATGTGTAGCGGTGGAATGCGTAGATATATGAAGGAACACCAATGGCGAAGGCAATCAACTATGCCATTACTGACATTCATGCACGAAAGCGTGGGGAGCGAATAGG            NA          \n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n\n\n\n\nThe first step is to rename the amplicon sequence variants so the designations are a bit more user friendly. By default, DADA2 names each ASV by its unique sequence so that data can be directly compared across studies (which is great). But this convention can get cumbersome downstream, so we rename the ASVs using a simpler convention—ASV1, ASV2, ASV3, and so on.\n\ntmp_tax &lt;- data.frame(tax_gsrdb)\n# adding unique ASV names\nrow.names(tmp_tax) &lt;- paste0(\"ASV\", seq(nrow(tmp_tax)))\n\nAnd this is how the taxonomy table looks after assigning new names.\n\n\n\n----------------------------------------------------------\n &nbsp;   Kingdom        Phylum              Class        \n-------- ---------- ---------------- ---------------------\n  ASV1    Bacteria   Proteobacteria   Gammaproteobacteria \n\n  ASV2    Bacteria   Proteobacteria   Gammaproteobacteria \n\n  ASV3    Bacteria         NA                 NA          \n----------------------------------------------------------\n\n\nNext we need to add rank definitions to each classification.\n\nrank_prefixes &lt;- c(\n  Kingdom = \"k\", \n  Phylum  = \"p\", \n  Class   = \"c\", \n  Order   = \"o\", \n  Family  = \"f\", \n  Genus   = \"g\"\n)\n\ntmp_tax &lt;- tmp_tax %&gt;%\n  mutate(across(everything(), ~replace_na(., \"\"))) %&gt;% \n  mutate(across(names(rank_prefixes), \n                ~ paste0(rank_prefixes[cur_column()], \"__\", .))) %&gt;%\ntidy_taxonomy()\n\nAnd then this. Exactly like the mock data.\n\n\n\n-------------------------------------------------------------------\n &nbsp;     Kingdom          Phylum                 Class          \n-------- ------------- ------------------- ------------------------\n  ASV1    k__Bacteria   p__Proteobacteria   c__Gammaproteobacteria \n\n  ASV2    k__Bacteria   p__Proteobacteria   c__Gammaproteobacteria \n\n  ASV3    k__Bacteria          p__                   c__           \n-------------------------------------------------------------------\n\n\nB. Sequence Table\nHere is what the sequence table looks like in the mock data.\n\n\n\n--------------------------------------------------------------------\n  &nbsp;    S1   S2   S3   S4   S5   S6   S7   S9   S10   S11   S12 \n---------- ---- ---- ---- ---- ---- ---- ---- ---- ----- ----- -----\n OTU_4272   1    0    1    1    0    0    1    1     0     1     1  \n\n OTU_236    1    4    0    2    35   5    94   0    177   14    27  \n\n OTU_399    9    2    2    4    4    0    3    6     0     1     2  \n--------------------------------------------------------------------\n\n\n\ntmp_st &lt;- data.frame(seqtab)\nidentical(colnames(tmp_st), row.names(tax_gsrdb))\nnames(tmp_st) &lt;- row.names(tmp_tax)\n\ntmp_st &lt;-  tmp_st %&gt;% tibble::rownames_to_column()\n\ntmp_st &lt;- tmp_st %&gt;%\n  tidyr::pivot_longer(cols = c(-1), names_to = \"tmp\") %&gt;%\n  tidyr::pivot_wider(names_from = c(1))\ntmp_st &lt;- tibble::column_to_rownames(tmp_st, \"tmp\")\n\nAnd now the final, modified sequence table.\n\n\n\n--------------------------------------------------------------------\n &nbsp;   EP_A_AREN_MG_8652   EP_A_AREN_MG_8654   EP_A_AREN_MG_8697 \n-------- ------------------- ------------------- -------------------\n  ASV1            7                   0                   0         \n\n  ASV2          1113                1803                 31         \n\n  ASV3            0                   0                   0         \n--------------------------------------------------------------------\n\n\nC. Sample Table\nHere is what the sample table looks like in the mock data.\n\npandoc.table(sample_info_16S[1:3,], emphasize.rownames = FALSE)\n\n\n----------------------------------------------------\n &nbsp;   SampleID   Group   Type       Saline      \n-------- ---------- ------- ------ -----------------\n   S1        S1       IW      NE    Non-saline soil \n\n   S2        S2       IW      NE    Non-saline soil \n\n   S3        S3       IW      NE    Non-saline soil \n----------------------------------------------------\n\n\n\nsamdf &lt;- readRDS(here(\"share/ssu/sampledata/\", \"sample_data.rds\"))\nsamdf &lt;- samdf %&gt;% tibble::column_to_rownames(\"SampleID\")\nsamdf$SampleID &lt;- rownames(samdf)\nsamdf &lt;- samdf %&gt;% relocate(SampleID)\n\nAnd now a partial view of the final, modified sample table.\n\n\n\n-------------------------------------------------------------------------\n      &nbsp;             SampleID        OCEAN   SPECIES   TISSUE    ID  \n------------------- ------------------- ------- --------- -------- ------\n EP_A_AREN_EG_8651   EP_A_AREN_EG_8651    EP     A_AREN      EG     8651 \n\n EP_A_AREN_EG_8654   EP_A_AREN_EG_8654    EP     A_AREN      EG     8654 \n\n EP_A_AREN_EG_8698   EP_A_AREN_EG_8698    EP     A_AREN      EG     8698 \n-------------------------------------------------------------------------",
    "crumbs": [
      "Home",
      "16S rRNA",
      "2. DADA2 ASV Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/dada2/index.html#experiment-level-objects",
    "href": "workflows/ssu/dada2/index.html#experiment-level-objects",
    "title": "2. DADA2 ASV Workflow",
    "section": "Experiment-level Objects",
    "text": "Experiment-level Objects\nIn the following section we create microtable and phyloseq objects. These are single, consolidated experiment-level data objects that organize various data types, such as OTU tables (taxa abundance), sample metadata, taxonomic classifications, as well as phylogenetic trees and reference sequences, into one structured format. This unified approach facilitates reproducible, interactive analysis and visualization by bringing all related information together.\nWe begin by creating the microtable and then use the function meco2phyloseq from the file2meco package to create the phyloseq object. This way all of the underlying data is identical across the two objects.\n\n\n\n\n\n\nNote\n\n\n\nThese objects contain an ASV table (taxa abundances), sample metadata, and taxonomy table (mapping between ASVs and higher-level taxonomic classifications).\n\n\n\nsample_info &lt;- samdf\ntax_tab &lt;- tmp_tax\notu_tab &lt;- tmp_st\n\n\ntmp_me &lt;- microtable$new(sample_table = sample_info, \n                         otu_table = otu_tab, \n                         tax_table = tax_tab)\ntmp_me\n\nmicrotable-class object:\nsample_table have 1909 rows and 13 columns\notu_table have 72851 rows and 1909 columns\ntax_table have 72851 rows and 6 columns\nAdd Representative Sequence\nWe can also add representative sequences for each OTU/ASV. For this step, we can simply grab the sequences from the row names of the DADA2 taxonomy object loaded above.\n\ntmp_seq &lt;- data.frame(row.names(data.frame(tax_gsrdb)) )\ntmp_names &lt;- data.frame(row.names(tax_tab))\ntmp_fa &lt;- cbind(tmp_names, tmp_seq)\ncolnames(tmp_fa) &lt;- c(\"ASV_ID\", \"ASV_SEQ\")\ntmp_fa$ASV_ID &lt;- sub(\"^\", \"&gt;\", tmp_fa$ASV_ID)\n\nwrite.table(tmp_fa, here(work_here, \"rep_seq.fasta\"),\n            sep = \"\\n\", col.names = FALSE, row.names = FALSE,\n            quote = FALSE, fileEncoding = \"UTF-8\")       \nrep_fasta &lt;- Biostrings::readDNAStringSet(here(work_here, \"rep_seq.fasta\"))\ntmp_me$rep_fasta &lt;- rep_fasta\ntmp_me$tidy_dataset()",
    "crumbs": [
      "Home",
      "16S rRNA",
      "2. DADA2 ASV Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/dada2/index.html#curate-the-data-set",
    "href": "workflows/ssu/dada2/index.html#curate-the-data-set",
    "title": "2. DADA2 ASV Workflow",
    "section": "Curate the Data Set",
    "text": "Curate the Data Set\nPretty much the last thing to do is remove unwanted taxa, negative controls, and low-count samples.\nRemove any Kingdom NAs\nHere we can just use the straight up subset command since we do not need to worry about any ranks above Kingdom also being removed.\n\ntmp_no_na &lt;- microeco::clone(tmp_me)\ntmp_no_na$tax_table %&lt;&gt;% base::subset(Kingdom == \"k__Archaea\" | Kingdom == \"k__Bacteria\")\ntmp_no_na$tidy_dataset()\n\n\n\nmicrotable-class object:\nsample_table have 1909 rows and 14 columns\notu_table have 72827 rows and 1909 columns\ntax_table have 72827 rows and 6 columns\nrep_fasta have 72827 sequences\n\n\nRemove Contaminants\nNow we can remove any potential contaminants like mitochondria or chloroplasts.\n\ntmp_no_cont &lt;- microeco::clone(tmp_no_na)\ntmp_no_cont$filter_pollution(taxa = c(\"mitochondria\", \"chloroplast\"))\ntmp_no_cont$tidy_dataset()\ntmp_no_cont\n\nTotal 0 features are removed from tax_table ...\n\n\nmicrotable-class object:\nsample_table have 1909 rows and 14 columns\notu_table have 72827 rows and 1909 columns\ntax_table have 72827 rows and 6 columns\nrep_fasta have 72827 sequences\n\n\nRemove Negative Controls (NC)\nNow we need to remove the NC samples and ASVs found in those sample. We first identified all ASVs that were present in at least one NC sample represented by at least 1 read. We did this by subsetting the NC samples from the new microtable object.\n\ntmp_nc &lt;- microeco::clone(tmp_no_cont)\ntmp_nc$sample_table &lt;- subset(tmp_nc$sample_table, TAXON == \"Control\")\ntmp_nc$tidy_dataset()\ntmp_nc\n\n72231 taxa with 0 abundance are removed from the otu_table ...\n\n\nmicrotable-class object:\nsample_table have 60 rows and 14 columns\notu_table have 596 rows and 60 columns\ntax_table have 596 rows and 6 columns\nrep_fasta have 596 sequences\n\n\nLooks like there are 596 ASVs in the NC samples from a total of 205834 reads.\n\nnc_asvs &lt;- row.names(tmp_nc$tax_table)\n\n\n\n [1] \"ASV1\"  \"ASV2\"  \"ASV4\"  \"ASV5\"  \"ASV6\"  \"ASV7\"  \"ASV10\" \"ASV11\" \"ASV13\"\n[10] \"ASV14\" \"ASV15\" \"ASV16\" \"ASV17\" \"ASV18\" \"ASV20\" \"ASV22\" \"ASV23\" \"ASV26\"\n[19] \"ASV28\" \"ASV31\"\n\n\nThere are 596 ASVs found in the NC sample. ASVs are numbered in order by total abundance in the data set so we know that many of the ASVs in the NC samples are not particularly abundant in the dataset. We can look at the abundance of these ASVs across all samples and compare it to the NC. This takes a bit of wrangling.\nEssentially, for each ASV, the code below calculates:\n\nThe total number of NC samples containing at least 1 read.\n\nThe total number of reads in NC samples.\n\nThe total number of non-NC samples containing at least 1 read.\n\nThe total number of reads in non-NC samples.\n\nThe percent of reads in the NC samples and the percent of NC samples containing reads.\n\n\ntmp_rem_nc &lt;- microeco::clone(tmp_no_cont)\ntmp_rem_nc_df &lt;- tmp_rem_nc$otu_table\ntmp_rem_nc_df &lt;- tmp_rem_nc_df %&gt;% \n                 dplyr::filter(row.names(tmp_rem_nc_df) %in% nc_asvs)\ntmp_rem_nc_df &lt;- tmp_rem_nc_df %&gt;% tibble::rownames_to_column(\"ASV_ID\")\n\n\n#-------provide a string unique to NC samples--------------#\nnc_name &lt;- \"Control_\"\n#----------------------------------------------------------#\ntmp_rem_nc_df &lt;- tmp_rem_nc_df  %&gt;% \n  dplyr::mutate(total_reads_NC = rowSums(dplyr::select(., contains(nc_name))), \n         .after = \"ASV_ID\")\ntmp_rem_nc_df &lt;- dplyr::select(tmp_rem_nc_df, -contains(nc_name))\ntmp_rem_nc_df &lt;- tmp_rem_nc_df %&gt;%\n  dplyr::mutate(total_reads_samps = rowSums(.[3:ncol(tmp_rem_nc_df)]), \n                .after = \"total_reads_NC\")\ntmp_rem_nc_df[, 4:ncol(tmp_rem_nc_df)] &lt;- list(NULL)\ntmp_rem_nc_df &lt;- tmp_rem_nc_df %&gt;%\n  dplyr::mutate(perc_in_neg = 100*(\n    total_reads_NC / (total_reads_NC + total_reads_samps)),\n                .after = \"total_reads_samps\")\n\n\ntmp_rem_nc_df$perc_in_neg &lt;- round(tmp_rem_nc_df$perc_in_neg, digits = 6)\n\ntmp_1 &lt;- data.frame(rowSums(tmp_rem_nc$otu_table != 0)) %&gt;% \n                   tibble::rownames_to_column(\"ASV_ID\") %&gt;% \n                   dplyr::rename(\"total_samples\" = 2) \n\ntmp_2 &lt;- dplyr::select(tmp_rem_nc$otu_table, contains(nc_name))\ntmp_2$num_samp_nc &lt;- rowSums(tmp_2 != 0)\ntmp_2 &lt;- dplyr::select(tmp_2, contains(\"num_samp_nc\")) %&gt;% \n                      tibble::rownames_to_column(\"ASV_ID\")\n\ntmp_3 &lt;- dplyr::select(tmp_rem_nc$otu_table, -contains(nc_name))\ntmp_3$num_samp_no_nc &lt;- rowSums(tmp_3 != 0)\ntmp_3 &lt;- dplyr::select(tmp_3, contains(\"num_samp_no_nc\")) %&gt;% \n                      tibble::rownames_to_column(\"ASV_ID\")\n\ntmp_rem_nc_df &lt;- dplyr::left_join(tmp_rem_nc_df, tmp_1) %&gt;%\n                 dplyr::left_join(., tmp_2) %&gt;%\n                 dplyr::left_join(., tmp_3)\n\ntmp_rem_nc_df &lt;- tmp_rem_nc_df %&gt;%\n  dplyr::mutate(perc_in_neg_samp = 100*( num_samp_nc / (num_samp_nc + num_samp_no_nc)),\n                .after = \"num_samp_no_nc\")\n\n\nnc_check &lt;- tmp_rem_nc_df\n\nLooking at these data we can see that ASVs like ASV1, ASV2, and so on, are only represented by a really small number of NC reads and samples. On the other hand, ASVs such as ASV91, ASV121, and ASV299 are relatively abundant in NC samples. We decided to remove ASVs if:\n\nThe number of reads found in NC samples accounted for more than 10% of total reads OR\nThe percent of NC samples containing the ASV was greater than 10% of total samples.\n\n\nnc_remove &lt;- nc_check %&gt;% \n  dplyr::filter(perc_in_neg &gt; 10 | perc_in_neg_samp &gt; 10)\n\n\n\n\n\n\n\n\n\n\n\nTotal ASVs\nNC reads\nnon NC reads\n% NC reads\n\n\n\nRemoved\n371\n170482\n233876\n42.161\n\n\nRetained\n225\n35352\n12394240\n0.284\n\n\n\nWe identified a total of 596 ASVs that were present in at least 1 NC sample by at least 1 read. We removed any ASV where more than 10% of total reads were found in NC samples OR any ASV found in more than 10% of NC samples. Based on these criteria we removed 371 ASVs from the data set, which represented 170482 total reads in NC samples and 233876 total reads in non-NC samples. Of the total reads removed 42.161% came from NC samples. Of all ASVs identified in NC samples,225 were retained because the fell below the threshhold criteria. These ASVs accounted for 35352 reads in NC samples and 12394240 reads in non-NC samples. NC samples accounted for 0.284% of these reads.\nOK, now we can remove the NC samples and any ASVs that met our criteria described above.\n\ntmp_no_nc &lt;- microeco::clone(tmp_no_cont)\n\ntmp_rem_asv &lt;- as.factor(nc_remove$ASV_ID)\ntmp_no_nc$otu_table &lt;- tmp_rem_nc$otu_table %&gt;% \n  filter(!row.names(tmp_no_nc$otu_table) %in% tmp_rem_asv)\ntmp_no_nc$tidy_dataset()\n\ntmp_no_nc$sample_table &lt;- subset(tmp_no_nc$sample_table, \n                                 TAXON != \"Control_\")\ntmp_no_nc$tidy_dataset()\ntmp_no_nc\n\n9 samples with 0 abundance are removed from the otu_table ...\n\n\nmicrotable-class object:\nsample_table have 1900 rows and 14 columns\notu_table have 72456 rows and 1900 columns\ntax_table have 72456 rows and 6 columns\nrep_fasta have 72456 sequences\n\n\nRemove Low-Count Samples\nNext, we can remove samples with really low read counts—here we set the threshold to 1000 reads.\n\ntmp_no_low &lt;- microeco::clone(tmp_no_nc)\ntmp_no_low$otu_table &lt;- tmp_no_nc$otu_table %&gt;%\n          dplyr::select(where(~ is.numeric(.) && sum(.) &gt;= 1000))\ntmp_no_low$tidy_dataset()\ntmp_no_low\n\n26 taxa with 0 abundance are removed from the otu_table ...\n\n\nmicrotable-class object:\nsample_table have 1848 rows and 14 columns\notu_table have 72430 rows and 1848 columns\ntax_table have 72430 rows and 6 columns\nrep_fasta have 72430 sequences\n\n\nGiving us the final microtable object.\n\nme_final &lt;- microeco::clone(tmp_no_low)\n\nLastly, we can use the package file2meco to generate a phyloseq object.\n\nps_final &lt;- file2meco::meco2phyloseq(me_final)",
    "crumbs": [
      "Home",
      "16S rRNA",
      "2. DADA2 ASV Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/dada2/index.html#summary",
    "href": "workflows/ssu/dada2/index.html#summary",
    "title": "2. DADA2 ASV Workflow",
    "section": "Summary",
    "text": "Summary\nNow time to summarize the data. For this we use the R package miaverse (Borman et al. 2024).\n\n\n\n\nme_dataset\ntotal_asvs\ntotal_reads\ntotal_samples\n\n\n\noriginal\n72851\n35764704\n1909\n\n\nno NA kingdoms\n72827\n35760968\n1909\n\n\nno contaminants\n72827\n35760968\n1909\n\n\nno negative controls\n72456\n35356610\n1900\n\n\nno low count samps\n72430\n35345679\n1848\n\n\nfinal\n72430\n35345679\n1848\n\n\n\n\n\n\n\n\nDataset metrics before and after curation.\n\nMetric\nStart\nEnd\n\n\n\nMin. no. of reads\n49\n1202\n\n\nMax. no. of reads\n251022\n249532\n\n\nTotal no. of reads\n35764704\n35345679\n\n\nAvg. no. of reads\n18735\n19126\n\n\nMedian no. of reads\n14827\n15149.500\n\n\nTotal ASVs\n72851\n72430\n\n\nNo. of singleton ASVs\n106\n106\n\n\n% of singleton ASVs\n0.146\n0.146\n\n\nSparsity\n0.996\n0.996\n\n\n\n\n\nWe started off with 72851 ASVs and 1909 samples. After curation of the dataset, there were 72430 ASVs and 1848 samples remaining.\nWe lost a total of 61 samples after curating the dataset. This includes 50 NC samples and 11 non-NC samples.\nHere is a list of non-NC samples that were removed.\n\n\n [1] \"WA_A_BAHA_GL_7474\" \"WA_A_NUTT_EG_9410\" \"WA_A_WEBS_HP_9411\"\n [4] \"WA_A_CRIS_GL_9624\" \"WA_A_CRIS_HP_7536\" \"WA_A_FLOR_GL_9659\"\n [7] \"WA_A_PARA_GL_9474\" \"WA_A_PARA_MG_9364\" \"WA_A_PARA_ST_9477\"\n[10] \"WA_A_PCNS_MG_9513\" \"WA_A_THOM_ST_9389\"",
    "crumbs": [
      "Home",
      "16S rRNA",
      "2. DADA2 ASV Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/dada2/index.html#footnotes",
    "href": "workflows/ssu/dada2/index.html#footnotes",
    "title": "2. DADA2 ASV Workflow",
    "section": "Footnotes",
    "text": "Footnotes\n\nFrom the developers: GSR database (Greengenes, SILVA, and RDP database) is an integrated and manually curated database for bacterial and archaeal 16S amplicon taxonomy analysis. Unlike previous integration approaches, this database creation pipeline includes a taxonomy unification step to ensure consistency in taxonomical annotations. The database was validated with three mock communities and two real datasets and compared with existing 16S databases such as Greengenes, GTDB, ITGDB, SILVA, RDP, and MetaSquare. Results showed that the GSR database enhances taxonomical annotations of 16S sequences, outperforming current 16S databases at the species level. The GSR database is available for full-length 16S sequences and the most commonly used hypervariable regions: V4, V1-V3, V3-V4, and V3-V5.↩︎",
    "crumbs": [
      "Home",
      "16S rRNA",
      "2. DADA2 ASV Workflow"
    ]
  },
  {
    "objectID": "workflows/pub/index.html",
    "href": "workflows/pub/index.html",
    "title": "Publication Material",
    "section": "",
    "text": "Submitting & Archiving Data/Code\n\n\nOn this page you can find information on submitting sequence data and code to public archives.\n\n\n\n\n\n9/27/25, 8:30:53 AM\n\n\n\n\n\n\n\nWorkflow References\n\n\nAccess to references for metagenomic and 16S RNA processing.\n\n\n\n\n\n10/3/25, 8:05:59 AM\n\n\n\n\n\nNo matching items\nCollected Links\n\n\nQuarto\nmetacrobe\nThe Istmobiome Project",
    "crumbs": [
      "Home",
      "Publication Material"
    ]
  },
  {
    "objectID": "workflows/portal/index.html",
    "href": "workflows/portal/index.html",
    "title": "Data Portal",
    "section": "",
    "text": "Quick access to fastq sequence files, processing scripts, and curated datasets. If you want to process the data yourself, select a pipeline, download the sequence data and processing scripts, and run the workflow.",
    "crumbs": [
      "Home",
      "Data Portal"
    ]
  },
  {
    "objectID": "workflows/portal/index.html#data-scripts",
    "href": "workflows/portal/index.html#data-scripts",
    "title": "Data Portal",
    "section": "Data & Scripts",
    "text": "Data & Scripts\n\n\n\n\n\n\nNoteA note about curated data\n\n\n\nCurated means that after processing, the data has been cleaned of unwanted taxa (e.g., NA kingdoms, potential contaminants, etc). Negative control & low-count samples have also been removed. These data are ready for downstream analysis.\n\n\nHere we provide two options for processing sequence data–the detailed pipeline (listed in the table) and a quick set of commands linked below.\n\n\n\n\nAsset\n\n\n\nFile Name\n\n\n\nDescription\n\n\n\n\n\n\nCurated Data\n\n\ndada2_curated_data.zip\n\n\nArchive containing the curated output of dada2 pipeline–-ASV table, taxonomy table, sample metadata, & representative fasta sequences. Data is available as a) stand-alone text files, b) bundled in a microtable object, and c) bundled in a phyloseq object. Archive also includes a table tracking sample read changes. \n\n\n\n\n\nSequence Data\n\n\nPRJEB36632\n\n\nTrimmed (primers removed) 16S rRNA data from 1909 samples. \n\n\n\n\n\nProcessing Data & Scripts\n\n\ndada2_processing.zip\n\n\nArchived directory containing fastq rename tables, fastq rename results, sample data, dada2 scripts, dada (command) results, run read changes, pipeline read changes, & Hydra job scripts. \n\n\n\n\n\nDetailed Pipeline\n\n\nASV workflow\n\n\nDetailed workflow for 16S rRNA ASV analysis using Dada2. Workflow uses paired end reads, beginning with raw fastq files, ending with sequence & taxonomy tables. Microtable & phyloseq objects are produced to collate the data for downstream analysis. \n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home",
      "Data Portal"
    ]
  },
  {
    "objectID": "workflows/portal/index.html#asv-processing",
    "href": "workflows/portal/index.html#asv-processing",
    "title": "Data Portal",
    "section": "ASV Processing",
    "text": "ASV Processing\nIndividual Runs\nBelow are processing scripts for ASV analysis of individual sequencing runs using dada2. We performed a total of six 16S rRNA sequencing runs–BCS_26, BCS_28, BCS_29, BCS_30, BCS_34, and BCS_35. In the first workflow of the pipeline, runs are processed separately for error rates, dereplication, and ASV inference. At the end of each workflow, forward and reverse reads are merged. If you have the raw data and everything in place simply run like so:\n\nconda activate R\nRscript BCS_26.R\nRscript BCS_28.R\nRscript BCS_29.R\nRscript BCS_30.R\nRscript BCS_34.R\nRscript BCS_35.R\n\nHere are the scripts for the individual runs, which can also be downloaded from the table above.\n\n\nBCS_26\nBCS_28\nBCS_29\nBCS_30\nBCS_34\nBCS_35\n\n\n\n\nProcessing script for run BCS_26################################################################\n#####           TRANSISTHMIAN SHRIMP MICROBIOME            #####\n#####                 MISEQ RUN: BCS_26                    #####\n#####            PLATES ISTHMO S5, S5, S7, S8              #####\n################################################################\n#\n# SCRIPT prepared by Matthieu Leray\n# last modified January 28th 2023\n#\n# READ PROCESSING\n#\n# Modified by Jarrod Scott\n# last modified December 26th 2023\n################################################################\n\nlibrary(dada2)\n#library(ggplot2)\nlibrary(tidyverse)\nlibrary(ff)\nlibrary(phyloseq)\nlibrary(gridExtra)\nlibrary(dplyr)\nlibrary(decontam)\n\n##Creating filepaths to data \npath &lt;- \"/pool/genomics/stri_istmobiome/data/TRANS_SHRIMP/RAW_DATA/16S/BCS_26\"\nhead(list.files(path)) #eventually to check if the path works\n\n##File preparation\n#extracting Forward (fnFs) and Reverse (fnRs) reads from files\nfnFs &lt;- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names &lt;- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs &lt;-file.path(path, fnFs)\nfnRs &lt;-file.path(path, fnRs)\n\n##plotting quality profiles\n#qprofile_fwd &lt;- print(plotQualityProfile(fnFs, aggregate = TRUE, n = 20000) \n#                                   + ggtitle(\"Forward\"))\n#qprofile_rev &lt;- print(plotQualityProfile(fnRs, aggregate = TRUE, n = 20000) \n#                                   + ggtitle(\"Reverse\"))\n\n\n####### UNCOMMENT TO RUN ######################\n#x &lt;- length(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\n#\n#qprofile_fwd &lt;- plotQualityProfile(fnFs[1:x], aggregate = TRUE, n = 20000)\n#qprofile_rev &lt;- plotQualityProfile(fnRs[1:x], aggregate = TRUE, n = 20000)\n\n#qprofile &lt;- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\n#ggsave(\"figures/BCS_26_filt_plot_qscores.png\", qprofile, width = 7, height = 3)\n#\n########################################\n\n\n#placing filtered files in a new filtered subdirectory\nfiltFs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_F_filt.fastq\"))\nfiltRs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_R_filt.fastq\"))\nnames(filtFs) &lt;- sample.names\nnames(filtRs) &lt;- sample.names\n\n#filtering and trimming, here truncation at 220 (Fwd) and 180 (Rev) bp, \n#2expected errors max (N discarded automatically)\n\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(220,180),\n                        maxN=0, maxEE=2, truncQ=2, rm.phix=TRUE,\n                        compress=TRUE, multithread=20)\n\nhead(out)\n\n#learning error rates\nerrF &lt;- learnErrors(filtFs, multithread = TRUE)\nerrR &lt;- learnErrors(filtRs, multithread = TRUE)\n#plotting errors\n#plotErrors(errF, nominalQ=TRUE)\n#plotErrors(errR, nominalQ=TRUE)\n\n## ----plot_errF------------------------------\np3 &lt;- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_26_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_26_plot_errorF_2.png\", p3)\n## ----plot_errR------------------------------\np4 &lt;- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_26_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_26_plot_errorR_2.png\", p4)\n\n##Dereplicating reads\nsam.names &lt;- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs &lt;- derepFastq(filtFs)\nnames(derepFs) &lt;- sam.names\nderepRs &lt;- derepFastq(filtRs)\nnames(derepRs) &lt;- sam.names\n\n##Infering Sequence Variants\ndadaFs &lt;- dada(derepFs, err = errF, pool = \"pseudo\", multithread = TRUE)\ndadaFs[[1]]\ndadaRs &lt;- dada(derepRs, err = errR, pool = \"pseudo\", multithread = TRUE)\ndadaRs[[1]]\n\n##Merging paired ends\nmergers &lt;- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\nBCS_26 &lt;- makeSequenceTable(mergers)\ndim(BCS_26)\n# [1]   384 29481\ntable(nchar(getSequences(BCS_26)))\n\n#exporting files to use in the next part of the workflow\nsaveRDS(BCS_26, \"/pool/genomics/stri_istmobiome/data/TRANS_SHRIMP/16S/ASV/BCS_26/BCS_26.rds\")\n\n#tracking changes through each step\ngetN &lt;- function(x) sum(getUniques(x))\ntrack &lt;-    cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN),\n                  sapply(mergers, getN))\ncolnames(track) &lt;- c(\"input\", \"filtered\", \"denoisedF\", \"denoisedR\",\n                        \"merged\")\nrownames(track) &lt;- sam.names\nwrite.table(track, \"/pool/genomics/stri_istmobiome/data/TRANS_SHRIMP/16S/ASV/BCS_26/BCS_26_read_changes.txt\", sep = \"\\t\", quote = FALSE,\n            col.names=NA)\n\nread_length &lt;-  data.frame(nchar(getSequences(BCS_26)))\n\ncolnames(read_length) &lt;- \"length\"\n\nplot_BCS_26 &lt;- qplot(length, data = read_length, geom = \"histogram\", binwidth = 1, xlab = \"read length\", ylab = \"total variants\", xlim = c(225,275)) \nggsave(\"figures/read_length_before_pseudo_BCS_26.png\", plot_BCS_26, width = 7, height = 3)\n\nsave.image(\"BCS_26.rdata\")\n\n\n\n\n\nProcessing script for run BCS_28################################################################\n#####           TRANSISTHMIAN SHRIMP MICROBIOME            #####\n#####                 MISEQ RUN: BCS_28                    #####\n#####               PLATES ISTHMO S3, S4                   #####\n################################################################\n#\n# SCRIPT prepared by Matthieu Leray\n# last modified January 28th 2023\n#\n# READ PROCESSING\n#\n# Modified by Jarrod Scott\n# last modified April 1st 2024\n################################################################\n\nlibrary(dada2)\n#library(ggplot2)\nlibrary(tidyverse)\nlibrary(ff)\nlibrary(phyloseq)\nlibrary(gridExtra)\nlibrary(dplyr)\nlibrary(decontam)\n\n##Creating filepaths to data \npath &lt;- \"/pool/genomics/stri_istmobiome/data/TRANS_SHRIMP/RAW_DATA/16S/BCS_28\"\nhead(list.files(path)) #eventually to check if the path works\n\n##File preparation\n#extracting Forward (fnFs) and Reverse (fnRs) reads from files\nfnFs &lt;- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names &lt;- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs &lt;-file.path(path, fnFs)\nfnRs &lt;-file.path(path, fnRs)\n\n##plotting quality profiles\n##qprofile_fwd &lt;- print(plotQualityProfile(fnFs, aggregate = TRUE) \n##                                   + ggtitle(\"Forward\"))\n##qprofile_rev &lt;- print(plotQualityProfile(fnRs, aggregate = TRUE) \n##                                   + ggtitle(\"Reverse\"))\n\n\n####### UNCOMMENT TO RUN ######################\n#x &lt;- length(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\n\n#qprofile_fwd &lt;- plotQualityProfile(fnFs[1:x], aggregate = TRUE)\n#qprofile_rev &lt;- plotQualityProfile(fnRs[1:x], aggregate = TRUE)\n\n#qprofile &lt;- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\n#ggsave(\"figures/BCS_28_filt_plot_qscores.png\", qprofile, width = 7, height = 3)\n#\n########################################\n\n\n#placing filtered files in a new filtered subdirectory\nfiltFs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_F_filt.fastq\"))\nfiltRs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_R_filt.fastq\"))\nnames(filtFs) &lt;- sample.names\nnames(filtRs) &lt;- sample.names\n\n#filtering and trimming, here truncation at 220 (Fwd) and 180 (Rev) bp, \n#2expected errors max (N discarded automatically)\n\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(220,180),\n                        maxN=0, maxEE=2, truncQ=2, rm.phix=TRUE,\n                        compress=TRUE, multithread=20)\n\nhead(out)\n\n#learning error rates\nerrF &lt;- learnErrors(filtFs, multithread = TRUE)\nerrR &lt;- learnErrors(filtRs, multithread = TRUE)\n#plotting errors\n#plotErrors(errF, nominalQ=TRUE)\n#plotErrors(errR, nominalQ=TRUE)\n\n## ----plot_errF------------------------------\np3 &lt;- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_28_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_28_plot_errorF_2.png\", p3)\n## ----plot_errR------------------------------\np4 &lt;- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_28_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_28_plot_errorR_2.png\", p4)\n\n##Dereplicating reads\nsam.names &lt;- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs &lt;- derepFastq(filtFs)\nnames(derepFs) &lt;- sam.names\nderepRs &lt;- derepFastq(filtRs)\nnames(derepRs) &lt;- sam.names\n\n##Infering Sequence Variants\ndadaFs &lt;- dada(derepFs, err = errF, pool = \"pseudo\", multithread = TRUE)\ndadaFs[[1]]\ndadaRs &lt;- dada(derepRs, err = errR, pool = \"pseudo\", multithread = TRUE)\ndadaRs[[1]]\n\n##Merging paired ends\nmergers &lt;- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\nBCS_28 &lt;- makeSequenceTable(mergers)\ndim(BCS_28)\n# [1]   384 29481\ntable(nchar(getSequences(BCS_28)))\n\n#exporting files to use in the next part of the workflow\nsaveRDS(BCS_28, \"/pool/genomics/stri_istmobiome/data/TRANS_SHRIMP/16S/ASV/BCS_28/BCS_28.rds\")\n\n#tracking changes through each step\ngetN &lt;- function(x) sum(getUniques(x))\ntrack &lt;-    cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN),\n                  sapply(mergers, getN))\ncolnames(track) &lt;- c(\"input\", \"filtered\", \"denoisedF\", \"denoisedR\",\n                        \"merged\")\nrownames(track) &lt;- sam.names\nwrite.table(track, \"/pool/genomics/stri_istmobiome/data/TRANS_SHRIMP/16S/ASV/BCS_28/BCS_28_read_changes.txt\", sep = \"\\t\", quote = FALSE,\n            col.names=NA)\n\nread_length &lt;-  data.frame(nchar(getSequences(BCS_28)))\n\ncolnames(read_length) &lt;- \"length\"\n\nplot_BCS_28 &lt;- qplot(length, data = read_length, geom = \"histogram\", binwidth = 1, xlab = \"read length\", ylab = \"total variants\", xlim = c(225,275)) \nggsave(\"figures/read_length_before_pseudo_BCS_28.png\", plot_BCS_28, width = 7, height = 3)\n\nsave.image(\"BCS_28.rdata\")\n\n\n\n\n\nProcessing script for run BCS_29################################################################\n#####           TRANSISTHMIAN SHRIMP MICROBIOME            #####\n#####                 MISEQ RUN: BCS_29                    #####\n#####          PLATES ISTHMO S13, S14, S15, S16            #####\n################################################################\n#\n# SCRIPT prepared by Matthieu Leray\n# last modified January 28th 2023\n#\n# READ PROCESSING\n#\n# Modified by Jarrod Scott\n# last modified December 26th 2023\n################################################################\n\nlibrary(dada2)\n#library(ggplot2)\nlibrary(tidyverse)\nlibrary(ff)\nlibrary(phyloseq)\nlibrary(gridExtra)\nlibrary(dplyr)\nlibrary(decontam)\n\n##Creating filepaths to data \npath &lt;- \"/pool/genomics/stri_istmobiome/data/TRANS_SHRIMP/RAW_DATA/16S/BCS_29\"\nhead(list.files(path)) #eventually to check if the path works\n\n##File preparation\n#extracting Forward (fnFs) and Reverse (fnRs) reads from files\nfnFs &lt;- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names &lt;- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs &lt;-file.path(path, fnFs)\nfnRs &lt;-file.path(path, fnRs)\n\n##plotting quality profiles\n##qprofile_fwd &lt;- print(plotQualityProfile(fnFs, aggregate = TRUE) \n##                                   + ggtitle(\"Forward\"))\n##qprofile_rev &lt;- print(plotQualityProfile(fnRs, aggregate = TRUE) \n##                                   + ggtitle(\"Reverse\"))\n\n\n####### UNCOMMENT TO RUN ######################\n#x &lt;- length(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\n#\n#qprofile_fwd &lt;- plotQualityProfile(fnFs[1:x], aggregate = TRUE)\n#qprofile_rev &lt;- plotQualityProfile(fnRs[1:x], aggregate = TRUE)\n#\n#qprofile &lt;- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\n#ggsave(\"figures/BCS_29_filt_plot_qscores.png\", qprofile, width = 7, height = 3)\n#\n########################################\n\n\n#placing filtered files in a new filtered subdirectory\nfiltFs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_F_filt.fastq\"))\nfiltRs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_R_filt.fastq\"))\nnames(filtFs) &lt;- sample.names\nnames(filtRs) &lt;- sample.names\n\n#filtering and trimming, here truncation at 220 (Fwd) and 180 (Rev) bp, \n#2expected errors max (N discarded automatically)\n\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(220,180),\n                        maxN=0, maxEE=2, truncQ=2, rm.phix=TRUE,\n                        compress=TRUE, multithread=20)\n\nhead(out)\n\n#learning error rates\nerrF &lt;- learnErrors(filtFs, multithread = TRUE)\nerrR &lt;- learnErrors(filtRs, multithread = TRUE)\n#plotting errors\n#plotErrors(errF, nominalQ=TRUE)\n#plotErrors(errR, nominalQ=TRUE)\n\n## ----plot_errF------------------------------\np3 &lt;- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_29_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_29_plot_errorF_2.png\", p3)\n## ----plot_errR------------------------------\np4 &lt;- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_29_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_29_plot_errorR_2.png\", p4)\n\n##Dereplicating reads\nsam.names &lt;- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs &lt;- derepFastq(filtFs)\nnames(derepFs) &lt;- sam.names\nderepRs &lt;- derepFastq(filtRs)\nnames(derepRs) &lt;- sam.names\n\n##Infering Sequence Variants\ndadaFs &lt;- dada(derepFs, err = errF, pool = \"pseudo\", multithread = TRUE)\ndadaFs[[1]]\ndadaRs &lt;- dada(derepRs, err = errR, pool = \"pseudo\", multithread = TRUE)\ndadaRs[[1]]\n\n##Merging paired ends\nmergers &lt;- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\nBCS_29 &lt;- makeSequenceTable(mergers)\ndim(BCS_29)\n# [1]   384 29481\ntable(nchar(getSequences(BCS_29)))\n\n#exporting files to use in the next part of the workflow\nsaveRDS(BCS_29, \"/pool/genomics/stri_istmobiome/data/TRANS_SHRIMP/16S/ASV/BCS_29/BCS_29.rds\")\n\n#tracking changes through each step\ngetN &lt;- function(x) sum(getUniques(x))\ntrack &lt;-    cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN),\n                  sapply(mergers, getN))\ncolnames(track) &lt;- c(\"input\", \"filtered\", \"denoisedF\", \"denoisedR\",\n                        \"merged\")\nrownames(track) &lt;- sam.names\nwrite.table(track, \"/pool/genomics/stri_istmobiome/data/TRANS_SHRIMP/16S/ASV/BCS_29/BCS_29_read_changes.txt\", sep = \"\\t\", quote = FALSE,\n            col.names=NA)\n\nread_length &lt;-  data.frame(nchar(getSequences(BCS_29)))\n\ncolnames(read_length) &lt;- \"length\"\n\nplot_BCS_29 &lt;- qplot(length, data = read_length, geom = \"histogram\", binwidth = 1, xlab = \"read length\", ylab = \"total variants\", xlim = c(225,275)) \nggsave(\"figures/read_length_before_pseudo_BCS_29.png\", plot_BCS_29, width = 7, height = 3)\n\nsave.image(\"BCS_29.rdata\")\n\n\n\n\n\nProcessing script for run BCS_30################################################################\n#####           TRANSISTHMIAN SHRIMP MICROBIOME            #####\n#####                 MISEQ RUN: BCS_30                    #####\n#####          PLATES ISTHMO S17, S18, S19, S20            #####\n################################################################\n#\n# SCRIPT prepared by Matthieu Leray\n# last modified January 28th 2023\n#\n# READ PROCESSING\n#\n# Modified by Jarrod Scott\n# last modified December 26th 2023\n################################################################\n\nlibrary(dada2)\n#library(ggplot2)\nlibrary(tidyverse)\nlibrary(ff)\nlibrary(phyloseq)\nlibrary(gridExtra)\nlibrary(dplyr)\nlibrary(decontam)\n\n##Creating filepaths to data \npath &lt;- \"/pool/genomics/stri_istmobiome/data/TRANS_SHRIMP/RAW_DATA/16S/BCS_30\"\nhead(list.files(path)) #eventually to check if the path works\n\n##File preparation\n#extracting Forward (fnFs) and Reverse (fnRs) reads from files\nfnFs &lt;- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names &lt;- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs &lt;-file.path(path, fnFs)\nfnRs &lt;-file.path(path, fnRs)\n\n##plotting quality profiles\n##qprofile_fwd &lt;- print(plotQualityProfile(fnFs, aggregate = TRUE) \n##                                   + ggtitle(\"Forward\"))\n##qprofile_rev &lt;- print(plotQualityProfile(fnRs, aggregate = TRUE) \n##                                   + ggtitle(\"Reverse\"))\n\n\n####### UNCOMMENT TO RUN ######################\nx &lt;- length(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\n#\n#qprofile_fwd &lt;- plotQualityProfile(fnFs[1:x], aggregate = TRUE, n = 20000)\n#qprofile_rev &lt;- plotQualityProfile(fnRs[1:x], aggregate = TRUE, n = 20000)\n#\n#qprofile &lt;- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\n#ggsave(\"figures/BCS_30_filt_plot_qscores.png\", qprofile, width = 7, height = 3)\n#\n########################################\n\n\n#placing filtered files in a new filtered subdirectory\nfiltFs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_F_filt.fastq\"))\nfiltRs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_R_filt.fastq\"))\nnames(filtFs) &lt;- sample.names\nnames(filtRs) &lt;- sample.names\n\n#filtering and trimming, here truncation at 220 (Fwd) and 180 (Rev) bp, \n#2expected errors max (N discarded automatically)\n\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(220,180),\n                        maxN=0, maxEE=2, truncQ=2, rm.phix=TRUE,\n                        compress=TRUE, multithread=20)\n\nhead(out)\n\n#learning error rates\nerrF &lt;- learnErrors(filtFs, multithread = TRUE)\nerrR &lt;- learnErrors(filtRs, multithread = TRUE)\n#plotting errors\n#plotErrors(errF, nominalQ=TRUE)\n#plotErrors(errR, nominalQ=TRUE)\n\n## ----plot_errF------------------------------\np3 &lt;- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_30_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_30_plot_errorF_2.png\", p3)\n## ----plot_errR------------------------------\np4 &lt;- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_30_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_30_plot_errorR_2.png\", p4)\n\n##Dereplicating reads\nsam.names &lt;- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs &lt;- derepFastq(filtFs)\nnames(derepFs) &lt;- sam.names\nderepRs &lt;- derepFastq(filtRs)\nnames(derepRs) &lt;- sam.names\n\n##Infering Sequence Variants\ndadaFs &lt;- dada(derepFs, err = errF, pool = \"pseudo\", multithread = TRUE)\ndadaFs[[1]]\ndadaRs &lt;- dada(derepRs, err = errR, pool = \"pseudo\", multithread = TRUE)\ndadaRs[[1]]\n\n##Merging paired ends\nmergers &lt;- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\nBCS_30 &lt;- makeSequenceTable(mergers)\ndim(BCS_30)\n# [1]   384 29481\ntable(nchar(getSequences(BCS_30)))\n\n#exporting files to use in the next part of the workflow\nsaveRDS(BCS_30, \"/pool/genomics/stri_istmobiome/data/TRANS_SHRIMP/16S/ASV/BCS_30/BCS_30.rds\")\n\n#tracking changes through each step\ngetN &lt;- function(x) sum(getUniques(x))\ntrack &lt;-    cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN),\n                  sapply(mergers, getN))\ncolnames(track) &lt;- c(\"input\", \"filtered\", \"denoisedF\", \"denoisedR\",\n                        \"merged\")\nrownames(track) &lt;- sam.names\nwrite.table(track, \"/pool/genomics/stri_istmobiome/data/TRANS_SHRIMP/16S/ASV/BCS_30/BCS_30_read_changes.txt\", sep = \"\\t\", quote = FALSE,\n            col.names=NA)\n\nread_length &lt;-  data.frame(nchar(getSequences(BCS_30)))\n\ncolnames(read_length) &lt;- \"length\"\n\nplot_BCS_30 &lt;- qplot(length, data = read_length, geom = \"histogram\", binwidth = 1, xlab = \"read length\", ylab = \"total variants\", xlim = c(225,275)) \nggsave(\"figures/read_length_before_pseudo_BCS_30.png\", plot_BCS_30, width = 7, height = 3)\n\nsave.image(\"BCS_30.rdata\")\n\n\n\n\n\nProcessing script for run BCS_34################################################################\n#####           TRANSISTHMIAN SHRIMP MICROBIOME            #####\n#####                 MISEQ RUN: BCS_34                    #####\n#####               PLATES ISTHMO S01, S02                 #####\n################################################################\n#\n# SCRIPT prepared by Matthieu Leray\n# last modified January 28th 2023\n#\n# READ PROCESSING\n#\n# Modified by Jarrod Scott\n# last modified December 26th 2023\n################################################################\n\nlibrary(dada2)\n#library(ggplot2)\nlibrary(tidyverse)\nlibrary(ff)\nlibrary(phyloseq)\nlibrary(gridExtra)\nlibrary(dplyr)\nlibrary(decontam)\n\n##Creating filepaths to data \npath &lt;- \"/pool/genomics/stri_istmobiome/data/TRANS_SHRIMP/RAW_DATA/16S/BCS_34\"\nhead(list.files(path)) #eventually to check if the path works\n\n##File preparation\n#extracting Forward (fnFs) and Reverse (fnRs) reads from files\nfnFs &lt;- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names &lt;- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs &lt;-file.path(path, fnFs)\nfnRs &lt;-file.path(path, fnRs)\n\n##plotting quality profiles\n##qprofile_fwd &lt;- print(plotQualityProfile(fnFs, aggregate = TRUE) \n##                                   + ggtitle(\"Forward\"))\n##qprofile_rev &lt;- print(plotQualityProfile(fnRs, aggregate = TRUE) \n##                                   + ggtitle(\"Reverse\"))\n\n\n####### UNCOMMENT TO RUN ######################\n#x &lt;- length(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\n#\n#qprofile_fwd &lt;- plotQualityProfile(fnFs[1:x], aggregate = TRUE)\n#qprofile_rev &lt;- plotQualityProfile(fnRs[1:x], aggregate = TRUE)\n#\n#qprofile &lt;- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\n#ggsave(\"figures/BCS_34_filt_plot_qscores.png\", qprofile, width = 7, height = 3)\n#\n########################################\n\n\n#placing filtered files in a new filtered subdirectory\nfiltFs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_F_filt.fastq\"))\nfiltRs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_R_filt.fastq\"))\nnames(filtFs) &lt;- sample.names\nnames(filtRs) &lt;- sample.names\n\n#filtering and trimming, here truncation at 220 (Fwd) and 180 (Rev) bp, \n#2expected errors max (N discarded automatically)\n\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(220,180),\n                        maxN=0, maxEE=2, truncQ=2, rm.phix=TRUE,\n                        compress=TRUE, multithread=20)\n\nhead(out)\n\n#learning error rates\nerrF &lt;- learnErrors(filtFs, multithread = TRUE)\nerrR &lt;- learnErrors(filtRs, multithread = TRUE)\n#plotting errors\n#plotErrors(errF, nominalQ=TRUE)\n#plotErrors(errR, nominalQ=TRUE)\n\n## ----plot_errF------------------------------\np3 &lt;- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_34_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_34_plot_errorF_2.png\", p3)\n## ----plot_errR------------------------------\np4 &lt;- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_34_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_34_plot_errorR_2.png\", p4)\n\n##Dereplicating reads\nsam.names &lt;- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs &lt;- derepFastq(filtFs)\nnames(derepFs) &lt;- sam.names\nderepRs &lt;- derepFastq(filtRs)\nnames(derepRs) &lt;- sam.names\n\n##Infering Sequence Variants\ndadaFs &lt;- dada(derepFs, err = errF, pool = \"pseudo\", multithread = TRUE)\ndadaFs[[1]]\ndadaRs &lt;- dada(derepRs, err = errR, pool = \"pseudo\", multithread = TRUE)\ndadaRs[[1]]\n\n##Merging paired ends\nmergers &lt;- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\nBCS_34 &lt;- makeSequenceTable(mergers)\ndim(BCS_34)\n# [1]   384 29481\ntable(nchar(getSequences(BCS_34)))\n\n#exporting files to use in the next part of the workflow\nsaveRDS(BCS_34, \"/pool/genomics/stri_istmobiome/data/TRANS_SHRIMP/16S/ASV/BCS_34/BCS_34.rds\")\n\n#tracking changes through each step\ngetN &lt;- function(x) sum(getUniques(x))\ntrack &lt;-    cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN),\n                  sapply(mergers, getN))\ncolnames(track) &lt;- c(\"input\", \"filtered\", \"denoisedF\", \"denoisedR\",\n                        \"merged\")\nrownames(track) &lt;- sam.names\nwrite.table(track, \"/pool/genomics/stri_istmobiome/data/TRANS_SHRIMP/16S/ASV/BCS_34/BCS_34_read_changes.txt\", sep = \"\\t\", quote = FALSE,\n            col.names=NA)\n\nread_length &lt;-  data.frame(nchar(getSequences(BCS_34)))\n\ncolnames(read_length) &lt;- \"length\"\n\nplot_BCS_34 &lt;- qplot(length, data = read_length, geom = \"histogram\", binwidth = 1, xlab = \"read length\", ylab = \"total variants\", xlim = c(225,275)) \nggsave(\"figures/read_length_before_pseudo_BCS_34.png\", plot_BCS_34, width = 7, height = 3)\n\nsave.image(\"BCS_34.rdata\")\n\n\n\n\n\nProcessing script for run BCS_35################################################################\n#####           TRANSISTHMIAN SHRIMP MICROBIOME            #####\n#####                 MISEQ RUN: BCS_35                    #####\n#####           PLATES ISTHMO S9, S10, S11, S12            #####\n################################################################\n#\n# SCRIPT prepared by Matthieu Leray\n# last modified January 28th 2023\n#\n# READ PROCESSING\n#\n# Modified by Jarrod Scott\n# last modified December 26th 2023\n################################################################\n\nlibrary(dada2)\n#library(ggplot2)\nlibrary(tidyverse)\nlibrary(ff)\nlibrary(phyloseq)\nlibrary(gridExtra)\nlibrary(dplyr)\nlibrary(decontam)\n\n##Creating filepaths to data \npath &lt;- \"/pool/genomics/stri_istmobiome/data/TRANS_SHRIMP/RAW_DATA/16S/BCS_35\"\nhead(list.files(path)) #eventually to check if the path works\n\n##File preparation\n#extracting Forward (fnFs) and Reverse (fnRs) reads from files\nfnFs &lt;- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names &lt;- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs &lt;-file.path(path, fnFs)\nfnRs &lt;-file.path(path, fnRs)\n\n##plotting quality profiles\n##qprofile_fwd &lt;- print(plotQualityProfile(fnFs, aggregate = TRUE) \n##                                   + ggtitle(\"Forward\"))\n##qprofile_rev &lt;- print(plotQualityProfile(fnRs, aggregate = TRUE) \n##                                   + ggtitle(\"Reverse\"))\n\n\n####### UNCOMMENT TO RUN ######################\n#x &lt;- length(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\n#\n#qprofile_fwd &lt;- plotQualityProfile(fnFs[1:x], aggregate = TRUE)\n#qprofile_rev &lt;- plotQualityProfile(fnRs[1:x], aggregate = TRUE)\n#\n#qprofile &lt;- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\n#ggsave(\"figures/BCS_35_filt_plot_qscores.png\", qprofile, width = 7, height = 3)\n#\n########################################\n\n\n#placing filtered files in a new filtered subdirectory\nfiltFs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_F_filt.fastq\"))\nfiltRs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_R_filt.fastq\"))\nnames(filtFs) &lt;- sample.names\nnames(filtRs) &lt;- sample.names\n\n#filtering and trimming, here truncation at 220 (Fwd) and 180 (Rev) bp, \n#2expected errors max (N discarded automatically)\n\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(220,180),\n                        maxN=0, maxEE=2, truncQ=2, rm.phix=TRUE,\n                        compress=TRUE, multithread=20)\n\nhead(out)\n\n#learning error rates\nerrF &lt;- learnErrors(filtFs, multithread = TRUE)\nerrR &lt;- learnErrors(filtRs, multithread = TRUE)\n#plotting errors\n#plotErrors(errF, nominalQ=TRUE)\n#plotErrors(errR, nominalQ=TRUE)\n\n## ----plot_errF------------------------------\np3 &lt;- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_35_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_35_plot_errorF_2.png\", p3)\n## ----plot_errR------------------------------\np4 &lt;- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_35_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_35_plot_errorR_2.png\", p4)\n\n##Dereplicating reads\nsam.names &lt;- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs &lt;- derepFastq(filtFs)\nnames(derepFs) &lt;- sam.names\nderepRs &lt;- derepFastq(filtRs)\nnames(derepRs) &lt;- sam.names\n\n##Infering Sequence Variants\ndadaFs &lt;- dada(derepFs, err = errF, pool = \"pseudo\", multithread = TRUE)\ndadaFs[[1]]\ndadaRs &lt;- dada(derepRs, err = errR, pool = \"pseudo\", multithread = TRUE)\ndadaRs[[1]]\n\n##Merging paired ends\nmergers &lt;- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\nBCS_35 &lt;- makeSequenceTable(mergers)\ndim(BCS_35)\n# [1]   384 29481\ntable(nchar(getSequences(BCS_35)))\n\n#exporting files to use in the next part of the workflow\nsaveRDS(BCS_35, \"/pool/genomics/stri_istmobiome/data/TRANS_SHRIMP/16S/ASV/BCS_35/BCS_35.rds\")\n\n#tracking changes through each step\ngetN &lt;- function(x) sum(getUniques(x))\ntrack &lt;-    cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN),\n                  sapply(mergers, getN))\ncolnames(track) &lt;- c(\"input\", \"filtered\", \"denoisedF\", \"denoisedR\",\n                        \"merged\")\nrownames(track) &lt;- sam.names\nwrite.table(track, \"/pool/genomics/stri_istmobiome/data/TRANS_SHRIMP/16S/ASV/BCS_35/BCS_35_read_changes.txt\", sep = \"\\t\", quote = FALSE,\n            col.names=NA)\n\nread_length &lt;-  data.frame(nchar(getSequences(BCS_35)))\n\ncolnames(read_length) &lt;- \"length\"\n\nplot_BCS_35 &lt;- qplot(length, data = read_length, geom = \"histogram\", binwidth = 1, xlab = \"read length\", ylab = \"total variants\", xlim = c(225,275)) \nggsave(\"figures/read_length_before_pseudo_BCS_35.png\", plot_BCS_35, width = 7, height = 3)\n\nsave.image(\"BCS_35.rdata\")\n\n\n\n\n\nMerged Runs\nOnce these workflows finish, we then merge the six sequence tables together and proceed with chimera removal and taxonomic classification.\n\nProcessing script for run merged runs#!/usr/bin/env Rscript\nset.seed(919191)\nlibrary(dada2); packageVersion(\"dada2\")\nlibrary(ggplot2)\nlibrary(ff)\nlibrary(phyloseq)\nlibrary(gridExtra)\nlibrary(dplyr)\nlibrary(decontam)\nlibrary(grid)\nlibrary(ShortRead); packageVersion(\"ShortRead\")\nlibrary(Biostrings); packageVersion(\"Biostrings\")\nlibrary(DECIPHER); packageVersion(\"DECIPHER\")\n\n########################################\n#\n# 2. MERGE ALL SEQ TABS\n#\n########################################\n\n############################################\n# PROBLEM: Duplicated sample names \n# detected in the sequence table row names: \n# 7512-G, 7512-H, 7512-M, 7512-S\n# BCS_30 and BCS_35\n# FOR BCS_30 changed name to 7512A...\n# FOR BCS_35 changed name to 7512B...\n############################################\n\nBCS_26 &lt;- readRDS(\"BCS_26/BCS_26.rds\")\nBCS_28 &lt;- readRDS(\"BCS_28/BCS_28.rds\")\nBCS_29 &lt;- readRDS(\"BCS_29/BCS_29.rds\")\nBCS_30 &lt;- readRDS(\"BCS_30/BCS_30.rds\")\nBCS_34 &lt;- readRDS(\"BCS_34/BCS_34.rds\")\nBCS_35 &lt;- readRDS(\"BCS_35/BCS_35.rds\")\n\nseqtab.merge &lt;- mergeSequenceTables(BCS_26, BCS_28, BCS_29, BCS_30, BCS_34, BCS_35)\ndim(seqtab.merge)\ntable(nchar(getSequences(seqtab.merge)))\n\nread_length_all &lt;-  data.frame(nchar(getSequences(seqtab.merge)))\ncolnames(read_length_all) &lt;- \"length\"\nplot_all &lt;- qplot(length, data = read_length_all, geom = \"histogram\", binwidth = 1, xlab = \"read length\", ylab = \"total variants\", xlim = c(200,400)) \nggsave(\"figures/read_length_before_collapse.png\", plot_all, width = 7, height = 3)\nsaveRDS(seqtab.merge, \"2.seqtab.merge.rds\")\n\nsave.image(\"rdata/2.merge.seqtabs.rdata\")\n\n########################################\n#\n# collapseNoMismatch\n# TESTED, only minor differences in\n# 13 samples. Takes long time to run\n#\n########################################\n\n# seqtab_to_collapse &lt;- collapseNoMismatch(st_all, minOverlap = 20, orderBy = \"abundance\",\n#   identicalOnly = FALSE, vec = TRUE, band = -1, verbose = TRUE)\n\n# dim(seqtab_to_collapse)\n# table(nchar(getSequences(seqtab_to_collapse)))\n\n# read_length_all_collapse &lt;-  data.frame(nchar(getSequences(seqtab_to_collapse)))\n# colnames(read_length_all_collapse) &lt;- \"length\"\n# plot_all_collapse &lt;- qplot(length, data = read_length_all_collapse, geom = \"histogram\", binwidth = 1, xlab = \"read length\", ylab = \"total variants\", xlim = c(200,400)) \n# ggsave(\"figures/read_length_after_collapse.png\", plot_all_collapse, width = 7, height = 3)\n# saveRDS(seqtab_to_collapse, \"seqtab_after_collapse.rds\")\n\n# save.image(\"rdata/2_merge_seqtabs_collapsed.rdata\")\n\n########################################\n#\n# 3. REMOVING CHIMERAS\n#\n########################################\n\n############################################\n# PROBLEM: Duplicated sample names \n# detected in the sequence table row names: \n# 7512-G, 7512-H, 7512-M, 7512-S\n# BCS_30 and BCS_35\n# FOR BCS_30 changed name to 7512A...\n# FOR BCS_35 changed name to 7512B...\n############################################\n\n## REMVOE OUTLIER READ LENGTHS\nseqtab &lt;- seqtab.merge\n#seqtab.merge &lt;- readRDS(\"seqtab_before_collapse.rds\")\ntable(nchar(getSequences(seqtab)))\n\n################################################################################# \n## \n## 220   221   222   223   224   225   226   227   228   229   230   231   232\n##   125    67    14    36    20    13    10    25     9     6     4    27     2\n##   234   235   236   237   238   239   240   241   242   243   244   245   246\n##     9     8  1373   151    46     6    99   407   298   452    31    14    13\n##   247   248   249   250   251   252   253   254   255   256   257   258   259\n##    26    23    19    49   159  3587 84485  3772   319   123    96    20    10\n##   260   261   262   263   264   265   266   267   268   269   270   271   272\n##     8    16     9     4     2     1     1     1     4     2     9     8     4\n##   273   274   275   276   277   278   279   280   281   282   284   285   286\n##     7     3     2     5     1     7     4     1     1     2     1     4     4\n##   288   289   290   291   292   293   294   295   296   297   298   300   303\n##     1     3     1     2     4     8     7     2     3     2     3     2     3\n##   304   305   307   308   309   310   311   312   313   315   316   317   318\n##     1     5     2     3     2     1     3     1     3     1     4     1     3\n##   319   320   321   322   323   324   325   326   328   329   330   332   333\n##     3     1     2     3     2     1     3     1     3     3     2     1     3\n##   334   335   336   337   338   339   340   341   342   343   344   345   346\n##    13     6     7    18     5    25    16    70    52     8     7     8     4\n##   347   348   349   350   351   352   353   354   355   356   357   358   359\n##    25    17    21    10     2    11     1     1     7     6    31     6    15\n##   360   361   362   363   364   365   366   367   368   369   370   371   372\n##    21   161   188    43   141   108    19     9    26     5     3     3     8\n##   373   374   376   377   378   379   380   384   385   386   387   388\n##    11     2     3     5     2     3     1     1     1     1     2     1\n##\n#################################################################################\n\n#######################################################\n## ----REMOVE OUTLIER READ LENGTHS------------------ ##\n#######################################################\n\nseqtab.trim &lt;- seqtab[,nchar(colnames(seqtab)) %in% seq(252, 254)]\ndim(seqtab.trim)\ntable(nchar(getSequences(seqtab.trim)))\n\n#####################\n##   252   253   254\n##  3587 84485  3772\n#####################\n\n#######################################################\n## ----chimera pooled------------------------------- ##\n#######################################################\n\nseqtab.trim.nochim.pool &lt;- removeBimeraDenovo(seqtab.trim, method = \"pooled\", multithread = 20, verbose = TRUE)\ndim(seqtab.trim.nochim.pool)\nsum(seqtab.trim.nochim.pool)/sum(seqtab.trim)\n\nsaveRDS(seqtab.trim.nochim.pool, \"3.seqtab.trim.nochim.pool.rds\")\n\ntable(nchar(getSequences(seqtab.trim.nochim.pool)))\n\ntable(colSums(seqtab.trim.nochim.pool &gt; 0))\ntable(rowSums(seqtab.trim.nochim.pool &gt; 0))\n\n##########################################################\n## ----chimera consensus------------------------------- ##\n##########################################################\n\nseqtab.trim.nochim.consensus &lt;- removeBimeraDenovo(seqtab.trim, method = \"consensus\", multithread = 20, verbose = TRUE)\ndim(seqtab.trim.nochim.consensus)\nsum(seqtab.trim.nochim.consensus)/sum(seqtab.trim)\n\nsaveRDS(seqtab.trim.nochim.consensus, \"3.seqtab.trim.nochim.consensus.rds\")\n\ntable(nchar(getSequences(seqtab.trim.nochim.consensus)))\n\ntable(colSums(seqtab.trim.nochim.consensus &gt; 0))\ntable(rowSums(seqtab.trim.nochim.consensus &gt; 0))\n\n##########################################################\n## ----tracking changes-------------------------------- ##\n##########################################################\n\ngetN &lt;- function(x) sum(getUniques(x))\ntrack &lt;- cbind(rowSums(seqtab), \n               rowSums(seqtab.trim), \n               rowSums(seqtab.trim.nochim.pool), \n               rowSums(seqtab.trim.nochim.consensus))\n\ncolnames(track) &lt;- c(\"merged\", \"trim\", \"chimera_pool\", \"chimera_concensus\")\nwrite.table(track, \"3.chimera_read_changes_pipeline.txt\", sep = \"\\t\", quote = FALSE,\n            col.names=NA)\n\nsave.image(\"rdata/3.trim.chimera.rdata\")\n\n########################################\n#\n# 4. ASSIGNING TAXONOMY\n#\n########################################\n\n###########################################################\n# reference datasets formatted for DADA2 can be found here: \n# https://benjjneb.github.io/dada2/training.html\n###########################################################\n\n########################################\n#\n# TAXONOMY chimera = pooled\n#\n########################################\n\n# seqtab &lt;- readRDS(\"3.seqtab.trim.nochim.pool.rds\")\n\n########################################\n# TAXONOMY = silva\n########################################\nseqtab.pool &lt;- seqtab.trim.nochim.pool\n\ntax_silva_v138.pool &lt;- assignTaxonomy(seqtab.pool, \"TAXONOMY_FILES/silva_nr99_v138.1_train_set.fa.gz\", multithread = TRUE, verbose = TRUE)\nsaveRDS(tax_silva_v138.pool, \"4.tax_silva_v138.pool.rds\")\n\ntax_silva_v132.pool &lt;- assignTaxonomy(seqtab.pool, \"TAXONOMY_FILES/silva_nr_v132_train_set.fa.gz\", multithread = TRUE, verbose = TRUE)\nsaveRDS(tax_silva_v132.pool, \"4.tax_silva_v132.pool.rds\")\n\n########################################\n# TAXONOMY = RDP\n########################################\n\ntax_rdp_v138.pool &lt;- assignTaxonomy(seqtab.pool, \"TAXONOMY_FILES/rdp_train_set_18.fa.gz\", multithread = TRUE, verbose = TRUE)\nsaveRDS(tax_rdp_v138.pool, \"4.tax_rdp_v138.pool.rds\")\n\n########################################\n#\n# TAXONOMY chimera = consensus\n#\n########################################\n\n#remove(list = ls())\n#seqtab &lt;- readRDS(\"3.seqtab.trim.nochim.consensus.rds\")\n#objects()\n\n########################################\n# TAXONOMY = silva\n########################################\nseqtab.consensus &lt;- seqtab.trim.nochim.consensus\n\ntax_silva_v138.consensus &lt;- assignTaxonomy(seqtab.consensus, \"TAXONOMY_FILES/silva_nr99_v138.1_train_set.fa.gz\", multithread = TRUE, verbose = TRUE)\nsaveRDS(tax_silva_v138.consensus, \"4.tax_silva_v138.consensus.rds\")\n\ntax_silva_v132.consensus &lt;- assignTaxonomy(seqtab.consensus, \"TAXONOMY_FILES/silva_nr_v132_train_set.fa.gz\", multithread = TRUE, verbose = TRUE)\nsaveRDS(tax_silva_v132.consensus, \"4.tax_silva_v132.consensus.rds\")\n\n########################################\n# TAXONOMY = RDP\n########################################\n\ntax_rdp_v138.consensus &lt;- assignTaxonomy(seqtab.consensus, \"TAXONOMY_FILES/rdp_train_set_18.fa.gz\", multithread = TRUE, verbose = TRUE)\nsaveRDS(tax_rdp_v138.consensus, \"4.tax_rdp_v138.consensus.rds\")\n\n########################################\n# TAXONOMY = ITGDB\n########################################\n\ntax_itgdb.consensus &lt;- assignTaxonomy(seqtab.consensus, \"TAXONOMY_FILES/itgdb_dada2.fa\", multithread = TRUE, verbose = TRUE)\nsaveRDS(tax_itgdb.consensus, \"4.tax_itgdb.consensus.rds\")\n\n########################################\n# TAXONOMY = GSRDB\n########################################\n\ntax_gsrdb.consensus &lt;- assignTaxonomy(seqtab.consensus, \"TAXONOMY_FILES/gsrdb_dada2.fa\", multithread = TRUE, verbose = TRUE)\nsaveRDS(tax_gsrdb.consensus, \"4.tax_gsrdb.consensus.rds\")\n\n\nsave.image(\"rdata/4.dada2.pipeline.rdata\")\n\nsessionInfo()\ndevtools::session_info()\n\nquit()\n\n\n\nconda activate R\nRscript merge_asv_workflow.R\n\nIn the resources listed above, we include a table that summarizes read changes for each sample through the pipeline.",
    "crumbs": [
      "Home",
      "Data Portal"
    ]
  },
  {
    "objectID": "workflows/portal/data-mg.html",
    "href": "workflows/portal/data-mg.html",
    "title": "Data Portal",
    "section": "",
    "text": "Quick access to fastq sequence files, processing scripts, and curated datasets. If you want to process the data yourself, select a pipeline, download the sequence data and processing scripts, and run the workflow."
  },
  {
    "objectID": "workflows/portal/data-mg.html#data-scripts",
    "href": "workflows/portal/data-mg.html#data-scripts",
    "title": "Data Portal",
    "section": "Data & Scripts",
    "text": "Data & Scripts\n\n\n\n\n\n\nNoteA note about curated data\n\n\n\nCurated means that after processing, the data has been cleaned of unwanted taxa (e.g., NA kingdoms, potential contaminants, etc). Negative control & low-count samples have also been removed. These data are ready for downstream analysis.\n\n\nHere we provide two options for processing sequence data–the detailed pipeline (listed in the table) and a quick set of commands linked below.\n\n\n\n\nAsset\n\n\n\nFile Name\n\n\n\nDescription\n\n\n\n\n\n\nSequence data\n\n\nPRJEB36632\n\n\nRaw metagenomic reads (forward and reverse) for 58 samples. \n\n\n\n\n\nSample data\n\n\nall_metadata.zip\n\n\nMetagenomic sample data \n\n\n\n\n\nAnvi’o config file\n\n\ndefault_mg.json.zip\n\n\nJSON-formatted configuration file for metagenomic workflow \n\n\n\n\n\nSample mapping file\n\n\nsamples.txt.zip\n\n\nFile needed for workflow that maps fastq files sample names and co-assembly group \n\n\n\n\n\nQC report\n\n\nqc-report.txt.zip\n\n\nTable showing the results of quality control filtering for each sample. \n\n\n\n\n\nHydra job scripts\n\n\nmg_hydra_scripts.zip\n\n\nHydra job scripts \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "workflows/portal/data-mg.html#metagenomic-processing",
    "href": "workflows/portal/data-mg.html#metagenomic-processing",
    "title": "Data Portal",
    "section": "Metagenomic Processing",
    "text": "Metagenomic Processing\nFor metagenomic analysis we used the anvi’o platform, which brings together many aspects of cutting-edge computational strategies of data-enabled microbiology, including genomics, metagenomics, metatranscriptomics, pangenomics, metapangenomics, phylogenomics, and microbial population genetics in an integrated and easy-to-use fashion through extensive interactive visualization capabilities. It is basically heavy metal.\nHere we run the anvi’o metagenomic workflow. For this pipeline we need the following:\nA. JSON-formatted config file, which we can get by running the following:\n\nanvi-run-workflow -w metagenomics --get-default-config default_mg.json\n\nB. Modify the file to suite your needs. Here is our final config file.\n\njson formatted config file for anvi’o metagenomic workflow.{\n    \"fasta_txt\": \"\",\n    \"anvi_gen_contigs_database\": {\n        \"--project-name\": \"{group}\",\n        \"--description\": \"\",\n        \"--skip-gene-calling\": \"\",\n        \"--ignore-internal-stop-codons\": \"\",\n        \"--skip-mindful-splitting\": \"\",\n        \"--contigs-fasta\": \"\",\n        \"--split-length\": \"\",\n        \"--kmer-size\": \"\",\n        \"--skip-predict-frame\": \"\",\n        \"--prodigal-translation-table\": \"\",\n        \"threads\": 14\n    },\n    \"centrifuge\": {\n        \"threads\": 14,\n        \"run\": true,\n        \"db\": \"/pool/genomics/stri_istmobiome/dbs/centrifuge_dbs/p+h+v\"\n    },\n    \"anvi_run_hmms\": {\n        \"run\": true,\n        \"threads\": 14,\n        \"--also-scan-trnas\": true,\n        \"--installed-hmm-profile\": \"\",\n        \"--hmm-profile-dir\": \"\",\n        \"--add-to-functions-table\": \"\"\n    },\n    \"anvi_run_kegg_kofams\": {\n        \"run\": true,\n        \"threads\": 14,\n        \"--kegg-data-dir\": \"/pool/genomics/stri_istmobiome/dbs/kegg_kofam/\",\n        \"--hmmer-program\": \"\",\n        \"--keep-all-hits\": \"\",\n        \"--log-bitscores\": \"\",\n        \"--just-do-it\": \"\"\n    },\n    \"anvi_run_ncbi_cogs\": {\n        \"run\": true,\n        \"threads\": 14,\n        \"--cog-data-dir\": \"/pool/genomics/stri_istmobiome/dbs/cog_db/\",\n        \"--temporary-dir-path\": \"/pool/genomics/stri_istmobiome/dbs/cog_db/tmp_mega/\",\n        \"--search-with\": \"\"\n    },\n    \"anvi_run_scg_taxonomy\": {\n        \"run\": true,\n        \"threads\": 14,\n        \"--scgs-taxonomy-data-dir\": \"\"\n    },\n    \"anvi_run_trna_scan\": {\n        \"run\": true,\n        \"threads\": 14,\n        \"--trna-cutoff-score\": \"\"\n    },\n    \"anvi_script_reformat_fasta\": {\n        \"run\": true,\n        \"--prefix\": \"{group}\",\n        \"--simplify-names\": true,\n        \"--keep-ids\": \"\",\n        \"--exclude-ids\": \"\",\n        \"--min-len\": \"1000\",\n        \"--seq-type\": \"\",\n        \"threads\": 7\n    },\n    \"emapper\": {\n        \"--database\": \"bact\",\n        \"--usemem\": true,\n        \"--override\": true,\n        \"path_to_emapper_dir\": \"\",\n        \"threads\": \"\"\n    },\n    \"anvi_script_run_eggnog_mapper\": {\n        \"--use-version\": \"0.12.6\",\n        \"run\": \"\",\n        \"--cog-data-dir\": \"\",\n        \"--drop-previous-annotations\": \"\",\n        \"threads\": \"\"\n    },\n    \"samples_txt\": \"samples.txt\",\n    \"metaspades\": {\n        \"additional_params\": \"--only-assembler\",\n        \"threads\": 7,\n        \"run\": \"\",\n        \"use_scaffolds\": \"\"\n    },\n    \"megahit\": {\n        \"--min-contig-len\": 1000,\n        \"--memory\": 0.9,\n        \"threads\": 14,\n        \"run\": true,\n        \"--min-count\": \"\",\n        \"--k-min\": \"\",\n        \"--k-max\": \"\",\n        \"--k-step\": \"\",\n        \"--k-list\": \"\",\n        \"--no-mercy\": \"\",\n        \"--no-bubble\": \"\",\n        \"--merge-level\": \"\",\n        \"--prune-level\": \"\",\n        \"--prune-depth\": \"\",\n        \"--low-local-ratio\": \"\",\n        \"--max-tip-len\": \"\",\n        \"--no-local\": \"\",\n        \"--kmin-1pass\": \"\",\n        \"--presets\": \"meta-sensitive\",\n        \"--mem-flag\": \"\",\n        \"--use-gpu\": \"\",\n        \"--gpu-mem\": \"\",\n        \"--keep-tmp-files\": \"\",\n        \"--tmp-dir\": \"\",\n        \"--continue\": true,\n        \"--verbose\": \"\"\n    },\n    \"idba_ud\": {\n        \"--min_contig\": 1000,\n        \"threads\": 7,\n        \"run\": \"\",\n        \"--mink\": \"\",\n        \"--maxk\": \"\",\n        \"--step\": \"\",\n        \"--inner_mink\": \"\",\n        \"--inner_step\": \"\",\n        \"--prefix\": \"\",\n        \"--min_count\": \"\",\n        \"--min_support\": \"\",\n        \"--seed_kmer\": \"\",\n        \"--similar\": \"\",\n        \"--max_mismatch\": \"\",\n        \"--min_pairs\": \"\",\n        \"--no_bubble\": \"\",\n        \"--no_local\": \"\",\n        \"--no_coverage\": \"\",\n        \"--no_correct\": \"\",\n        \"--pre_correction\": \"\",\n        \"use_scaffolds\": \"\"\n    },\n    \"iu_filter_quality_minoche\": {\n        \"run\": true,\n        \"--ignore-deflines\": true,\n        \"--visualize-quality-curves\": \"\",\n        \"--limit-num-pairs\": \"\",\n        \"--print-qual-scores\": \"\",\n        \"--store-read-fate\": \"\",\n        \"threads\": 7\n    },\n    \"gzip_fastqs\": {\n        \"run\": true,\n        \"threads\": 7\n    },\n    \"bowtie\": {\n        \"additional_params\": \"--no-unal\",\n        \"threads\": 7\n    },\n    \"samtools_view\": {\n        \"additional_params\": \"-F 4\",\n        \"threads\": 7\n    },\n    \"anvi_profile\": {\n        \"threads\": 7,\n        \"--sample-name\": \"{sample}\",\n        \"--overwrite-output-destinations\": true,\n        \"--report-variability-full\": \"\",\n        \"--skip-SNV-profiling\": \"\",\n        \"--profile-SCVs\": true,\n        \"--description\": \"\",\n        \"--skip-hierarchical-clustering\": \"\",\n        \"--distance\": \"\",\n        \"--linkage\": \"\",\n        \"--min-contig-length\": \"\",\n        \"--min-mean-coverage\": \"\",\n        \"--min-coverage-for-variability\": \"\",\n        \"--cluster-contigs\": \"\",\n        \"--contigs-of-interest\": \"\",\n        \"--queue-size\": \"\",\n        \"--write-buffer-size-per-thread\": \"\",\n        \"--fetch-filter\": \"\",\n        \"--min-percent-identity\": \"\",\n        \"--max-contig-length\": \"\"\n    },\n    \"anvi_merge\": {\n        \"--sample-name\": \"{group}\",\n        \"--overwrite-output-destinations\": true,\n        \"--description\": \"\",\n        \"--skip-hierarchical-clustering\": \"\",\n        \"--enforce-hierarchical-clustering\": \"\",\n        \"--distance\": \"\",\n        \"--linkage\": \"\",\n        \"threads\": 14\n    },\n    \"import_percent_of_reads_mapped\": {\n        \"run\": true,\n        \"threads\": 7\n    },\n    \"krakenuniq\": {\n        \"threads\": 3,\n        \"--gzip-compressed\": true,\n        \"additional_params\": \"\",\n        \"run\": \"\",\n        \"--db\": \"\"\n    },\n    \"remove_short_reads_based_on_references\": {\n        \"delimiter-for-iu-remove-ids-from-fastq\": \" \",\n        \"dont_remove_just_map\": \"\",\n        \"references_for_removal_txt\": \"\",\n        \"threads\": \"\"\n    },\n    \"anvi_cluster_contigs\": {\n        \"--collection-name\": \"{driver}\",\n        \"run\": \"\",\n        \"--driver\": \"\",\n        \"--just-do-it\": \"\",\n        \"--additional-params-concoct\": \"\",\n        \"--additional-params-metabat2\": \"\",\n        \"--additional-params-maxbin2\": \"\",\n        \"--additional-params-dastool\": \"\",\n        \"--additional-params-binsanity\": \"\",\n        \"threads\": \"\"\n    },\n    \"gen_external_genome_file\": {\n        \"threads\": \"\"\n    },\n    \"export_gene_calls_for_centrifuge\": {\n        \"threads\": \"\"\n    },\n    \"anvi_import_taxonomy_for_genes\": {\n        \"threads\": \"\"\n    },\n    \"annotate_contigs_database\": {\n        \"threads\": \"\"\n    },\n    \"anvi_get_sequences_for_gene_calls\": {\n        \"threads\": \"\"\n    },\n    \"gunzip_fasta\": {\n        \"threads\": \"\"\n    },\n    \"reformat_external_gene_calls_table\": {\n        \"threads\": \"\"\n    },\n    \"reformat_external_functions\": {\n        \"threads\": \"\"\n    },\n    \"import_external_functions\": {\n        \"threads\": \"\"\n    },\n    \"anvi_run_pfams\": {\n        \"run\": true,\n        \"--pfam-data-dir\": \"/pool/genomics/stri_istmobiome/dbs/pfam_db\",\n        \"threads\": 14\n    },\n    \"iu_gen_configs\": {\n        \"--r1-prefix\": \"\",\n        \"--r2-prefix\": \"\",\n        \"threads\": 14\n    },\n    \"gen_qc_report\": {\n        \"threads\": \"\"\n    },\n    \"merge_fastqs_for_co_assembly\": {\n        \"threads\": \"\"\n    },\n    \"merge_fastas_for_co_assembly\": {\n        \"threads\": \"\"\n    },\n    \"bowtie_build\": {\n        \"additional_params\": \"\",\n        \"threads\": \"\"\n    },\n    \"anvi_init_bam\": {\n        \"threads\": \"\"\n    },\n    \"krakenuniq_mpa_report\": {\n        \"threads\": \"\"\n    },\n    \"import_krakenuniq_taxonomy\": {\n        \"--min-abundance\": \"\",\n        \"threads\": \"\"\n    },\n    \"anvi_summarize\": {\n        \"additional_params\": \"\",\n        \"run\": \"\",\n        \"threads\": \"\"\n    },\n    \"anvi_split\": {\n        \"additional_params\": \"\",\n        \"run\": \"\",\n        \"threads\": \"\"\n    },\n    \"references_mode\": \"\",\n    \"all_against_all\": \"\",\n    \"kraken_txt\": \"\",\n    \"collections_txt\": \"\",\n    \"output_dirs\": {\n        \"FASTA_DIR\": \"02_FASTA\",\n        \"CONTIGS_DIR\": \"03_CONTIGS\",\n        \"QC_DIR\": \"01_QC\",\n        \"MAPPING_DIR\": \"04_MAPPING\",\n        \"PROFILE_DIR\": \"05_ANVIO_PROFILE\",\n        \"MERGE_DIR\": \"06_MERGED\",\n        \"TAXONOMY_DIR\": \"07_TAXONOMY\",\n        \"SUMMARY_DIR\": \"08_SUMMARY\",\n        \"SPLIT_PROFILES_DIR\": \"09_SPLIT_PROFILES\",\n        \"LOGS_DIR\": \"00_LOGS\"\n    },\n    \"max_threads\": \"\",\n    \"config_version\": \"3\",\n    \"workflow_name\": \"metagenomics\"\n}\n\n\nC. A four-column tab-delimited text file where each row is a sample. The column headers are as follows:\n\n\nsample: the sample name\n\ngroup: group for co-assembly\n\nr1: sample forward reads\n\nr2: sample reverse reads\n\nAnd here is a little mock example. The file we used (samples.txt) is linked in the table above.\nsample  group   r1  r2\nEP_ALPH_AREN_GL_P01 EP  /path/to/files/sample-1_R1_001.fastq.gz /path/to/files/sample-1_R2_001.fastq.gz\nEP_ALPH_AREN_HP_P01 EP  /path/to/files/sample-2_R1_001.fastq.gz /path/to/files/sample-2_R2_001.fastq.gz\nWA_ALPH_WEBS_HP_P01 WA  /path/to/files/sample_3_R1_001.fastq.gz /path/to/files/sample-3_R2_001.fastq.gz\nWA_ALPH_WEBS_MG_P01 WA  /path/to/files/sample-4_R1_001.fastq.gz /path/to/files/sample-4_R2_001.fastq.gz\nD. And of course a bunch of fastq files (linked in the table above).\nOnce all of these pieces are in place, let er rip by running this command:\n\nanvi-run-workflow --workflow metagenomics \\\n                  --get-default-config default_mg.json  \\\n                  --list-dependencies\n\nThe entire pipeline can take several days to run depending on the size and complexity of your dataset.\nIn the resources listed above, we include a table that summarizes the initial QC screening for each sample."
  },
  {
    "objectID": "workflows/portal/data-dada2.html",
    "href": "workflows/portal/data-dada2.html",
    "title": "Data Portal",
    "section": "",
    "text": "Quick access to fastq sequence files, processing scripts, and curated datasets. If you want to process the data yourself, select a pipeline, download the sequence data and processing scripts, and run the workflow."
  },
  {
    "objectID": "workflows/portal/data-dada2.html#data-scripts",
    "href": "workflows/portal/data-dada2.html#data-scripts",
    "title": "Data Portal",
    "section": "Data & Scripts",
    "text": "Data & Scripts\n\n\n\n\n\n\nNoteA note about curated data\n\n\n\nCurated means that after processing, the data has been cleaned of unwanted taxa (e.g., NA kingdoms, potential contaminants, etc). Negative control & low-count samples have also been removed. These data are ready for downstream analysis.\n\n\nHere we provide two options for processing sequence data–the detailed pipeline (listed in the table) and a quick set of commands linked below.\n\n\n\n\nAsset\n\n\n\nFile Name\n\n\n\nDescription\n\n\n\n\n\n\nCurated Data\n\n\ndada2_curated_data.zip\n\n\nArchive containing the curated output of dada2 pipeline–-ASV table, taxonomy table, sample metadata, & representative fasta sequences. Data is available as a) stand-alone text files, b) bundled in a microtable object, and c) bundled in a phyloseq object. Archive also includes a table tracking sample read changes. \n\n\n\n\n\nSequence Data\n\n\nPRJEB36632\n\n\nTrimmed (primers removed) 16S rRNA data from 1909 samples. \n\n\n\n\n\nProcessing Data & Scripts\n\n\ndada2_processing.zip\n\n\nArchived directory containing fastq rename tables, fastq rename results, sample data, dada2 scripts, dada (command) results, run read changes, pipeline read changes, & Hydra job scripts. \n\n\n\n\n\nDetailed Pipeline\n\n\nASV workflow\n\n\nDetailed workflow for 16S rRNA ASV analysis using Dada2. Workflow uses paired end reads, beginning with raw fastq files, ending with sequence & taxonomy tables. Microtable & phyloseq objects are produced to collate the data for downstream analysis. \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "workflows/portal/data-dada2.html#asv-processing",
    "href": "workflows/portal/data-dada2.html#asv-processing",
    "title": "Data Portal",
    "section": "ASV Processing",
    "text": "ASV Processing\nIndividual Runs\nBelow are processing scripts for ASV analysis of individual sequencing runs using dada2. We performed a total of six 16S rRNA sequencing runs–BCS_26, BCS_28, BCS_29, BCS_30, BCS_34, and BCS_35. In the first workflow of the pipeline, runs are processed separately for error rates, dereplication, and ASV inference. At the end of each workflow, forward and reverse reads are merged. If you have the raw data and everything in place simply run like so:\n\nconda activate R\nRscript BCS_26.R\nRscript BCS_28.R\nRscript BCS_29.R\nRscript BCS_30.R\nRscript BCS_34.R\nRscript BCS_35.R\n\nHere are the scripts for the individual runs, which can also be downloaded from the table above.\n\n\nBCS_26\nBCS_28\nBCS_29\nBCS_30\nBCS_34\nBCS_35\n\n\n\n\nProcessing script for run BCS_26################################################################\n#####           TRANSISTHMIAN SHRIMP MICROBIOME            #####\n#####                 MISEQ RUN: BCS_26                    #####\n#####            PLATES ISTHMO S5, S5, S7, S8              #####\n################################################################\n#\n# SCRIPT prepared by Matthieu Leray\n# last modified January 28th 2023\n#\n# READ PROCESSING\n#\n# Modified by Jarrod Scott\n# last modified December 26th 2023\n################################################################\n\nlibrary(dada2)\n#library(ggplot2)\nlibrary(tidyverse)\nlibrary(ff)\nlibrary(phyloseq)\nlibrary(gridExtra)\nlibrary(dplyr)\nlibrary(decontam)\n\n##Creating filepaths to data \npath &lt;- \"/pool/genomics/stri_istmobiome/data/TRANS_SHRIMP/RAW_DATA/16S/BCS_26\"\nhead(list.files(path)) #eventually to check if the path works\n\n##File preparation\n#extracting Forward (fnFs) and Reverse (fnRs) reads from files\nfnFs &lt;- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names &lt;- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs &lt;-file.path(path, fnFs)\nfnRs &lt;-file.path(path, fnRs)\n\n##plotting quality profiles\n#qprofile_fwd &lt;- print(plotQualityProfile(fnFs, aggregate = TRUE, n = 20000) \n#                                   + ggtitle(\"Forward\"))\n#qprofile_rev &lt;- print(plotQualityProfile(fnRs, aggregate = TRUE, n = 20000) \n#                                   + ggtitle(\"Reverse\"))\n\n\n####### UNCOMMENT TO RUN ######################\n#x &lt;- length(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\n#\n#qprofile_fwd &lt;- plotQualityProfile(fnFs[1:x], aggregate = TRUE, n = 20000)\n#qprofile_rev &lt;- plotQualityProfile(fnRs[1:x], aggregate = TRUE, n = 20000)\n\n#qprofile &lt;- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\n#ggsave(\"figures/BCS_26_filt_plot_qscores.png\", qprofile, width = 7, height = 3)\n#\n########################################\n\n\n#placing filtered files in a new filtered subdirectory\nfiltFs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_F_filt.fastq\"))\nfiltRs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_R_filt.fastq\"))\nnames(filtFs) &lt;- sample.names\nnames(filtRs) &lt;- sample.names\n\n#filtering and trimming, here truncation at 220 (Fwd) and 180 (Rev) bp, \n#2expected errors max (N discarded automatically)\n\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(220,180),\n                        maxN=0, maxEE=2, truncQ=2, rm.phix=TRUE,\n                        compress=TRUE, multithread=20)\n\nhead(out)\n\n#learning error rates\nerrF &lt;- learnErrors(filtFs, multithread = TRUE)\nerrR &lt;- learnErrors(filtRs, multithread = TRUE)\n#plotting errors\n#plotErrors(errF, nominalQ=TRUE)\n#plotErrors(errR, nominalQ=TRUE)\n\n## ----plot_errF------------------------------\np3 &lt;- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_26_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_26_plot_errorF_2.png\", p3)\n## ----plot_errR------------------------------\np4 &lt;- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_26_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_26_plot_errorR_2.png\", p4)\n\n##Dereplicating reads\nsam.names &lt;- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs &lt;- derepFastq(filtFs)\nnames(derepFs) &lt;- sam.names\nderepRs &lt;- derepFastq(filtRs)\nnames(derepRs) &lt;- sam.names\n\n##Infering Sequence Variants\ndadaFs &lt;- dada(derepFs, err = errF, pool = \"pseudo\", multithread = TRUE)\ndadaFs[[1]]\ndadaRs &lt;- dada(derepRs, err = errR, pool = \"pseudo\", multithread = TRUE)\ndadaRs[[1]]\n\n##Merging paired ends\nmergers &lt;- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\nBCS_26 &lt;- makeSequenceTable(mergers)\ndim(BCS_26)\n# [1]   384 29481\ntable(nchar(getSequences(BCS_26)))\n\n#exporting files to use in the next part of the workflow\nsaveRDS(BCS_26, \"/pool/genomics/stri_istmobiome/data/TRANS_SHRIMP/16S/ASV/BCS_26/BCS_26.rds\")\n\n#tracking changes through each step\ngetN &lt;- function(x) sum(getUniques(x))\ntrack &lt;-    cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN),\n                  sapply(mergers, getN))\ncolnames(track) &lt;- c(\"input\", \"filtered\", \"denoisedF\", \"denoisedR\",\n                        \"merged\")\nrownames(track) &lt;- sam.names\nwrite.table(track, \"/pool/genomics/stri_istmobiome/data/TRANS_SHRIMP/16S/ASV/BCS_26/BCS_26_read_changes.txt\", sep = \"\\t\", quote = FALSE,\n            col.names=NA)\n\nread_length &lt;-  data.frame(nchar(getSequences(BCS_26)))\n\ncolnames(read_length) &lt;- \"length\"\n\nplot_BCS_26 &lt;- qplot(length, data = read_length, geom = \"histogram\", binwidth = 1, xlab = \"read length\", ylab = \"total variants\", xlim = c(225,275)) \nggsave(\"figures/read_length_before_pseudo_BCS_26.png\", plot_BCS_26, width = 7, height = 3)\n\nsave.image(\"BCS_26.rdata\")\n\n\n\n\n\nProcessing script for run BCS_28################################################################\n#####           TRANSISTHMIAN SHRIMP MICROBIOME            #####\n#####                 MISEQ RUN: BCS_28                    #####\n#####               PLATES ISTHMO S3, S4                   #####\n################################################################\n#\n# SCRIPT prepared by Matthieu Leray\n# last modified January 28th 2023\n#\n# READ PROCESSING\n#\n# Modified by Jarrod Scott\n# last modified April 1st 2024\n################################################################\n\nlibrary(dada2)\n#library(ggplot2)\nlibrary(tidyverse)\nlibrary(ff)\nlibrary(phyloseq)\nlibrary(gridExtra)\nlibrary(dplyr)\nlibrary(decontam)\n\n##Creating filepaths to data \npath &lt;- \"/pool/genomics/stri_istmobiome/data/TRANS_SHRIMP/RAW_DATA/16S/BCS_28\"\nhead(list.files(path)) #eventually to check if the path works\n\n##File preparation\n#extracting Forward (fnFs) and Reverse (fnRs) reads from files\nfnFs &lt;- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names &lt;- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs &lt;-file.path(path, fnFs)\nfnRs &lt;-file.path(path, fnRs)\n\n##plotting quality profiles\n##qprofile_fwd &lt;- print(plotQualityProfile(fnFs, aggregate = TRUE) \n##                                   + ggtitle(\"Forward\"))\n##qprofile_rev &lt;- print(plotQualityProfile(fnRs, aggregate = TRUE) \n##                                   + ggtitle(\"Reverse\"))\n\n\n####### UNCOMMENT TO RUN ######################\n#x &lt;- length(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\n\n#qprofile_fwd &lt;- plotQualityProfile(fnFs[1:x], aggregate = TRUE)\n#qprofile_rev &lt;- plotQualityProfile(fnRs[1:x], aggregate = TRUE)\n\n#qprofile &lt;- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\n#ggsave(\"figures/BCS_28_filt_plot_qscores.png\", qprofile, width = 7, height = 3)\n#\n########################################\n\n\n#placing filtered files in a new filtered subdirectory\nfiltFs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_F_filt.fastq\"))\nfiltRs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_R_filt.fastq\"))\nnames(filtFs) &lt;- sample.names\nnames(filtRs) &lt;- sample.names\n\n#filtering and trimming, here truncation at 220 (Fwd) and 180 (Rev) bp, \n#2expected errors max (N discarded automatically)\n\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(220,180),\n                        maxN=0, maxEE=2, truncQ=2, rm.phix=TRUE,\n                        compress=TRUE, multithread=20)\n\nhead(out)\n\n#learning error rates\nerrF &lt;- learnErrors(filtFs, multithread = TRUE)\nerrR &lt;- learnErrors(filtRs, multithread = TRUE)\n#plotting errors\n#plotErrors(errF, nominalQ=TRUE)\n#plotErrors(errR, nominalQ=TRUE)\n\n## ----plot_errF------------------------------\np3 &lt;- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_28_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_28_plot_errorF_2.png\", p3)\n## ----plot_errR------------------------------\np4 &lt;- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_28_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_28_plot_errorR_2.png\", p4)\n\n##Dereplicating reads\nsam.names &lt;- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs &lt;- derepFastq(filtFs)\nnames(derepFs) &lt;- sam.names\nderepRs &lt;- derepFastq(filtRs)\nnames(derepRs) &lt;- sam.names\n\n##Infering Sequence Variants\ndadaFs &lt;- dada(derepFs, err = errF, pool = \"pseudo\", multithread = TRUE)\ndadaFs[[1]]\ndadaRs &lt;- dada(derepRs, err = errR, pool = \"pseudo\", multithread = TRUE)\ndadaRs[[1]]\n\n##Merging paired ends\nmergers &lt;- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\nBCS_28 &lt;- makeSequenceTable(mergers)\ndim(BCS_28)\n# [1]   384 29481\ntable(nchar(getSequences(BCS_28)))\n\n#exporting files to use in the next part of the workflow\nsaveRDS(BCS_28, \"/pool/genomics/stri_istmobiome/data/TRANS_SHRIMP/16S/ASV/BCS_28/BCS_28.rds\")\n\n#tracking changes through each step\ngetN &lt;- function(x) sum(getUniques(x))\ntrack &lt;-    cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN),\n                  sapply(mergers, getN))\ncolnames(track) &lt;- c(\"input\", \"filtered\", \"denoisedF\", \"denoisedR\",\n                        \"merged\")\nrownames(track) &lt;- sam.names\nwrite.table(track, \"/pool/genomics/stri_istmobiome/data/TRANS_SHRIMP/16S/ASV/BCS_28/BCS_28_read_changes.txt\", sep = \"\\t\", quote = FALSE,\n            col.names=NA)\n\nread_length &lt;-  data.frame(nchar(getSequences(BCS_28)))\n\ncolnames(read_length) &lt;- \"length\"\n\nplot_BCS_28 &lt;- qplot(length, data = read_length, geom = \"histogram\", binwidth = 1, xlab = \"read length\", ylab = \"total variants\", xlim = c(225,275)) \nggsave(\"figures/read_length_before_pseudo_BCS_28.png\", plot_BCS_28, width = 7, height = 3)\n\nsave.image(\"BCS_28.rdata\")\n\n\n\n\n\nProcessing script for run BCS_29################################################################\n#####           TRANSISTHMIAN SHRIMP MICROBIOME            #####\n#####                 MISEQ RUN: BCS_29                    #####\n#####          PLATES ISTHMO S13, S14, S15, S16            #####\n################################################################\n#\n# SCRIPT prepared by Matthieu Leray\n# last modified January 28th 2023\n#\n# READ PROCESSING\n#\n# Modified by Jarrod Scott\n# last modified December 26th 2023\n################################################################\n\nlibrary(dada2)\n#library(ggplot2)\nlibrary(tidyverse)\nlibrary(ff)\nlibrary(phyloseq)\nlibrary(gridExtra)\nlibrary(dplyr)\nlibrary(decontam)\n\n##Creating filepaths to data \npath &lt;- \"/pool/genomics/stri_istmobiome/data/TRANS_SHRIMP/RAW_DATA/16S/BCS_29\"\nhead(list.files(path)) #eventually to check if the path works\n\n##File preparation\n#extracting Forward (fnFs) and Reverse (fnRs) reads from files\nfnFs &lt;- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names &lt;- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs &lt;-file.path(path, fnFs)\nfnRs &lt;-file.path(path, fnRs)\n\n##plotting quality profiles\n##qprofile_fwd &lt;- print(plotQualityProfile(fnFs, aggregate = TRUE) \n##                                   + ggtitle(\"Forward\"))\n##qprofile_rev &lt;- print(plotQualityProfile(fnRs, aggregate = TRUE) \n##                                   + ggtitle(\"Reverse\"))\n\n\n####### UNCOMMENT TO RUN ######################\n#x &lt;- length(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\n#\n#qprofile_fwd &lt;- plotQualityProfile(fnFs[1:x], aggregate = TRUE)\n#qprofile_rev &lt;- plotQualityProfile(fnRs[1:x], aggregate = TRUE)\n#\n#qprofile &lt;- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\n#ggsave(\"figures/BCS_29_filt_plot_qscores.png\", qprofile, width = 7, height = 3)\n#\n########################################\n\n\n#placing filtered files in a new filtered subdirectory\nfiltFs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_F_filt.fastq\"))\nfiltRs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_R_filt.fastq\"))\nnames(filtFs) &lt;- sample.names\nnames(filtRs) &lt;- sample.names\n\n#filtering and trimming, here truncation at 220 (Fwd) and 180 (Rev) bp, \n#2expected errors max (N discarded automatically)\n\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(220,180),\n                        maxN=0, maxEE=2, truncQ=2, rm.phix=TRUE,\n                        compress=TRUE, multithread=20)\n\nhead(out)\n\n#learning error rates\nerrF &lt;- learnErrors(filtFs, multithread = TRUE)\nerrR &lt;- learnErrors(filtRs, multithread = TRUE)\n#plotting errors\n#plotErrors(errF, nominalQ=TRUE)\n#plotErrors(errR, nominalQ=TRUE)\n\n## ----plot_errF------------------------------\np3 &lt;- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_29_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_29_plot_errorF_2.png\", p3)\n## ----plot_errR------------------------------\np4 &lt;- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_29_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_29_plot_errorR_2.png\", p4)\n\n##Dereplicating reads\nsam.names &lt;- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs &lt;- derepFastq(filtFs)\nnames(derepFs) &lt;- sam.names\nderepRs &lt;- derepFastq(filtRs)\nnames(derepRs) &lt;- sam.names\n\n##Infering Sequence Variants\ndadaFs &lt;- dada(derepFs, err = errF, pool = \"pseudo\", multithread = TRUE)\ndadaFs[[1]]\ndadaRs &lt;- dada(derepRs, err = errR, pool = \"pseudo\", multithread = TRUE)\ndadaRs[[1]]\n\n##Merging paired ends\nmergers &lt;- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\nBCS_29 &lt;- makeSequenceTable(mergers)\ndim(BCS_29)\n# [1]   384 29481\ntable(nchar(getSequences(BCS_29)))\n\n#exporting files to use in the next part of the workflow\nsaveRDS(BCS_29, \"/pool/genomics/stri_istmobiome/data/TRANS_SHRIMP/16S/ASV/BCS_29/BCS_29.rds\")\n\n#tracking changes through each step\ngetN &lt;- function(x) sum(getUniques(x))\ntrack &lt;-    cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN),\n                  sapply(mergers, getN))\ncolnames(track) &lt;- c(\"input\", \"filtered\", \"denoisedF\", \"denoisedR\",\n                        \"merged\")\nrownames(track) &lt;- sam.names\nwrite.table(track, \"/pool/genomics/stri_istmobiome/data/TRANS_SHRIMP/16S/ASV/BCS_29/BCS_29_read_changes.txt\", sep = \"\\t\", quote = FALSE,\n            col.names=NA)\n\nread_length &lt;-  data.frame(nchar(getSequences(BCS_29)))\n\ncolnames(read_length) &lt;- \"length\"\n\nplot_BCS_29 &lt;- qplot(length, data = read_length, geom = \"histogram\", binwidth = 1, xlab = \"read length\", ylab = \"total variants\", xlim = c(225,275)) \nggsave(\"figures/read_length_before_pseudo_BCS_29.png\", plot_BCS_29, width = 7, height = 3)\n\nsave.image(\"BCS_29.rdata\")\n\n\n\n\n\nProcessing script for run BCS_30################################################################\n#####           TRANSISTHMIAN SHRIMP MICROBIOME            #####\n#####                 MISEQ RUN: BCS_30                    #####\n#####          PLATES ISTHMO S17, S18, S19, S20            #####\n################################################################\n#\n# SCRIPT prepared by Matthieu Leray\n# last modified January 28th 2023\n#\n# READ PROCESSING\n#\n# Modified by Jarrod Scott\n# last modified December 26th 2023\n################################################################\n\nlibrary(dada2)\n#library(ggplot2)\nlibrary(tidyverse)\nlibrary(ff)\nlibrary(phyloseq)\nlibrary(gridExtra)\nlibrary(dplyr)\nlibrary(decontam)\n\n##Creating filepaths to data \npath &lt;- \"/pool/genomics/stri_istmobiome/data/TRANS_SHRIMP/RAW_DATA/16S/BCS_30\"\nhead(list.files(path)) #eventually to check if the path works\n\n##File preparation\n#extracting Forward (fnFs) and Reverse (fnRs) reads from files\nfnFs &lt;- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names &lt;- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs &lt;-file.path(path, fnFs)\nfnRs &lt;-file.path(path, fnRs)\n\n##plotting quality profiles\n##qprofile_fwd &lt;- print(plotQualityProfile(fnFs, aggregate = TRUE) \n##                                   + ggtitle(\"Forward\"))\n##qprofile_rev &lt;- print(plotQualityProfile(fnRs, aggregate = TRUE) \n##                                   + ggtitle(\"Reverse\"))\n\n\n####### UNCOMMENT TO RUN ######################\nx &lt;- length(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\n#\n#qprofile_fwd &lt;- plotQualityProfile(fnFs[1:x], aggregate = TRUE, n = 20000)\n#qprofile_rev &lt;- plotQualityProfile(fnRs[1:x], aggregate = TRUE, n = 20000)\n#\n#qprofile &lt;- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\n#ggsave(\"figures/BCS_30_filt_plot_qscores.png\", qprofile, width = 7, height = 3)\n#\n########################################\n\n\n#placing filtered files in a new filtered subdirectory\nfiltFs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_F_filt.fastq\"))\nfiltRs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_R_filt.fastq\"))\nnames(filtFs) &lt;- sample.names\nnames(filtRs) &lt;- sample.names\n\n#filtering and trimming, here truncation at 220 (Fwd) and 180 (Rev) bp, \n#2expected errors max (N discarded automatically)\n\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(220,180),\n                        maxN=0, maxEE=2, truncQ=2, rm.phix=TRUE,\n                        compress=TRUE, multithread=20)\n\nhead(out)\n\n#learning error rates\nerrF &lt;- learnErrors(filtFs, multithread = TRUE)\nerrR &lt;- learnErrors(filtRs, multithread = TRUE)\n#plotting errors\n#plotErrors(errF, nominalQ=TRUE)\n#plotErrors(errR, nominalQ=TRUE)\n\n## ----plot_errF------------------------------\np3 &lt;- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_30_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_30_plot_errorF_2.png\", p3)\n## ----plot_errR------------------------------\np4 &lt;- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_30_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_30_plot_errorR_2.png\", p4)\n\n##Dereplicating reads\nsam.names &lt;- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs &lt;- derepFastq(filtFs)\nnames(derepFs) &lt;- sam.names\nderepRs &lt;- derepFastq(filtRs)\nnames(derepRs) &lt;- sam.names\n\n##Infering Sequence Variants\ndadaFs &lt;- dada(derepFs, err = errF, pool = \"pseudo\", multithread = TRUE)\ndadaFs[[1]]\ndadaRs &lt;- dada(derepRs, err = errR, pool = \"pseudo\", multithread = TRUE)\ndadaRs[[1]]\n\n##Merging paired ends\nmergers &lt;- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\nBCS_30 &lt;- makeSequenceTable(mergers)\ndim(BCS_30)\n# [1]   384 29481\ntable(nchar(getSequences(BCS_30)))\n\n#exporting files to use in the next part of the workflow\nsaveRDS(BCS_30, \"/pool/genomics/stri_istmobiome/data/TRANS_SHRIMP/16S/ASV/BCS_30/BCS_30.rds\")\n\n#tracking changes through each step\ngetN &lt;- function(x) sum(getUniques(x))\ntrack &lt;-    cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN),\n                  sapply(mergers, getN))\ncolnames(track) &lt;- c(\"input\", \"filtered\", \"denoisedF\", \"denoisedR\",\n                        \"merged\")\nrownames(track) &lt;- sam.names\nwrite.table(track, \"/pool/genomics/stri_istmobiome/data/TRANS_SHRIMP/16S/ASV/BCS_30/BCS_30_read_changes.txt\", sep = \"\\t\", quote = FALSE,\n            col.names=NA)\n\nread_length &lt;-  data.frame(nchar(getSequences(BCS_30)))\n\ncolnames(read_length) &lt;- \"length\"\n\nplot_BCS_30 &lt;- qplot(length, data = read_length, geom = \"histogram\", binwidth = 1, xlab = \"read length\", ylab = \"total variants\", xlim = c(225,275)) \nggsave(\"figures/read_length_before_pseudo_BCS_30.png\", plot_BCS_30, width = 7, height = 3)\n\nsave.image(\"BCS_30.rdata\")\n\n\n\n\n\nProcessing script for run BCS_34################################################################\n#####           TRANSISTHMIAN SHRIMP MICROBIOME            #####\n#####                 MISEQ RUN: BCS_34                    #####\n#####               PLATES ISTHMO S01, S02                 #####\n################################################################\n#\n# SCRIPT prepared by Matthieu Leray\n# last modified January 28th 2023\n#\n# READ PROCESSING\n#\n# Modified by Jarrod Scott\n# last modified December 26th 2023\n################################################################\n\nlibrary(dada2)\n#library(ggplot2)\nlibrary(tidyverse)\nlibrary(ff)\nlibrary(phyloseq)\nlibrary(gridExtra)\nlibrary(dplyr)\nlibrary(decontam)\n\n##Creating filepaths to data \npath &lt;- \"/pool/genomics/stri_istmobiome/data/TRANS_SHRIMP/RAW_DATA/16S/BCS_34\"\nhead(list.files(path)) #eventually to check if the path works\n\n##File preparation\n#extracting Forward (fnFs) and Reverse (fnRs) reads from files\nfnFs &lt;- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names &lt;- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs &lt;-file.path(path, fnFs)\nfnRs &lt;-file.path(path, fnRs)\n\n##plotting quality profiles\n##qprofile_fwd &lt;- print(plotQualityProfile(fnFs, aggregate = TRUE) \n##                                   + ggtitle(\"Forward\"))\n##qprofile_rev &lt;- print(plotQualityProfile(fnRs, aggregate = TRUE) \n##                                   + ggtitle(\"Reverse\"))\n\n\n####### UNCOMMENT TO RUN ######################\n#x &lt;- length(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\n#\n#qprofile_fwd &lt;- plotQualityProfile(fnFs[1:x], aggregate = TRUE)\n#qprofile_rev &lt;- plotQualityProfile(fnRs[1:x], aggregate = TRUE)\n#\n#qprofile &lt;- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\n#ggsave(\"figures/BCS_34_filt_plot_qscores.png\", qprofile, width = 7, height = 3)\n#\n########################################\n\n\n#placing filtered files in a new filtered subdirectory\nfiltFs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_F_filt.fastq\"))\nfiltRs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_R_filt.fastq\"))\nnames(filtFs) &lt;- sample.names\nnames(filtRs) &lt;- sample.names\n\n#filtering and trimming, here truncation at 220 (Fwd) and 180 (Rev) bp, \n#2expected errors max (N discarded automatically)\n\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(220,180),\n                        maxN=0, maxEE=2, truncQ=2, rm.phix=TRUE,\n                        compress=TRUE, multithread=20)\n\nhead(out)\n\n#learning error rates\nerrF &lt;- learnErrors(filtFs, multithread = TRUE)\nerrR &lt;- learnErrors(filtRs, multithread = TRUE)\n#plotting errors\n#plotErrors(errF, nominalQ=TRUE)\n#plotErrors(errR, nominalQ=TRUE)\n\n## ----plot_errF------------------------------\np3 &lt;- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_34_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_34_plot_errorF_2.png\", p3)\n## ----plot_errR------------------------------\np4 &lt;- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_34_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_34_plot_errorR_2.png\", p4)\n\n##Dereplicating reads\nsam.names &lt;- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs &lt;- derepFastq(filtFs)\nnames(derepFs) &lt;- sam.names\nderepRs &lt;- derepFastq(filtRs)\nnames(derepRs) &lt;- sam.names\n\n##Infering Sequence Variants\ndadaFs &lt;- dada(derepFs, err = errF, pool = \"pseudo\", multithread = TRUE)\ndadaFs[[1]]\ndadaRs &lt;- dada(derepRs, err = errR, pool = \"pseudo\", multithread = TRUE)\ndadaRs[[1]]\n\n##Merging paired ends\nmergers &lt;- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\nBCS_34 &lt;- makeSequenceTable(mergers)\ndim(BCS_34)\n# [1]   384 29481\ntable(nchar(getSequences(BCS_34)))\n\n#exporting files to use in the next part of the workflow\nsaveRDS(BCS_34, \"/pool/genomics/stri_istmobiome/data/TRANS_SHRIMP/16S/ASV/BCS_34/BCS_34.rds\")\n\n#tracking changes through each step\ngetN &lt;- function(x) sum(getUniques(x))\ntrack &lt;-    cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN),\n                  sapply(mergers, getN))\ncolnames(track) &lt;- c(\"input\", \"filtered\", \"denoisedF\", \"denoisedR\",\n                        \"merged\")\nrownames(track) &lt;- sam.names\nwrite.table(track, \"/pool/genomics/stri_istmobiome/data/TRANS_SHRIMP/16S/ASV/BCS_34/BCS_34_read_changes.txt\", sep = \"\\t\", quote = FALSE,\n            col.names=NA)\n\nread_length &lt;-  data.frame(nchar(getSequences(BCS_34)))\n\ncolnames(read_length) &lt;- \"length\"\n\nplot_BCS_34 &lt;- qplot(length, data = read_length, geom = \"histogram\", binwidth = 1, xlab = \"read length\", ylab = \"total variants\", xlim = c(225,275)) \nggsave(\"figures/read_length_before_pseudo_BCS_34.png\", plot_BCS_34, width = 7, height = 3)\n\nsave.image(\"BCS_34.rdata\")\n\n\n\n\n\nProcessing script for run BCS_35################################################################\n#####           TRANSISTHMIAN SHRIMP MICROBIOME            #####\n#####                 MISEQ RUN: BCS_35                    #####\n#####           PLATES ISTHMO S9, S10, S11, S12            #####\n################################################################\n#\n# SCRIPT prepared by Matthieu Leray\n# last modified January 28th 2023\n#\n# READ PROCESSING\n#\n# Modified by Jarrod Scott\n# last modified December 26th 2023\n################################################################\n\nlibrary(dada2)\n#library(ggplot2)\nlibrary(tidyverse)\nlibrary(ff)\nlibrary(phyloseq)\nlibrary(gridExtra)\nlibrary(dplyr)\nlibrary(decontam)\n\n##Creating filepaths to data \npath &lt;- \"/pool/genomics/stri_istmobiome/data/TRANS_SHRIMP/RAW_DATA/16S/BCS_35\"\nhead(list.files(path)) #eventually to check if the path works\n\n##File preparation\n#extracting Forward (fnFs) and Reverse (fnRs) reads from files\nfnFs &lt;- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names &lt;- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs &lt;-file.path(path, fnFs)\nfnRs &lt;-file.path(path, fnRs)\n\n##plotting quality profiles\n##qprofile_fwd &lt;- print(plotQualityProfile(fnFs, aggregate = TRUE) \n##                                   + ggtitle(\"Forward\"))\n##qprofile_rev &lt;- print(plotQualityProfile(fnRs, aggregate = TRUE) \n##                                   + ggtitle(\"Reverse\"))\n\n\n####### UNCOMMENT TO RUN ######################\n#x &lt;- length(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\n#\n#qprofile_fwd &lt;- plotQualityProfile(fnFs[1:x], aggregate = TRUE)\n#qprofile_rev &lt;- plotQualityProfile(fnRs[1:x], aggregate = TRUE)\n#\n#qprofile &lt;- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\n#ggsave(\"figures/BCS_35_filt_plot_qscores.png\", qprofile, width = 7, height = 3)\n#\n########################################\n\n\n#placing filtered files in a new filtered subdirectory\nfiltFs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_F_filt.fastq\"))\nfiltRs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_R_filt.fastq\"))\nnames(filtFs) &lt;- sample.names\nnames(filtRs) &lt;- sample.names\n\n#filtering and trimming, here truncation at 220 (Fwd) and 180 (Rev) bp, \n#2expected errors max (N discarded automatically)\n\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(220,180),\n                        maxN=0, maxEE=2, truncQ=2, rm.phix=TRUE,\n                        compress=TRUE, multithread=20)\n\nhead(out)\n\n#learning error rates\nerrF &lt;- learnErrors(filtFs, multithread = TRUE)\nerrR &lt;- learnErrors(filtRs, multithread = TRUE)\n#plotting errors\n#plotErrors(errF, nominalQ=TRUE)\n#plotErrors(errR, nominalQ=TRUE)\n\n## ----plot_errF------------------------------\np3 &lt;- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_35_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_35_plot_errorF_2.png\", p3)\n## ----plot_errR------------------------------\np4 &lt;- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_35_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_35_plot_errorR_2.png\", p4)\n\n##Dereplicating reads\nsam.names &lt;- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs &lt;- derepFastq(filtFs)\nnames(derepFs) &lt;- sam.names\nderepRs &lt;- derepFastq(filtRs)\nnames(derepRs) &lt;- sam.names\n\n##Infering Sequence Variants\ndadaFs &lt;- dada(derepFs, err = errF, pool = \"pseudo\", multithread = TRUE)\ndadaFs[[1]]\ndadaRs &lt;- dada(derepRs, err = errR, pool = \"pseudo\", multithread = TRUE)\ndadaRs[[1]]\n\n##Merging paired ends\nmergers &lt;- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\nBCS_35 &lt;- makeSequenceTable(mergers)\ndim(BCS_35)\n# [1]   384 29481\ntable(nchar(getSequences(BCS_35)))\n\n#exporting files to use in the next part of the workflow\nsaveRDS(BCS_35, \"/pool/genomics/stri_istmobiome/data/TRANS_SHRIMP/16S/ASV/BCS_35/BCS_35.rds\")\n\n#tracking changes through each step\ngetN &lt;- function(x) sum(getUniques(x))\ntrack &lt;-    cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN),\n                  sapply(mergers, getN))\ncolnames(track) &lt;- c(\"input\", \"filtered\", \"denoisedF\", \"denoisedR\",\n                        \"merged\")\nrownames(track) &lt;- sam.names\nwrite.table(track, \"/pool/genomics/stri_istmobiome/data/TRANS_SHRIMP/16S/ASV/BCS_35/BCS_35_read_changes.txt\", sep = \"\\t\", quote = FALSE,\n            col.names=NA)\n\nread_length &lt;-  data.frame(nchar(getSequences(BCS_35)))\n\ncolnames(read_length) &lt;- \"length\"\n\nplot_BCS_35 &lt;- qplot(length, data = read_length, geom = \"histogram\", binwidth = 1, xlab = \"read length\", ylab = \"total variants\", xlim = c(225,275)) \nggsave(\"figures/read_length_before_pseudo_BCS_35.png\", plot_BCS_35, width = 7, height = 3)\n\nsave.image(\"BCS_35.rdata\")\n\n\n\n\n\nMerged Runs\nOnce these workflows finish, we then merge the six sequence tables together and proceed with chimera removal and taxonomic classification.\n\nProcessing script for run merged runs#!/usr/bin/env Rscript\nset.seed(919191)\nlibrary(dada2); packageVersion(\"dada2\")\nlibrary(ggplot2)\nlibrary(ff)\nlibrary(phyloseq)\nlibrary(gridExtra)\nlibrary(dplyr)\nlibrary(decontam)\nlibrary(grid)\nlibrary(ShortRead); packageVersion(\"ShortRead\")\nlibrary(Biostrings); packageVersion(\"Biostrings\")\nlibrary(DECIPHER); packageVersion(\"DECIPHER\")\n\n########################################\n#\n# 2. MERGE ALL SEQ TABS\n#\n########################################\n\n############################################\n# PROBLEM: Duplicated sample names \n# detected in the sequence table row names: \n# 7512-G, 7512-H, 7512-M, 7512-S\n# BCS_30 and BCS_35\n# FOR BCS_30 changed name to 7512A...\n# FOR BCS_35 changed name to 7512B...\n############################################\n\nBCS_26 &lt;- readRDS(\"BCS_26/BCS_26.rds\")\nBCS_28 &lt;- readRDS(\"BCS_28/BCS_28.rds\")\nBCS_29 &lt;- readRDS(\"BCS_29/BCS_29.rds\")\nBCS_30 &lt;- readRDS(\"BCS_30/BCS_30.rds\")\nBCS_34 &lt;- readRDS(\"BCS_34/BCS_34.rds\")\nBCS_35 &lt;- readRDS(\"BCS_35/BCS_35.rds\")\n\nseqtab.merge &lt;- mergeSequenceTables(BCS_26, BCS_28, BCS_29, BCS_30, BCS_34, BCS_35)\ndim(seqtab.merge)\ntable(nchar(getSequences(seqtab.merge)))\n\nread_length_all &lt;-  data.frame(nchar(getSequences(seqtab.merge)))\ncolnames(read_length_all) &lt;- \"length\"\nplot_all &lt;- qplot(length, data = read_length_all, geom = \"histogram\", binwidth = 1, xlab = \"read length\", ylab = \"total variants\", xlim = c(200,400)) \nggsave(\"figures/read_length_before_collapse.png\", plot_all, width = 7, height = 3)\nsaveRDS(seqtab.merge, \"2.seqtab.merge.rds\")\n\nsave.image(\"rdata/2.merge.seqtabs.rdata\")\n\n########################################\n#\n# collapseNoMismatch\n# TESTED, only minor differences in\n# 13 samples. Takes long time to run\n#\n########################################\n\n# seqtab_to_collapse &lt;- collapseNoMismatch(st_all, minOverlap = 20, orderBy = \"abundance\",\n#   identicalOnly = FALSE, vec = TRUE, band = -1, verbose = TRUE)\n\n# dim(seqtab_to_collapse)\n# table(nchar(getSequences(seqtab_to_collapse)))\n\n# read_length_all_collapse &lt;-  data.frame(nchar(getSequences(seqtab_to_collapse)))\n# colnames(read_length_all_collapse) &lt;- \"length\"\n# plot_all_collapse &lt;- qplot(length, data = read_length_all_collapse, geom = \"histogram\", binwidth = 1, xlab = \"read length\", ylab = \"total variants\", xlim = c(200,400)) \n# ggsave(\"figures/read_length_after_collapse.png\", plot_all_collapse, width = 7, height = 3)\n# saveRDS(seqtab_to_collapse, \"seqtab_after_collapse.rds\")\n\n# save.image(\"rdata/2_merge_seqtabs_collapsed.rdata\")\n\n########################################\n#\n# 3. REMOVING CHIMERAS\n#\n########################################\n\n############################################\n# PROBLEM: Duplicated sample names \n# detected in the sequence table row names: \n# 7512-G, 7512-H, 7512-M, 7512-S\n# BCS_30 and BCS_35\n# FOR BCS_30 changed name to 7512A...\n# FOR BCS_35 changed name to 7512B...\n############################################\n\n## REMVOE OUTLIER READ LENGTHS\nseqtab &lt;- seqtab.merge\n#seqtab.merge &lt;- readRDS(\"seqtab_before_collapse.rds\")\ntable(nchar(getSequences(seqtab)))\n\n################################################################################# \n## \n## 220   221   222   223   224   225   226   227   228   229   230   231   232\n##   125    67    14    36    20    13    10    25     9     6     4    27     2\n##   234   235   236   237   238   239   240   241   242   243   244   245   246\n##     9     8  1373   151    46     6    99   407   298   452    31    14    13\n##   247   248   249   250   251   252   253   254   255   256   257   258   259\n##    26    23    19    49   159  3587 84485  3772   319   123    96    20    10\n##   260   261   262   263   264   265   266   267   268   269   270   271   272\n##     8    16     9     4     2     1     1     1     4     2     9     8     4\n##   273   274   275   276   277   278   279   280   281   282   284   285   286\n##     7     3     2     5     1     7     4     1     1     2     1     4     4\n##   288   289   290   291   292   293   294   295   296   297   298   300   303\n##     1     3     1     2     4     8     7     2     3     2     3     2     3\n##   304   305   307   308   309   310   311   312   313   315   316   317   318\n##     1     5     2     3     2     1     3     1     3     1     4     1     3\n##   319   320   321   322   323   324   325   326   328   329   330   332   333\n##     3     1     2     3     2     1     3     1     3     3     2     1     3\n##   334   335   336   337   338   339   340   341   342   343   344   345   346\n##    13     6     7    18     5    25    16    70    52     8     7     8     4\n##   347   348   349   350   351   352   353   354   355   356   357   358   359\n##    25    17    21    10     2    11     1     1     7     6    31     6    15\n##   360   361   362   363   364   365   366   367   368   369   370   371   372\n##    21   161   188    43   141   108    19     9    26     5     3     3     8\n##   373   374   376   377   378   379   380   384   385   386   387   388\n##    11     2     3     5     2     3     1     1     1     1     2     1\n##\n#################################################################################\n\n#######################################################\n## ----REMOVE OUTLIER READ LENGTHS------------------ ##\n#######################################################\n\nseqtab.trim &lt;- seqtab[,nchar(colnames(seqtab)) %in% seq(252, 254)]\ndim(seqtab.trim)\ntable(nchar(getSequences(seqtab.trim)))\n\n#####################\n##   252   253   254\n##  3587 84485  3772\n#####################\n\n#######################################################\n## ----chimera pooled------------------------------- ##\n#######################################################\n\nseqtab.trim.nochim.pool &lt;- removeBimeraDenovo(seqtab.trim, method = \"pooled\", multithread = 20, verbose = TRUE)\ndim(seqtab.trim.nochim.pool)\nsum(seqtab.trim.nochim.pool)/sum(seqtab.trim)\n\nsaveRDS(seqtab.trim.nochim.pool, \"3.seqtab.trim.nochim.pool.rds\")\n\ntable(nchar(getSequences(seqtab.trim.nochim.pool)))\n\ntable(colSums(seqtab.trim.nochim.pool &gt; 0))\ntable(rowSums(seqtab.trim.nochim.pool &gt; 0))\n\n##########################################################\n## ----chimera consensus------------------------------- ##\n##########################################################\n\nseqtab.trim.nochim.consensus &lt;- removeBimeraDenovo(seqtab.trim, method = \"consensus\", multithread = 20, verbose = TRUE)\ndim(seqtab.trim.nochim.consensus)\nsum(seqtab.trim.nochim.consensus)/sum(seqtab.trim)\n\nsaveRDS(seqtab.trim.nochim.consensus, \"3.seqtab.trim.nochim.consensus.rds\")\n\ntable(nchar(getSequences(seqtab.trim.nochim.consensus)))\n\ntable(colSums(seqtab.trim.nochim.consensus &gt; 0))\ntable(rowSums(seqtab.trim.nochim.consensus &gt; 0))\n\n##########################################################\n## ----tracking changes-------------------------------- ##\n##########################################################\n\ngetN &lt;- function(x) sum(getUniques(x))\ntrack &lt;- cbind(rowSums(seqtab), \n               rowSums(seqtab.trim), \n               rowSums(seqtab.trim.nochim.pool), \n               rowSums(seqtab.trim.nochim.consensus))\n\ncolnames(track) &lt;- c(\"merged\", \"trim\", \"chimera_pool\", \"chimera_concensus\")\nwrite.table(track, \"3.chimera_read_changes_pipeline.txt\", sep = \"\\t\", quote = FALSE,\n            col.names=NA)\n\nsave.image(\"rdata/3.trim.chimera.rdata\")\n\n########################################\n#\n# 4. ASSIGNING TAXONOMY\n#\n########################################\n\n###########################################################\n# reference datasets formatted for DADA2 can be found here: \n# https://benjjneb.github.io/dada2/training.html\n###########################################################\n\n########################################\n#\n# TAXONOMY chimera = pooled\n#\n########################################\n\n# seqtab &lt;- readRDS(\"3.seqtab.trim.nochim.pool.rds\")\n\n########################################\n# TAXONOMY = silva\n########################################\nseqtab.pool &lt;- seqtab.trim.nochim.pool\n\ntax_silva_v138.pool &lt;- assignTaxonomy(seqtab.pool, \"TAXONOMY_FILES/silva_nr99_v138.1_train_set.fa.gz\", multithread = TRUE, verbose = TRUE)\nsaveRDS(tax_silva_v138.pool, \"4.tax_silva_v138.pool.rds\")\n\ntax_silva_v132.pool &lt;- assignTaxonomy(seqtab.pool, \"TAXONOMY_FILES/silva_nr_v132_train_set.fa.gz\", multithread = TRUE, verbose = TRUE)\nsaveRDS(tax_silva_v132.pool, \"4.tax_silva_v132.pool.rds\")\n\n########################################\n# TAXONOMY = RDP\n########################################\n\ntax_rdp_v138.pool &lt;- assignTaxonomy(seqtab.pool, \"TAXONOMY_FILES/rdp_train_set_18.fa.gz\", multithread = TRUE, verbose = TRUE)\nsaveRDS(tax_rdp_v138.pool, \"4.tax_rdp_v138.pool.rds\")\n\n########################################\n#\n# TAXONOMY chimera = consensus\n#\n########################################\n\n#remove(list = ls())\n#seqtab &lt;- readRDS(\"3.seqtab.trim.nochim.consensus.rds\")\n#objects()\n\n########################################\n# TAXONOMY = silva\n########################################\nseqtab.consensus &lt;- seqtab.trim.nochim.consensus\n\ntax_silva_v138.consensus &lt;- assignTaxonomy(seqtab.consensus, \"TAXONOMY_FILES/silva_nr99_v138.1_train_set.fa.gz\", multithread = TRUE, verbose = TRUE)\nsaveRDS(tax_silva_v138.consensus, \"4.tax_silva_v138.consensus.rds\")\n\ntax_silva_v132.consensus &lt;- assignTaxonomy(seqtab.consensus, \"TAXONOMY_FILES/silva_nr_v132_train_set.fa.gz\", multithread = TRUE, verbose = TRUE)\nsaveRDS(tax_silva_v132.consensus, \"4.tax_silva_v132.consensus.rds\")\n\n########################################\n# TAXONOMY = RDP\n########################################\n\ntax_rdp_v138.consensus &lt;- assignTaxonomy(seqtab.consensus, \"TAXONOMY_FILES/rdp_train_set_18.fa.gz\", multithread = TRUE, verbose = TRUE)\nsaveRDS(tax_rdp_v138.consensus, \"4.tax_rdp_v138.consensus.rds\")\n\n########################################\n# TAXONOMY = ITGDB\n########################################\n\ntax_itgdb.consensus &lt;- assignTaxonomy(seqtab.consensus, \"TAXONOMY_FILES/itgdb_dada2.fa\", multithread = TRUE, verbose = TRUE)\nsaveRDS(tax_itgdb.consensus, \"4.tax_itgdb.consensus.rds\")\n\n########################################\n# TAXONOMY = GSRDB\n########################################\n\ntax_gsrdb.consensus &lt;- assignTaxonomy(seqtab.consensus, \"TAXONOMY_FILES/gsrdb_dada2.fa\", multithread = TRUE, verbose = TRUE)\nsaveRDS(tax_gsrdb.consensus, \"4.tax_gsrdb.consensus.rds\")\n\n\nsave.image(\"rdata/4.dada2.pipeline.rdata\")\n\nsessionInfo()\ndevtools::session_info()\n\nquit()\n\n\n\nconda activate R\nRscript merge_asv_workflow.R\n\nIn the resources listed above, we include a table that summarizes read changes for each sample through the pipeline."
  },
  {
    "objectID": "workflows/mg/index.html",
    "href": "workflows/mg/index.html",
    "title": "Metagenomic Workflows",
    "section": "",
    "text": "1. Build Databases\n\n\nConstructing taxonomic & functional annotation databases.\n\n\n\n\n\n9/30/25, 11:36:28 AM\n\n\n\n\n\n\n\n2. Co-Assembly & Annotations\n\n\nThis section describes the steps we took to co-assemble the metagenomic samples, classify taxonomy, and assign functions.\n\n\n\n\n\n9/30/25, 1:00:32 PM\n\n\n\n\n\n\n\n3. Assembly & Annotation Results\n\n\nThis section describes the steps we took to co-assemble the metagenomic samples, classify taxonomy, and assign functions.\n\n\n\n\n\n9/30/25, 1:11:26 PM\n\n\n\n\n\nNo matching items\nCollected Links\n\n\nQuarto\nmetacrobe\nThe Istmobiome Project",
    "crumbs": [
      "Home",
      "Metagenomics"
    ]
  },
  {
    "objectID": "workflows/mg/aa-results/index.html",
    "href": "workflows/mg/aa-results/index.html",
    "title": "3. Assembly & Annotation Results",
    "section": "",
    "text": "The first thing we should do is look at the results of the initial QC step. For each sample, anvi’o spits out individual quality control reports. Thankfully anvi’o also concatenates those files into one table. This table contains information like the number of pairs analyzed, the total pairs passed, etc.",
    "crumbs": [
      "Home",
      "Metagenomics",
      "3. Assembly & Annotation Results"
    ]
  },
  {
    "objectID": "workflows/mg/aa-results/index.html#qc-results",
    "href": "workflows/mg/aa-results/index.html#qc-results",
    "title": "3. Assembly & Annotation Results",
    "section": "",
    "text": "The first thing we should do is look at the results of the initial QC step. For each sample, anvi’o spits out individual quality control reports. Thankfully anvi’o also concatenates those files into one table. This table contains information like the number of pairs analyzed, the total pairs passed, etc.",
    "crumbs": [
      "Home",
      "Metagenomics",
      "3. Assembly & Annotation Results"
    ]
  },
  {
    "objectID": "workflows/mg/aa-results/index.html#assembly-results",
    "href": "workflows/mg/aa-results/index.html#assembly-results",
    "title": "3. Assembly & Annotation Results",
    "section": "Assembly Results",
    "text": "Assembly Results\nNext we can look at the results of the co-assembly, the number of HMM hits, and the estimated number of predicted genomes. These data not only give us a general idea of assembly quality but will also help us decide parameters for automatic clustering down the road.\nWe can use anvi’o to generate a simple table of contig stats for this assembly.\n\nanvi-display-contigs-stats PAN-contigs.db \\\n                           --output-file contig-stats.txt \\\n                           --report-as-text",
    "crumbs": [
      "Home",
      "Metagenomics",
      "3. Assembly & Annotation Results"
    ]
  },
  {
    "objectID": "workflows/mg/aa-results/index.html#short-read-taxonomy",
    "href": "workflows/mg/aa-results/index.html#short-read-taxonomy",
    "title": "3. Assembly & Annotation Results",
    "section": "Short-read Taxonomy",
    "text": "Short-read Taxonomy\nSince the Kraken2 classification was performed BEFORE the assembly we can look at the Krona plots for each individual sample. Here samples are separated by site.\n\nanvi-export-table PROFILE.db --table layer_additional_data \\\n                             --output-file  layer_additional_data.txt \\\n                             --index item_name \\\n                             --columns data_key \\\n                             --values data_value \\\n                             --matrix-format",
    "crumbs": [
      "Home",
      "Metagenomics",
      "3. Assembly & Annotation Results"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Transisthmian Shrimp Microbiomes",
    "section": "",
    "text": "Data Portal \n Publication \n 16S rRNA \n Metagenomics"
  },
  {
    "objectID": "site_info.html",
    "href": "site_info.html",
    "title": "Site Information",
    "section": "",
    "text": "Property\n\n\nValue\n\n\n\n\n\nOperating System\n\n\nmacOS\n\n\n\n\nLua Version\n\n\nLua 5.4\n\n\n\n\n\n\n\n\n\n\n\nEnvironment Variable\n\n\nValue\n\n\n\n\n\nQUARTO_ROOT\n\n\n/\n\n\n\n\nQUARTO_SHARE_PATH\n\n\n/Applications/quarto/share\n\n\n\n\nQUARTO_BIN_PATH\n\n\n/Applications/quarto/bin\n\n\n\n\n\n\n\n\n\n\n\nProperty\n\n\nValue\n\n\n\n\nquarto.project.output_directory\n\n\n/Users/rad/Smithsonian Dropbox/Jarrod Scott/GITHUB_PROJECTS/TRANS_ISTHMIAN/shrimp-draft/docs\n\n\n\n\n\n\n\n\n\n\nSoftware\n\n\nVersion\n\n\n\n\n\nQuarto Version\n\n\n1.8.25\n\n\n\n\nPandoc Version\n\n\n3.6.3\n\n\n\n\nR Version\n\n\n4.5.1\n\n\n\n\nPython Version\n\n\n3.11.5\n\n\n\n\n\n\n\n\n\n\n\n\nName\n\n\nVersion\n\n\nLocation\n\n\n\n\n\ndetails\n\n\n1.0.0\n\n\njmgirard/details\n\n\n\n\nfontawesome\n\n\n1.2.0\n\n\nquarto-ext/fontawesome\n\n\n\n\nlinkate\n\n\n0.0.0-dev.1\n\n\ncoatless-quarto/linkate\n\n\n\n\nstamp\n\n\n0.0.0-dev.1\n\n\ncoatless-quarto/stamp"
  },
  {
    "objectID": "site_info.html#document-information",
    "href": "site_info.html#document-information",
    "title": "Site Information",
    "section": "",
    "text": "Property\n\n\nValue\n\n\n\n\n\nOperating System\n\n\nmacOS\n\n\n\n\nLua Version\n\n\nLua 5.4\n\n\n\n\n\n\n\n\n\n\n\nEnvironment Variable\n\n\nValue\n\n\n\n\n\nQUARTO_ROOT\n\n\n/\n\n\n\n\nQUARTO_SHARE_PATH\n\n\n/Applications/quarto/share\n\n\n\n\nQUARTO_BIN_PATH\n\n\n/Applications/quarto/bin\n\n\n\n\n\n\n\n\n\n\n\nProperty\n\n\nValue\n\n\n\n\nquarto.project.output_directory\n\n\n/Users/rad/Smithsonian Dropbox/Jarrod Scott/GITHUB_PROJECTS/TRANS_ISTHMIAN/shrimp-draft/docs\n\n\n\n\n\n\n\n\n\n\nSoftware\n\n\nVersion\n\n\n\n\n\nQuarto Version\n\n\n1.8.25\n\n\n\n\nPandoc Version\n\n\n3.6.3\n\n\n\n\nR Version\n\n\n4.5.1\n\n\n\n\nPython Version\n\n\n3.11.5\n\n\n\n\n\n\n\n\n\n\n\n\nName\n\n\nVersion\n\n\nLocation\n\n\n\n\n\ndetails\n\n\n1.0.0\n\n\njmgirard/details\n\n\n\n\nfontawesome\n\n\n1.2.0\n\n\nquarto-ext/fontawesome\n\n\n\n\nlinkate\n\n\n0.0.0-dev.1\n\n\ncoatless-quarto/linkate\n\n\n\n\nstamp\n\n\n0.0.0-dev.1\n\n\ncoatless-quarto/stamp"
  },
  {
    "objectID": "workflows/mg/assembly/index.html",
    "href": "workflows/mg/assembly/index.html",
    "title": "2. Co-Assembly & Annotations",
    "section": "",
    "text": "PENDING",
    "crumbs": [
      "Home",
      "Metagenomics",
      "2. Co-Assembly & Annotations"
    ]
  },
  {
    "objectID": "workflows/mg/assembly/index.html#snakemake-citations",
    "href": "workflows/mg/assembly/index.html#snakemake-citations",
    "title": "2. Co-Assembly & Annotations",
    "section": "Snakemake Citations",
    "text": "Snakemake Citations\nThere are many tools used in the workflow that need to be cited. See the Workflow References page for details.\nThere are also a few tools that we ran outside of the Snakemake workflow. Results from these steps need to be added to the individual PROFILE.db’s, merged PROFILE.db, or CONTIGS.db. Therefore, before the anvi-merge portion of the Snakemake workflow finished, we killed the job, ran the accessory analyses described below, and then restarted the workflow to finish the missing step. Cumbersome, yes, but it got the job done.",
    "crumbs": [
      "Home",
      "Metagenomics",
      "2. Co-Assembly & Annotations"
    ]
  },
  {
    "objectID": "workflows/mg/assembly/index.html#taxonomic",
    "href": "workflows/mg/assembly/index.html#taxonomic",
    "title": "2. Co-Assembly & Annotations",
    "section": "Taxonomic",
    "text": "Taxonomic\nIn this section we discuss taxonomic classification of short reads, contigs, and gene calls. We go through the steps of analyzing the data and getting the results into anvi’o databases.\nShort-reads with Kraken2\nIn this section we use Kraken2 (Wood, Lu, and Langmead 2019) to classify the short reads. Our goal is to classify short-reads, generate an input file for anvi’o, and create Krona plots for data visualization. Brace yourself.\n\n\n\n\n\n\nImportant\n\n\n\nSince Kraken2 annotation is performed on individual samples and the results are imported into the individual profile.db’s we will need to re-merge the all profile.db after these steps are completed. The merging step is basically the last step of the Snakemake workflow.\n\n\n\nconda activate kraken2\nfor sample in `cat sampleskraken2.txt`\ndo\n    kraken2 --paired 01_QC/$sample-QUALITY_PASSED_R1.fastq.gz  \\\n                     01_QC/$sample-QUALITY_PASSED_R2.fastq.gz \\\n                     --db kraken2_db/ \\\n                     --use-names \\\n                     --threads $NSLOTS \\\n                     --output $sample-kraken.out \\\n                     --report $sample-kraken-report.txt\ndone\nconda deactivate \n\nAfter this is finished we should have two files for each sample–a .out file containing the results of the Kraken2 annotation and a .report.txt file that summarizes the results.\nFirst, we generate the file that anvi’o needs–the format is very specific. For this task we use KrakenTool–a suite of very handy scripts to Kraken 2 data. We will use a tool called kreport2mpa.py, which takes a Kraken report file and prints out a MPA (MetaPhlAn)-style TEXT file.\n\nconda activate krakentools\nfor sample in `cat sampleskraken2.txt`\ndo\n    kreport2mpa.py -r $sample-kraken-report.txt -o $sample-kraken-mpa.txt    \ndone\nconda deactivate \n\nEasy as that. Now we can import all MPA files into their respective contig databases. Here we had to split the sample list by co-assembly because I could not figure out an easier way.\n\nfor sample in `cat EP_list.txt`\ndo \n    anvi-import-taxonomy-for-layers \\\n                   -p 05_ANVIO_PROFILE/EP/$sample/PROFILE.db \\\n                   --parse krakenuniq \\\n                   -i 07_TAXONOMY/KRAKEN_TAXONOMY/$sample-kraken-mpa.txt\ndone\n\nfor sample in `cat WA_list.txt`\ndo \n    anvi-import-taxonomy-for-layers \\\n                   -p 05_ANVIO_PROFILE/WA/$sample/PROFILE.db \\\n                   --parse krakenuniq -i \\\n                   07_TAXONOMY/KRAKEN_TAXONOMY/$sample-kraken-mpa.txt\ndone\n\nRad. With this done we can re-merge the profile databases.\n\nanvi-merge 05_ANVIO_PROFILE/WA/*/PROFILE.db \\\n          -c 03_CONTIGS/WA-contigs.db \\\n          -o 06_MERGED/WA\nanvi-merge 05_ANVIO_PROFILE/EP/*/PROFILE.db \\\n          -c 03_CONTIGS/EP-contigs.db \\\n          -o 06_MERGED/EP\n\nAlright, time to make some Krona plots. This is a two-steo process. First, we use two scripts from metaWRAP. The first, called kraken2_translate.py, is used to generate full taxonomic lineages from a taxid. The input file for this is the output of the Kraken2 annotation. The next script is called kraken_to_krona.py which takes the output of the first script (translated Kraken file) and parses it into a format that Krona can use to produce plots.\n\nconda activate metawrap\nfor sample in `cat sampleskraken2.txt`\ndo\n    kraken2_translate.py kraken2_db $sample-kraken.out $sample-kraken.trans\n    kraken_to_krona.py $sample-kraken.trans &gt; $KRAKEN/$sample-kraken.krona\ndone\nconda deactivate \n\nOnce that is complete, we can use the output files and a script called ktImportText from the Krona package to produce HTML Krona plots for each sample.\n\nconda activate krona\nfor sample in `cat sampleskraken2.txt`\ndo\n    ktImportText $sample-kraken.krona -o $sample-kraken.krona.html\ndone\nconda deactivate \n\nA plot for every sample. How great is that?\nVirSorter Annotation\nTo classify any viral sequences, we ran VirSorter2 (Guo et al. 2021) on contigs from the co-assembly using our newly created contig.db. First, we need something for VirSorter2 to classify. For that we export fasta files from each anvi’o co-assembly.\n\nanvi-export-contigs -c 03_CONTIGS/WA-contigs.db \\\n                    -o 03_CONTIGS/WA-splits.fa \\ \n                    --splits-mode --no-wrap\nanvi-export-contigs -c 03_CONTIGS/EP-contigs.db \\\n                    -o 03_CONTIGS/EP-splits.fa \\\n                    --splits-mode --no-wrap\n\nAnd the code we used to run VirSorter2.\n\nconda activate virsorter2\nvirsorter run --seqfile 03_CONTIGS/WA-splits.fa \\\n              --working-dir WA/ \\\n              --keep-original-seq \\\n              --prep-for-dramv \\\n              --hallmark-required-on-short \\\n              --db-dir virsorter2_db \nvirsorter run --seqfile 03_CONTIGS/EP-splits.fa \\\n              --working-dir EP/ \\\n              --keep-original-seq \\\n              --prep-for-dramv \\\n              --hallmark-required-on-short \\\n              --db-dir virsorter2_db\nconda deactivate\n\nNow, to get Virsorter2 annotations into the anvi’o contig databases there are a few special steps that need to be taken. Please see this post and associated virsorter_to_anvio.py script for more details. Here we will only include the code with minimal explanation. Too long…\nFirst we export two tables that the virsorter_to_anvio.py script needs for import.\n\nanvi-export-table 03_CONTIGS/WA-contigs.db  --table splits_basic_info \\\n                  --output-file WA_splits_basic_info.txt\nanvi-export-table 03_CONTIGS/EP-contigs.db  --table splits_basic_info \\\n                  --output-file EP_splits_basic_info.txt\n\nanvi-export-gene-calls -c 03_CONTIGS/WA-contigs.db \\\n                       --output-file WA_all_gene_calls.txt \\\n                       --gene-caller prodigal\nanvi-export-gene-calls -c 03_CONTIGS/EP-contigs.db \\\n                       --output-file EP_all_gene_calls.txt \\\n                       --gene-caller prodigal\n\nTime to get messy. At the time of this writing, the gene_calls file exported from anvi’o is a 10-column tab-delimited text file. The virsorter_to_anvio.py script needs only 8 of these, and they need to be in a specific order. No problem, we can use awk.\n\n\n\n\n\n\nNoteirSorter2 column values\n\n\n\nThese are the column values needed by the virsorter_to_anvio.py script:gene_callers_id, contig, start, stop, direction, partial, source, version\n\n\n\nawk 'BEGIN {FS=\"\\t\"; OFS=\"\\t\"} {print $1, $2, $3, $4, $5, $6, $8, $9}' WA_all_gene_calls.txt\nawk 'BEGIN {FS=\"\\t\"; OFS=\"\\t\"} {print $1, $2, $3, $4, $5, $6, $8, $9}' EP_all_gene_calls.txt\n\nAfter this we run the parsing script.\n\nvirsorter_to_anvio.py -i WA/ -s WA/WA_splits_basic_info.txt \\\n                      -n WA/WA_all_gene_calls.txt \\\n                      -d virsorter2_db \\\n                      -A WA/WA_virsorter_additional_info.txt \\\n                      -C WA/WA_virsorter_collection.txt \\\n                      -F WA/WA_virsorter_annotations.txt\n\nvirsorter_to_anvio.py -i EP/ -s EP/EP_splits_basic_info.txt \\\n                      -n EP/EP_all_gene_calls.txt \\\n                      -d virsorter2_db \\\n                      -A EP/EP_virsorter_additional_info.txt \\\n                      -C EP/EP_virsorter_collection.txt \\\n                      -F EP/EP_virsorter_annotations.txt\n\nAnd import the resulting files to anvi’o\n\nanvi-import-misc-data WA-virsorter_additional_info.txt \n                      -p 06_MERGED/WA/PROFILE.db  \n                      --target-data-table items\nanvi-import-misc-data EP-virsorter_additional_info.txt \n                      -p 06_MERGED/EP/PROFILE.db  \n                      --target-data-table items\n\nanvi-import-collection WA-virsorter_collection.txt \n                      -c 03_CONTIGS/WA-contigs.db \n                      -p 06_MERGED/WA/PROFILE.db \n                      -C VIRSORTER2\nanvi-import-collection EP-virsorter_collection.txt \n                      -c 03_CONTIGS/EP-contigs.db \n                      -p 06_MERGED/EP/PROFILE.db\n                      -C VIRSORTER2\n\nVirSorter2 annotation complete.",
    "crumbs": [
      "Home",
      "Metagenomics",
      "2. Co-Assembly & Annotations"
    ]
  },
  {
    "objectID": "workflows/mg/assembly/index.html#kaiju-annotation",
    "href": "workflows/mg/assembly/index.html#kaiju-annotation",
    "title": "2. Co-Assembly & Annotations",
    "section": "Kaiju Annotation",
    "text": "Kaiju Annotation\nHere we use Kaiju (Menzel, Ng, and Krogh 2016) to classify gene calls. We do this against the progenomes databases, a r epresentative set of genomes from the proGenomes database and viruses from the NCBI RefSeq database. We describe the contruction of this database here.\nStart by grabbing gene call fasta files.\n\nanvi-get-sequences-for-gene-calls -c 03_CONTIGS/EP-contigs.db -o EP_gene_calls.fna\nanvi-get-sequences-for-gene-calls -c 03_CONTIGS/WA-contigs.db -o WA_gene_calls.fna\n\n\nconda activate kaiju\nkaiju -t kaiju_db/nodes.dmp \\\n      -f kaiju_db/kaiju_db_progenomes.fmi \\\n      -i EP_gene_calls.fna \\ \n      -o EP_kaiju_nr.out \\\n      -z $NSLOTS -v\n\nkaiju -t kaiju_db/nodes.dmp \\\n      -f kaiju_db/kaiju_db_progenomes.fmi \\\n      -i WA_gene_calls.fna \\\n      -o WA_kaiju_nr.out \\\n      -z $NSLOTS -v\n\nkaiju-addTaxonNames -t kaiju_db/nodes.dmp \\\n                    -n kaiju_db/names.dmp \\\n                    -i EP_kaiju_nr.out \\\n                    -o EP_kaiju_nr.names \\\n                    -r superkingdom,phylum,order,class,family,genus,species\n\nkaiju-addTaxonNames -t kaiju_db/nodes.dmp \n                    -n kaiju_db/names.dmp \\\n                    -i WA_kaiju_nr.out \\\n                    -o WA_kaiju_nr.names \\ \n                    -r superkingdom,phylum,order,class,family,genus,species\nconda deactivate\n\nImport the output to anvi’o.\n\nanvi-import-taxonomy-for-genes -c 03_CONTIGS/EP-contigs.db \\\n                               -p kaiju \\\n                               -i EP_kaiju_nr.names \n\nanvi-import-taxonomy-for-genes -c 03_CONTIGS/WA-contigs.db \\\n                               -p kaiju \\\n                               -i WA_kaiju_nr.names\n\nAnd generate Krona plots of the data. A little dance between the Kaiju and Krona environments.\n\nconda activate kaiju\nkaiju2krona -t kaiju_db/nodes.dmp \\\n            -n kaiju_db/names.dmp \\\n            -i $KAIJU/WA_kaiju_nr.out \\\n            -o WA_kaiju_nr.out.krona\n\nkaiju2krona -t kaiju_db/nodes.dmp \\\n            -n kaiju_db/names.dmp \\\n            -i EP_kaiju_nr.out \\\n            -o EP_kaiju_nr.out.krona\nconda deactivate\n\n\nconda activate krona\nktImportText -o WA_kaiju_nr.out.html WA_kaiju_nr.out.krona\nktImportText -o EP_kaiju_nr.out.html EP_kaiju_nr.out.krona\nconda deactivate \n\n\nconda activate kaiju\n\nkaiju2table -t kaiju_db/nodes.dmp \\\n            -n kaiju_db/names.dmp \\\n            -r class \\\n            -l phylum,class,order,family \\\n            -o WA_kaiju_nr.out.summary WA_kaiju_nr.out \\\nkaiju2table -t kaiju_db/nodes.dmp \\\n            -n kaiju_db/names.dmp \\\n            -r class \\\n            -l phylum,class,order,family \\\n            -o EP_kaiju_nr.out.summary EP_kaiju_nr.out\n\nconda deactivate\n\nKaiju done.",
    "crumbs": [
      "Home",
      "Metagenomics",
      "2. Co-Assembly & Annotations"
    ]
  },
  {
    "objectID": "workflows/mg/setup/index.html",
    "href": "workflows/mg/setup/index.html",
    "title": "1. Build Databases",
    "section": "",
    "text": "There are two main types of annotations we are interested in for this metagenomic project—taxonomic and functional—and there are many, many ways to accomplish both of these goals. This next section involves building the databases and installing any additional tools we need for annotation.\nLet’s start with the tools and databases for taxonomic classification.\n\nThere are many algorithms and databases for taxonomic classification. We will use Kraken2 to classify short reads. We will also use Kaiju and VirSorter2 for contigs. Anvi’o has methods of importing data from each of these approaches but if you have a have a favorite tool/database there are workarounds to get most results into the appropriate anvio database.\n\nKaiju (Menzel, Ng, and Krogh 2016)… finds maximum (in-)exact matches on the protein-level using the Burrows–Wheeler transform. Kaiju is a program for the taxonomic classification… of metagenomic DNA. Reads are directly assigned to taxa using the NCBI taxonomy and a reference database of protein sequences from microbial and viral genomes.\nFirst we need to install Kaiju. Again, I will run kaiju in a separate conda environment. Now I can either install kaiju from the source code or as conda package. The latter is easier but often conda packages may lag behind the source code versions. I usually compare the release dates of the conda package with the source code and look at the number of downloads. In this case, the conda version of kaiju looks fine.\n\n# create generic environment\nconda create -n kaiju\nconda activate kaiji\nconda install -c bioconda kaiju\n\nAfter kaiju is installed, the next thing to do is generate the database. You can find a description of kaiju databases in the section on dreating the Kaiju index. I downloaded and formatted the progenomes database, whic contains representative sets of genomes from the proGenomes database and viruses from the NCBI RefSeq database.\nSimply run the kaiju-makedb command and specify a database.\n\nsource activate kaiju\nmkdir kaiju\ncd kaiju\nkaiju-makedb -s progenomes\n\nThe progenomes database is 95GB.\n\n\n\n\n\n\nNoteExpand for the KAIJU_DB_BUILD Hydra script\n\n\n\n\n\n# /bin/sh\n# ----------------Parameters---------------------- #\n#$ -S /bin/sh\n#$ -pe mthread 15\n#$ -q sThM.q\n#$ -l mres=225G,h_data=15G,h_vmem=15G,himem\n#$ -cwd\n#$ -j y\n#$ -N job_build_anvio_dbs\n#$ -o hydra_logs/job_build_kaiju.log\n#\n# ----------------Modules------------------------- #\nmodule load gcc/4.9.2\n#\n# ----------------Your Commands------------------- #\n#\necho + `date` job $JOB_NAME started in $QUEUE with jobID=$JOB_ID on $HOSTNAME\necho + NSLOTS = $NSLOTS\n#\n# ----------------CALLING ANVIO------------------- #\nexport PATH=/home/scottjj/miniconda3:$PATH\nexport PATH=/home/scottjj/miniconda3/bin:$PATH\n#\nsource activate kaiju\nmkdir kaiju\ncd kaiju\nkaiju-makedb -s progenomes\n\n\n\n\nKraken2 (Wood, Lu, and Langmead 2019) is a novel metagenomics classifier that combines the fast k-mer-based classification of Kraken with an efficient algorithm for assessing the coverage of unique k-mers found in each species in a dataset.\nInstalled in separate conda environment\n\nconda create -n kraken2\nconda install kraken2\nconda activate kraken2\n\nThe standard way of installing a Kraken2 database is to run kraken2-build and calling the --standard flag, which will construct a database containing Refeq archaea, bacteria, viral, plasmid, human data plus data from UniVec_Core. Here is the command.\n\nkraken2-build --standard --db kraken2_db --threads $NSLOTS\n\nThe first problem arises because of issues with the NCBI servers and kraken2-build use of rsync. So we add the flag --use-ftp. Like so…\n\nkraken2-build --standard --db kraken2_db --threads $NSLOTS --use-ftp\n\nBut this did not work either–it failed repeatedly. If you look at the Kraken2 GitHub issues page you will see that this is a common issue. So instead we tried using one of the prebuilt database, however none of these contained files we needed for downstream analysis. Strike 2. So we tried a different approach.\nFirst, we followed this suggestion to change line 16 of the file PATH_to_KRAKEN2/download_taxonomy.sh from this…\n\nFTP_SERVER=\"ftp://$NCBI_SERVER\"\n\n…to this\n\nTO: FTP_SERVER=\"https://$NCBI_SERVER\"\n\nSupposedly this is to help with the timeout issues from the NCBI servers. Next, we tried building our own database. The first step was to download the NCBI taxonomy.\n\nkraken2-build --download-taxonomy --db kraken2_db --use-ftp\n\nOnce this step was complete, we downloaded individual libraries in order to build a custom Kraken2 database.\n\nkraken2-build --download-library archaea --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library bacteria --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library plasmid --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library viral --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library fungi --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library protozoa --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library UniVec_Core --db kraken2_db --threads $NSLOTS --use-ftp \n\nAnd finally constructed the database.\n\nkraken2-build --build --db kraken2_db\n\nAnd it worked. The whole process took about 8 hours and the final DB is ~90GB.\n\n\n\n\n\n\nNoteExpand for the KRAKEN2_DB_BUILD Hydra script\n\n\n\n\n\n# /bin/sh\n# ----------------Parameters---------------------- #\n#$ -S /bin/sh\n#$ -pe mthread 30\n#$ -q mThM.q\n#$ -l mres=900G,h_data=30G,h_vmem=30G,himem\n#$ -cwd\n#$ -j y\n#$ -N job_00_kraken2-build\n#$ -o job_00_kraken2-build4.job\n#\n# ----------------Modules------------------------- #\nmodule load bioinformatics/blast\n#\n# ----------------Load Envs------------------- #\n#\necho + `date` job $JOB_NAME started in $QUEUE with jobID=$JOB_ID on $HOSTNAME\necho + NSLOTS = $NSLOTS\n#\nexport PATH=/home/scottjj/miniconda3/bin:$PATH\nexport PATH=/home/scottjj/miniconda3/envs/kraken2/bin:$PATH\n\n######################################################################\n# Changed line 16 of this file:\n# FROM: FTP_SERVER=\"ftp://$NCBI_SERVER\"\n# TO: FTP_SERVER=\"https://$NCBI_SERVER\"\n# /home/scottjj/miniconda3/envs/kraken2/share/kraken2-2.1.3-1/libexec/download_taxonomy.sh\n# per https://github.com/DerrickWood/kraken2/issues/515#issuecomment-949354093\n######################################################################\n\nsource activate kraken2\n\n######################################################################\n## STANDARD BUILD-FAILED\n######################################################################\n###kraken2-build --standard --db kraken2_db --threads $NSLOTS --use-ftp\n\n######################################################################\n## CUSTOM BUILD\n######################################################################\n\n## TAXONOMY\nkraken2-build --download-taxonomy --db kraken2_db --use-ftp\n\n## DATABASES\nkraken2-build --download-library archaea --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library bacteria --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library plasmid --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library viral --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library fungi --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library protozoa --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library UniVec_Core --db kraken2_db --threads $NSLOTS --use-ftp \n\n## BUILD\nkraken2-build --build --db kraken2_db\n\nconda deactivate\n\necho = `date` job $JOB_NAME done\n\n\n\n\nVirSorter (Guo et al. 2021), applies a multi-classifier, expert-guided approach to detect diverse DNA and RNA virus genomes. It has made major updates to its previous version.\nI followed this recipe for installing VirSorter2. Piece of cake.\n\nmamba create -n virsorter2 -c conda-forge -c bioconda virsorter=2\nmamba activate virsorter2\n\nGood to go? Now time to build the VirSorter2 database. Pretty straightforward actually. The general instructions can be found here.\n\nsource activate virsorter2\nvirsorter setup --db-dir /pool/genomics/stri_istmobiome/dbs/virsorter2/ -j $NSLOTS\n\nAnd then a quick test to make sure the database is ok.\n\nmkdir TEST_virsorter2\ncd TEST_virsorter2\nwget -O test.fa https://raw.githubusercontent.com/jiarong/VirSorter2/master/test/8seq.fa\nvirsorter run -w test.out -i test.fa -j $NSLOTS --db-dir dbs/virsorter2/ --tmpdir TEST_virsorter2/tmp_vir --rm-tmpdir --min-length 1500 all\n\nThe uncompressed database is a little over 10GB.\n\n\n\n\n\n\nNoteExpand for the VIRSORTER2_DB_BUILD Hydra script\n\n\n\n\n\n# /bin/sh\n# ----------------Parameters---------------------- #\n#$ -S /bin/sh\n#$ -pe mthread 15\n#$ -q sThM.q\n#$ -l mres=225G,h_data=15G,h_vmem=15G,himem\n#$ -cwd\n#$ -j y\n#$ -N job_build_anvio_dbs\n#$ -o hydra_logs/job_build_virsorter2.log\n#\n# ----------------Modules------------------------- #\nmodule load gcc/4.9.2\n#\n# ----------------Your Commands------------------- #\n#\necho + `date` job $JOB_NAME started in $QUEUE with jobID=$JOB_ID on $HOSTNAME\necho + NSLOTS = $NSLOTS\n#\n# ----------------CALLING ANVIO------------------- #\nexport PATH=/home/scottjj/miniconda3:$PATH\nexport PATH=/home/scottjj/miniconda3/bin:$PATH\n#\n\nsource activate virsorter2\nvirsorter setup --db-dir /pool/genomics/stri_istmobiome/dbs/virsorter2/ -j $NSLOTS\n\n############### TEST VIRSORTER 2 ###############\n\nmkdir TEST_virsorter2\ncd TEST_virsorter2\nwget -O test.fa https://raw.githubusercontent.com/jiarong/VirSorter2/master/test/8seq.fa\nvirsorter run -w test.out -i test.fa -j $NSLOTS --db-dir /pool/genomics/stri_istmobiome/dbs/virsorter2/ --tmpdir TEST_virsorter2/tmp_vir --rm-tmpdir --min-length 1500 all\n\nconda deactivate\n\necho = `date` job $JOB_NAME done\n\n\n\n\nAnvi’o has native support for building tRNA and Single Copy Gene (SCG) taxonomy from the Genome Taxonomy Database (GTDB).\n\nanvi-setup-scg-taxonomy -T $NSLOTS\nanvi-setup-trna-taxonomy -T $NSLOTS\n\n\nNow we can install databases for functional annotation. Anvi’o has native support for building Pfam, COG, KEGG, and CAZymes databases.\nAnvi’o makes this super simple.\n\nanvi-setup-pfams --pfam-data-dir dbs/pfam_db\nanvi-setup-kegg-data --mode all --kegg-data-dir dbs/kegg_kofam  -T $NSLOTS\nanvi-setup-ncbi-cogs --cog-data-dir dbs/cog_db -T $NSLOTS\nanvi-setup-cazymes --cazyme-data-dir dbs/cazymes\n\nAnd that’s it. We can add more databases as we need them.",
    "crumbs": [
      "Home",
      "Metagenomics",
      "1. Build Databases"
    ]
  },
  {
    "objectID": "workflows/mg/setup/index.html#taxonomic-classification",
    "href": "workflows/mg/setup/index.html#taxonomic-classification",
    "title": "1. Build Databases",
    "section": "",
    "text": "There are many algorithms and databases for taxonomic classification. We will use Kraken2 to classify short reads. We will also use Kaiju and VirSorter2 for contigs. Anvi’o has methods of importing data from each of these approaches but if you have a have a favorite tool/database there are workarounds to get most results into the appropriate anvio database.\n\nKaiju (Menzel, Ng, and Krogh 2016)… finds maximum (in-)exact matches on the protein-level using the Burrows–Wheeler transform. Kaiju is a program for the taxonomic classification… of metagenomic DNA. Reads are directly assigned to taxa using the NCBI taxonomy and a reference database of protein sequences from microbial and viral genomes.\nFirst we need to install Kaiju. Again, I will run kaiju in a separate conda environment. Now I can either install kaiju from the source code or as conda package. The latter is easier but often conda packages may lag behind the source code versions. I usually compare the release dates of the conda package with the source code and look at the number of downloads. In this case, the conda version of kaiju looks fine.\n\n# create generic environment\nconda create -n kaiju\nconda activate kaiji\nconda install -c bioconda kaiju\n\nAfter kaiju is installed, the next thing to do is generate the database. You can find a description of kaiju databases in the section on dreating the Kaiju index. I downloaded and formatted the progenomes database, whic contains representative sets of genomes from the proGenomes database and viruses from the NCBI RefSeq database.\nSimply run the kaiju-makedb command and specify a database.\n\nsource activate kaiju\nmkdir kaiju\ncd kaiju\nkaiju-makedb -s progenomes\n\nThe progenomes database is 95GB.\n\n\n\n\n\n\nNoteExpand for the KAIJU_DB_BUILD Hydra script\n\n\n\n\n\n# /bin/sh\n# ----------------Parameters---------------------- #\n#$ -S /bin/sh\n#$ -pe mthread 15\n#$ -q sThM.q\n#$ -l mres=225G,h_data=15G,h_vmem=15G,himem\n#$ -cwd\n#$ -j y\n#$ -N job_build_anvio_dbs\n#$ -o hydra_logs/job_build_kaiju.log\n#\n# ----------------Modules------------------------- #\nmodule load gcc/4.9.2\n#\n# ----------------Your Commands------------------- #\n#\necho + `date` job $JOB_NAME started in $QUEUE with jobID=$JOB_ID on $HOSTNAME\necho + NSLOTS = $NSLOTS\n#\n# ----------------CALLING ANVIO------------------- #\nexport PATH=/home/scottjj/miniconda3:$PATH\nexport PATH=/home/scottjj/miniconda3/bin:$PATH\n#\nsource activate kaiju\nmkdir kaiju\ncd kaiju\nkaiju-makedb -s progenomes\n\n\n\n\nKraken2 (Wood, Lu, and Langmead 2019) is a novel metagenomics classifier that combines the fast k-mer-based classification of Kraken with an efficient algorithm for assessing the coverage of unique k-mers found in each species in a dataset.\nInstalled in separate conda environment\n\nconda create -n kraken2\nconda install kraken2\nconda activate kraken2\n\nThe standard way of installing a Kraken2 database is to run kraken2-build and calling the --standard flag, which will construct a database containing Refeq archaea, bacteria, viral, plasmid, human data plus data from UniVec_Core. Here is the command.\n\nkraken2-build --standard --db kraken2_db --threads $NSLOTS\n\nThe first problem arises because of issues with the NCBI servers and kraken2-build use of rsync. So we add the flag --use-ftp. Like so…\n\nkraken2-build --standard --db kraken2_db --threads $NSLOTS --use-ftp\n\nBut this did not work either–it failed repeatedly. If you look at the Kraken2 GitHub issues page you will see that this is a common issue. So instead we tried using one of the prebuilt database, however none of these contained files we needed for downstream analysis. Strike 2. So we tried a different approach.\nFirst, we followed this suggestion to change line 16 of the file PATH_to_KRAKEN2/download_taxonomy.sh from this…\n\nFTP_SERVER=\"ftp://$NCBI_SERVER\"\n\n…to this\n\nTO: FTP_SERVER=\"https://$NCBI_SERVER\"\n\nSupposedly this is to help with the timeout issues from the NCBI servers. Next, we tried building our own database. The first step was to download the NCBI taxonomy.\n\nkraken2-build --download-taxonomy --db kraken2_db --use-ftp\n\nOnce this step was complete, we downloaded individual libraries in order to build a custom Kraken2 database.\n\nkraken2-build --download-library archaea --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library bacteria --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library plasmid --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library viral --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library fungi --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library protozoa --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library UniVec_Core --db kraken2_db --threads $NSLOTS --use-ftp \n\nAnd finally constructed the database.\n\nkraken2-build --build --db kraken2_db\n\nAnd it worked. The whole process took about 8 hours and the final DB is ~90GB.\n\n\n\n\n\n\nNoteExpand for the KRAKEN2_DB_BUILD Hydra script\n\n\n\n\n\n# /bin/sh\n# ----------------Parameters---------------------- #\n#$ -S /bin/sh\n#$ -pe mthread 30\n#$ -q mThM.q\n#$ -l mres=900G,h_data=30G,h_vmem=30G,himem\n#$ -cwd\n#$ -j y\n#$ -N job_00_kraken2-build\n#$ -o job_00_kraken2-build4.job\n#\n# ----------------Modules------------------------- #\nmodule load bioinformatics/blast\n#\n# ----------------Load Envs------------------- #\n#\necho + `date` job $JOB_NAME started in $QUEUE with jobID=$JOB_ID on $HOSTNAME\necho + NSLOTS = $NSLOTS\n#\nexport PATH=/home/scottjj/miniconda3/bin:$PATH\nexport PATH=/home/scottjj/miniconda3/envs/kraken2/bin:$PATH\n\n######################################################################\n# Changed line 16 of this file:\n# FROM: FTP_SERVER=\"ftp://$NCBI_SERVER\"\n# TO: FTP_SERVER=\"https://$NCBI_SERVER\"\n# /home/scottjj/miniconda3/envs/kraken2/share/kraken2-2.1.3-1/libexec/download_taxonomy.sh\n# per https://github.com/DerrickWood/kraken2/issues/515#issuecomment-949354093\n######################################################################\n\nsource activate kraken2\n\n######################################################################\n## STANDARD BUILD-FAILED\n######################################################################\n###kraken2-build --standard --db kraken2_db --threads $NSLOTS --use-ftp\n\n######################################################################\n## CUSTOM BUILD\n######################################################################\n\n## TAXONOMY\nkraken2-build --download-taxonomy --db kraken2_db --use-ftp\n\n## DATABASES\nkraken2-build --download-library archaea --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library bacteria --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library plasmid --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library viral --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library fungi --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library protozoa --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library UniVec_Core --db kraken2_db --threads $NSLOTS --use-ftp \n\n## BUILD\nkraken2-build --build --db kraken2_db\n\nconda deactivate\n\necho = `date` job $JOB_NAME done\n\n\n\n\nVirSorter (Guo et al. 2021), applies a multi-classifier, expert-guided approach to detect diverse DNA and RNA virus genomes. It has made major updates to its previous version.\nI followed this recipe for installing VirSorter2. Piece of cake.\n\nmamba create -n virsorter2 -c conda-forge -c bioconda virsorter=2\nmamba activate virsorter2\n\nGood to go? Now time to build the VirSorter2 database. Pretty straightforward actually. The general instructions can be found here.\n\nsource activate virsorter2\nvirsorter setup --db-dir /pool/genomics/stri_istmobiome/dbs/virsorter2/ -j $NSLOTS\n\nAnd then a quick test to make sure the database is ok.\n\nmkdir TEST_virsorter2\ncd TEST_virsorter2\nwget -O test.fa https://raw.githubusercontent.com/jiarong/VirSorter2/master/test/8seq.fa\nvirsorter run -w test.out -i test.fa -j $NSLOTS --db-dir dbs/virsorter2/ --tmpdir TEST_virsorter2/tmp_vir --rm-tmpdir --min-length 1500 all\n\nThe uncompressed database is a little over 10GB.\n\n\n\n\n\n\nNoteExpand for the VIRSORTER2_DB_BUILD Hydra script\n\n\n\n\n\n# /bin/sh\n# ----------------Parameters---------------------- #\n#$ -S /bin/sh\n#$ -pe mthread 15\n#$ -q sThM.q\n#$ -l mres=225G,h_data=15G,h_vmem=15G,himem\n#$ -cwd\n#$ -j y\n#$ -N job_build_anvio_dbs\n#$ -o hydra_logs/job_build_virsorter2.log\n#\n# ----------------Modules------------------------- #\nmodule load gcc/4.9.2\n#\n# ----------------Your Commands------------------- #\n#\necho + `date` job $JOB_NAME started in $QUEUE with jobID=$JOB_ID on $HOSTNAME\necho + NSLOTS = $NSLOTS\n#\n# ----------------CALLING ANVIO------------------- #\nexport PATH=/home/scottjj/miniconda3:$PATH\nexport PATH=/home/scottjj/miniconda3/bin:$PATH\n#\n\nsource activate virsorter2\nvirsorter setup --db-dir /pool/genomics/stri_istmobiome/dbs/virsorter2/ -j $NSLOTS\n\n############### TEST VIRSORTER 2 ###############\n\nmkdir TEST_virsorter2\ncd TEST_virsorter2\nwget -O test.fa https://raw.githubusercontent.com/jiarong/VirSorter2/master/test/8seq.fa\nvirsorter run -w test.out -i test.fa -j $NSLOTS --db-dir /pool/genomics/stri_istmobiome/dbs/virsorter2/ --tmpdir TEST_virsorter2/tmp_vir --rm-tmpdir --min-length 1500 all\n\nconda deactivate\n\necho = `date` job $JOB_NAME done\n\n\n\n\nAnvi’o has native support for building tRNA and Single Copy Gene (SCG) taxonomy from the Genome Taxonomy Database (GTDB).\n\nanvi-setup-scg-taxonomy -T $NSLOTS\nanvi-setup-trna-taxonomy -T $NSLOTS",
    "crumbs": [
      "Home",
      "Metagenomics",
      "1. Build Databases"
    ]
  },
  {
    "objectID": "workflows/mg/setup/index.html#functional-annotations",
    "href": "workflows/mg/setup/index.html#functional-annotations",
    "title": "1. Build Databases",
    "section": "",
    "text": "Now we can install databases for functional annotation. Anvi’o has native support for building Pfam, COG, KEGG, and CAZymes databases.\nAnvi’o makes this super simple.\n\nanvi-setup-pfams --pfam-data-dir dbs/pfam_db\nanvi-setup-kegg-data --mode all --kegg-data-dir dbs/kegg_kofam  -T $NSLOTS\nanvi-setup-ncbi-cogs --cog-data-dir dbs/cog_db -T $NSLOTS\nanvi-setup-cazymes --cazyme-data-dir dbs/cazymes\n\nAnd that’s it. We can add more databases as we need them.",
    "crumbs": [
      "Home",
      "Metagenomics",
      "1. Build Databases"
    ]
  },
  {
    "objectID": "workflows/portal/data-med.html",
    "href": "workflows/portal/data-med.html",
    "title": "Data Portal",
    "section": "",
    "text": "Quick access to fastq sequence files, processing scripts, and curated datasets. If you want to process the data yourself, select a pipeline, download the sequence data and processing scripts, and run the workflow."
  },
  {
    "objectID": "workflows/portal/data-med.html#data-scripts",
    "href": "workflows/portal/data-med.html#data-scripts",
    "title": "Data Portal",
    "section": "Data & Scripts",
    "text": "Data & Scripts\n\n\n\n\n\n\nNoteA note about curated data\n\n\n\nCurated means that after processing, the data has been cleaned of unwanted taxa (e.g., NA kingdoms, potential contaminants, etc). Negative control & low-count samples have also been removed. These data are ready for downstream analysis.\n\n\nHere we provide two options for processing sequence data–the detailed pipeline (listed in the table) and a quick set of commands linked below.\n\n\n\n\nAsset\n\n\n\nFile Name\n\n\n\nDescription\n\n\n\n\n\n\nCurated Data\n\n\nmed_curated_data.zip\n\n\nArchive containing the curated output of MED pipeline–-OTU table, taxonomy table, sample metadata, & representative fasta sequences. Data is available as a) stand-alone text files, b) bundled in a microtable object, and c) bundled in a phyloseq object. Archive also includes a table tracking sample read changes. \n\n\n\n\n\nSequence Data\n\n\nPRJEB36632\n\n\nTrimmed (primers removed) 16S rRNA data from 1909 samples. \n\n\n\n\n\nProcessing Data & Scripts\n\n\nmed_processing.zip\n\n\nArchived directory containing fastq rename tables, fastq rename results, sample data, mothur batchfile script, MED prep scripts, MED mapping file, pipeline read changes, & Hydra job scripts. \n\n\n\n\n\nDetailed Pipeline\n\n\nMED workflow\n\n\nDetailed workflow for 16S rRNA analysis using MED. Workflow uses paired end reads, beginning with raw fastq files, ending with sequence & taxonomy tables. Microtable & phyloseq objects are produced to collate the data for downstream analysis. \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "workflows/portal/data-med.html#med-processing",
    "href": "workflows/portal/data-med.html#med-processing",
    "title": "Data Portal",
    "section": "MED Processing",
    "text": "MED Processing\nProcessing scripts for Minimum Entropy Decomposition (MED) analysis. The pipeline begins with the output fasta and count files from the align.seqs portion of the mothur OTU pipeline. From there we use R and mothur to remove negative control samples, check for chimera, and run taxonomic classifications. It is important to note that the MED workflow does not precluster sequences (as in the mothur pipeline) because MED relies on every sequence (including redundant reads) for the analysis. This pipeline has four main steps:\n\nrun the mothur workflow.\n\nmodify and run the mothur2oligo.sh script. This script transforms the mothur output to appropriate format for MED. It must be run in the mothur environment because the script uses mothur. We need access to the following mothur files to run this script.\n\n\ntaxonomy file: final_med.taxonomy\n\n\ncount file: final_med.count_table\n\n\nfasta file: final_med.fasta\n\n\n\ntrim uninformative columns from alignment (in the MED environment)\n\nrun the MED command\n\n\nmothur processing script for MED analysis########################################################################\n#$ export DATA=01_TRIMMED_DATA/\n#$ export TYPE=gz\n#$ export PROC=30\n\n#$ export REF_LOC=reference_dbs\n#$ export TAXREF_FASTA=gsrdb.fasta\n#$ export TAXREF_TAX=gsrdb.tax\n#$ export ALIGNREF=silva.v4.fasta\n\n#$ export CONTAMINENTS=Chloroplast-Mitochondria-unknown-Eukaryota\n###############################################################################\n#### THIS PIPELINE BEGINS AFTER ALIGN.SEQS (AND ASSOCIATED SCREENING STEP)\n#### IN THE MOTHUR OTU PIPLINE. IF YOU HAVE NO USE FOR STRICT OTU ANAYSIS\n#### SIMPLY UNCOMMENT THE MOTHUR COMMANDS BELOW TO RUN THE FULL PIPELINE HERE\n###############################################################################\nset.dir(output=pipelineFiles_med/)\n###############################################################################\n# UNCOMMENT FROM HERE....\n###############################################################################\n#make.file(inputdir=$DATA/, type=$TYPE, prefix=shrimp)\n########################################################################\n#### had to fix shimp.files bc mothur splits name at first underscore (_)\n########################################################################\n#make.contigs(file=shrimp.files, processors=$PROC)\n#summary.seqs(fasta=shrimp.trim.contigs.fasta, count=shrimp.contigs.count_table, processors=$PROC)\n#count.groups(count=shrimp.contigs.count_table)\n#screen.seqs(fasta=current, count=current, maxambig=0, minlength=252, maxlength=254, maxhomop=6, processors=$PROC)\n#summary.seqs(fasta=current, count=current, processors=$PROC)\n#count.groups(count=current)\n#unique.seqs(fasta=current, count=current)\n#summary.seqs(count=current, processors=$PROC)\n########################################################################\n#### Aligning reads\n########################################################################\n# https://mothur.org/wiki/silva_reference_files/\n#### Prep reference file\n#pcr.seqs(fasta=$REF_LOC/silva.seed_v138_2.align, start=13862, end=23445, keepdots=F, processors=$PROC)\n#rename.file(input=pipelineFiles/silva.seed_v138_2.pcr.align, new=pipelineFiles/$ALIGNREF)\n#summary.seqs(fasta=pipelineFiles/$ALIGNREF, processors=$PROC)\n#### Align reads\n#align.seqs(fasta=shrimp.trim.contigs.good.unique.fasta, reference=pipelineFiles/$ALIGNREF, processors=$PROC)\n#summary.seqs(fasta=shrimp.trim.contigs.good.unique.align, count=shrimp.trim.contigs.good.count_table, processors=$PROC)\n########################################################################\n#### Further processing\n########################################################################\n#screen.seqs(fasta=current, count=current, start=1, end=9583, processors=$PROC)\n#summary.seqs(fasta=current, count=current, processors=$PROC)\n#count.groups(count=current)\n#filter.seqs(fasta=current, vertical=T, trump=., processors=$PROC)\n#unique.seqs(fasta=current, count=current)\n#summary.seqs(fasta=current, count=current, processors=$PROC)\n###############################################################################\n# ....TO HERE\n###############################################################################\n#### Copy mothur output files\n########################################################################\nsystem(cp pipelineFiles/shrimp.trim.contigs.good.unique.good.filter.unique.fasta pipelineFiles_med/)\nsystem(cp pipelineFiles/shrimp.trim.contigs.good.unique.good.filter.count_table pipelineFiles_med/)\n########################################################################\n#### Remove Negative Control samples (files generated in R)\n########################################################################\n\n########## IN R ########################################################\n#tmp_accnos &lt;- readr::read_delim(here(work_here, \"nc_screen/shrimp.files\"), delim = \"\\t\", col_names = FALSE)\n#tmp_accnos[, 2:3] &lt;- NULL\n#tmp_accnos &lt;- tmp_accnos[grepl(\"Control_\", tmp_accnos$X1), ]\n#readr::write_delim(tmp_accnos, file = here(work_here, \"nc_screen/nc_samples.accnos\"), col_names = FALSE)\n########################################################################\nget.groups(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.fasta, count=shrimp.trim.contigs.good.unique.good.filter.count_table, accnos=nc_samples.accnos)\nrename.file(input=shrimp.trim.contigs.good.unique.good.filter.unique.pick.fasta, new=nc.fasta)\nrename.file(input=shrimp.trim.contigs.good.unique.good.filter.pick.count_table, new=nc.count_table)\nsummary.seqs(fasta=nc.fasta, count=nc.count_table, processors=$PROC)\nlist.seqs(count=nc.count_table)\ncount.seqs(count=nc.count_table, compress=f)\nget.seqs(accnos=nc.accnos, fasta=shrimp.trim.contigs.good.unique.good.filter.unique.fasta, count=shrimp.trim.contigs.good.unique.good.filter.count_table)\nrename.file(input=shrimp.trim.contigs.good.unique.good.filter.unique.pick.fasta, new=subset.fasta)\nrename.file(input=shrimp.trim.contigs.good.unique.good.filter.pick.count_table, new=subset.count_table)\ncount.seqs(count=subset.count_table, compress=f)\n########################################################################\n########## IN R ########################################################\n########################################################################\n#full_count_tab &lt;- readr::read_delim(here(work_here, \"nc_screen/subset.full.count_table\"), delim = \"\\t\", col_names = TRUE)\n#control_cols     &lt;- grep(\"^Control_\", names(full_count_tab), value = TRUE)\n#noncontrol_cols  &lt;- setdiff(names(full_count_tab)[-(1:2)], control_cols)\n#read_totals &lt;- full_count_tab %&gt;%\n#  rowwise() %&gt;%\n#  mutate(\n#    total_reads_nc   = sum(c_across(all_of(control_cols)), na.rm = TRUE),\n#    total_reads_non_nc = sum(c_across(all_of(noncontrol_cols)), na.rm = TRUE)\n#  ) %&gt;%\n#  ungroup() %&gt;%\n#  select(1, 2, total_reads_nc, total_reads_non_nc)\n#read_totals &lt;- read_totals %&gt;% dplyr::rename(\"total_reads\" = 2)\n#tmp_read_totals &lt;- read_totals %&gt;% dplyr::mutate(perc_reads_in_nc = 100*(total_reads_nc / (total_reads_nc + total_reads_non_nc)), .after = \"total_reads_non_nc\")\n#tmp_read_totals$perc_reads_in_nc &lt;- round(tmp_read_totals$perc_reads_in_nc, digits = 6)\n#control_cols     &lt;- grep(\"^Control_\", names(full_count_tab), value = TRUE)\n#noncontrol_cols  &lt;- setdiff(names(full_count_tab)[-(1:2)], control_cols)\n# rowwise tally of non-zero columns\n#samp_totals &lt;- full_count_tab %&gt;%\n#  rowwise() %&gt;%\n#  mutate(\n#    num_nc_samp     = sum(c_across(all_of(control_cols)) != 0, na.rm = TRUE),\n#    num_non_nc_samp = sum(c_across(all_of(noncontrol_cols)) != 0, na.rm = TRUE)\n#  ) %&gt;%\n#  ungroup() %&gt;%\n#  select(1, num_nc_samp, num_non_nc_samp)\n#samp_totals$total_samp &lt;- samp_totals$num_nc_samp + samp_totals$num_non_nc_samp\n#samp_totals &lt;- samp_totals %&gt;%  dplyr::relocate(\"total_samp\", .after = \"Representative_Sequence\")\n#samp_totals &lt;- samp_totals %&gt;% dplyr::mutate(perc_nc_samp = 100*( num_nc_samp / (num_nc_samp + num_non_nc_samp)), .after = \"num_non_nc_samp\")\n#nc_check &lt;- dplyr::left_join(tmp_read_totals, samp_totals, by = \"Representative_Sequence\")\n#write_delim(nc_check, here(work_here, \"nc_screen/reads_in_nc_samples.txt\"), delim = \"\\t\")\n#nc_remove &lt;- nc_check %&gt;% filter(perc_reads_in_nc &gt; 10 | perc_nc_samp &gt; 10)\n#nc_remain &lt;- dplyr::anti_join(nc_check, nc_remove)\n#rem_nc_reads &lt;- sum(nc_remove$total_reads_nc)\n#rem_sam_reads &lt;- sum(nc_remove$total_reads_non_nc)\n#per_reads_rem &lt;- round(100*( rem_nc_reads / (rem_nc_reads + rem_sam_reads)), digits = 3)\n#ret_nc_reads &lt;- sum(nc_remain$total_reads_nc)\n#ret_sam_reads &lt;- sum(nc_remain$total_reads_non_nc)\n#per_reads_ret &lt;- round(100*( ret_nc_reads / (ret_nc_reads + ret_sam_reads)), digits = 3)\n\n########################################################################\nremove.seqs(accnos=nc_repseq_remove.accnos, fasta=shrimp.trim.contigs.good.unique.good.filter.unique.fasta, count=shrimp.trim.contigs.good.unique.good.filter.count_table)\ncount.groups(count=shrimp.trim.contigs.good.unique.good.filter.pick.count_table)\n########################################################################\n########## IN R ########################################################\n########################################################################\n#tmp_before &lt;- read_tsv(here(work_here, \"nc_screen/shrimp.trim.contigs.good.unique.good.filter.count.summary\"), col_names = FALSE, col_select = 1)\n#tmp_after &lt;- read_tsv(here(work_here, \"nc_screen/shrimp.trim.contigs.good.unique.good.filter.pick.count.summary\"), col_names = FALSE, col_select = 1)\n#tmp_nc_lost &lt;- anti_join(tmp_before, tmp_after)\n#tmp_nc_lost$X1\n#nc_to_remove &lt;- semi_join(tmp_before, tmp_after)\n#nc_to_remove &lt;- nc_to_remove %&gt;% dplyr::filter(stringr::str_starts(X1, \"Control\")) \n#readr::write_delim(nc_to_remove, file = here(work_here, \"nc_screen/nc_samples_remove.accnos\"), col_names = FALSE)\n########################################################################\nremove.groups(count=shrimp.trim.contigs.good.unique.good.filter.pick.count_table, fasta=shrimp.trim.contigs.good.unique.good.filter.unique.pick.fasta, accnos=nc_samples_remove.accnos)\n########################################\n### NEGATIVE CONTROLS Should be GONE ###\n########################################\nsummary.seqs(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.fasta, count=shrimp.trim.contigs.good.unique.good.filter.pick.pick.count_table, processors=$PROC)\ncount.groups(count=shrimp.trim.contigs.good.unique.good.filter.pick.pick.count_table)\nchimera.vsearch(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.fasta, count=shrimp.trim.contigs.good.unique.good.filter.pick.pick.count_table, dereplicate=t, processors=$PROC)\nsummary.seqs(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.fasta, count=shrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.count_table, processors=$PROC)\ncount.groups(count=shrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.count_table)\nclassify.seqs(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.fasta, count=shrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.count_table, reference=reference_dbs/gsrdb.fasta, taxonomy=reference_dbs/gsrdb.tax, processors=$PROC)\nremove.lineage(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.fasta, count=shrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.count_table, taxonomy=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.gsrdb.wang.taxonomy, taxon=Chloroplast-Mitochondria-unknown-Eukaryota)\nsummary.seqs(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.pick.fasta, count=shrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.pick.count_table, processors=$PROC)\nsummary.tax(taxonomy=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.gsrdb.wang.pick.taxonomy, count=shrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.pick.count_table)\ncount.groups(count=shrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.pick.count_table)\n############################################\n### PROCEED To  mothur2oligo.sh pipeline ###\n### MUST BE RUN IN MOTHUR ENVIRONMENT  #####\n############################################\n\n\nOnce you have the script and data you simply run the pipeline like so.\n\nmothur med_batchfile_processing\n\nOnce the mothur portion of the workflow is complete, the script mothur2oligo.sh needs to be run in the mothur environment and modified for your specific purposes. You should not need to modify the associated renamer.pl script but it does need to be in the same location as mothur2oligo.sh. You can download the modified script we used in the table above.\n\nbash mothur2oligo.sh\n\n\n\nmothur2oligo\nrenamer\n\n\n\n\nExpand for the mothur2oligo.sh scriptconda activate mothur\n# USAGE: sh mothur2oligo.sh\n# This is a shell script for transforming mothur output to appropriate format for \n# A. murat Eren's oligotyping pipeline \n\n## Set variables\n\n# Adjust the file names to your own study - these are the files from the mothur SOP\ntaxonomy=\"final_med.taxonomy\"\nfasta=\"final_med.fasta\"\ncount=\"final_med.count_table\"\nprocessors=3\n\n# Set the taxon you want to select, separate taxonomic levels with \";\" \n# Do not touch inner and outer quotes\ntaxon=\"'Bacteria;-Archaea;'\"\n\n\n################################\n########## Script  #############\n################################\n\nredundantFasta=$(echo ${fasta}.pick.redundant.fasta | sed 's/.fasta//')\ngroups=$(echo ${count}.pick.redundant.groups | sed 's/.count_table//') \n\n# Call mothur commands for generating deuniqued fasta file for a specific lineage\nmothur \"#set.current(processors=$processors); get.lineage(taxonomy=$taxonomy, taxon=$taxon, count=$count); list.seqs(count=current); get.seqs(accnos=current, fasta=$fasta); deunique.seqs(fasta=current, count=current)\"\n\n# Replace all \"_\" in fasta header with a \":\"\ncat $groups | sed 's/_/:/g' &gt; intermediate1\n# Make a file which maps sample names to sequence headers\npaste $groups intermediate1 | awk 'BEGIN{FS=\"\\t\"}{print $1\"\\t\"$2\"_\"$3}' &gt; intermediate2\n\n# Perl script to rename the headers of the fasta to include the sample name at the beginning followed by a \"_\"\nperl renamer.pl $redundantFasta intermediate2 \n\n\n\n\n\nand the companion renamer.pl scriptconda activate mothur\n#! /usr/bin/perl\n#from http://www.perlmonks.org/?node_id=975419\n\nuse strict;\nuse warnings;\n\n@ARGV == 2 or die \"usage: $0 &lt;multifasta file&gt; &lt;header replacement fil\n+e&gt;\\n\";\n\nmy ( $fasta_file, $header_file ) = @ARGV;\nmy $destination = $fasta_file . '_headers-replaced.fasta';\n\nopen IN2, '&lt;', $header_file or die \"Can't read from tab-delimited head\n+er replacement file $header_file: $!\\n\";\n\nmy %head_seqs;\nwhile ( &lt;IN2&gt; ) {\n    chomp;\n    my ( $old, $new ) = split /\\t/;\n    $head_seqs{ $old } = $new;\n    }\nclose IN2;\n\nopen IN1, '&lt;', $fasta_file or die \"Can't read from multifasta file wit\n+h alternating lines of headers and sequences $fasta_file: $!\\n\";\n\nopen OUT, '&gt;', $destination or die \"Can't write to file $destination: \n+$!\\n\";    \n\nwhile ( &lt;IN1&gt; ) {\n    if ( /^&gt;(.+)$/ && exists $head_seqs{ $1 } ) {\n        $_ = \"&gt;$head_seqs{ $1 }\\n\";\n        }\n    print OUT $_;\n    }\nclose IN1;\nclose OUT;\n\n\n\n\n\nGreat. Now within the oligotype/MED environment run the following commands for the MED analysis. You will need the mapping.txt file linked above for this step.\n\nconda activate oligotyping\no-trim-uninformative-columns-from-alignment \\\n        final_med.pick.redundant.fasta_headers-replaced.fasta\n\ndecompose final_med.pick.redundant.fasta_headers-replaced.fasta-TRIMMED \\\n        -E mapping.txt \\\n        --output-directory MED \\\n        --number-of-threads 24 \\\n        --skip-gen-figures\n\nIn the resources listed above, we include a table that summarizes read changes for each sample through the pipeline."
  },
  {
    "objectID": "workflows/portal/data-otu.html",
    "href": "workflows/portal/data-otu.html",
    "title": "Data Portal",
    "section": "",
    "text": "Quick access to fastq sequence files, processing scripts, and curated datasets. If you want to process the data yourself, select a pipeline, download the sequence data and processing scripts, and run the workflow."
  },
  {
    "objectID": "workflows/portal/data-otu.html#data-scripts",
    "href": "workflows/portal/data-otu.html#data-scripts",
    "title": "Data Portal",
    "section": "Data & Scripts",
    "text": "Data & Scripts\n\n\n\n\n\n\nNoteA note about curated data\n\n\n\nCurated means that after processing, the data has been cleaned of unwanted taxa (e.g., NA kingdoms, potential contaminants, etc). Negative control & low-count samples have also been removed. These data are ready for downstream analysis.\n\n\nHere we provide two options for processing sequence data–the detailed pipeline (listed in the table) and a quick set of commands linked below.\n\n\n\n\nAsset\n\n\n\nFile Name\n\n\n\nDescription\n\n\n\n\n\n\nCurated Data\n\n\notu_curated_data.zip\n\n\nArchive containing the curated output of mothur OTU pipeline–-OTU table, taxonomy table, sample metadata, & representative fasta sequences. Data is available as a) stand-alone text files, b) bundled in a microtable object, and c) bundled in a phyloseq object. Archive also includes a table tracking sample read changes. \n\n\n\n\n\nSequence Data\n\n\nPRJEB36632\n\n\nTrimmed (primers removed) 16S rRNA data from 1909 samples. \n\n\n\n\n\nProcessing Data & Scripts\n\n\notu_processing.zip\n\n\nArchived directory containing fastq rename tables, fastq rename results, sample data, mothur batchfile script, pipeline read changes, & Hydra job scripts. \n\n\n\n\n\nDetailed Pipeline\n\n\nMothur OTU workflow\n\n\nDetailed workflow for 16S rRNA OTU analysis using mothur. Workflow uses paired end reads, beginning with raw fastq files, ending with sequence & taxonomy tables. Microtable & phyloseq objects are produced to collate the data for downstream analysis. \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "workflows/portal/data-otu.html#otu-processing",
    "href": "workflows/portal/data-otu.html#otu-processing",
    "title": "Data Portal",
    "section": "OTU Processing",
    "text": "OTU Processing\nProcessing scripts for OTU analysis using mothur. All steps for processing are contained within a single mothur batchfile.\nScripts\n\nProcessing script for mothur OTU analysis########################################################################\n#$ export DATA=01_TRIMMED_DATA/\n#$ export TYPE=gz\n#$ export PROC=30\n\n#$ export REF_LOC=reference_dbs\n#$ export TAXREF_FASTA=gsrdb.fasta\n#$ export TAXREF_TAX=gsrdb.tax\n#$ export ALIGNREF=silva.v4.fasta\n\n#$ export CONTAMINENTS=Chloroplast-Mitochondria-unknown-Eukaryota\n########################################################################\nset.dir(output=pipelineFiles/)\nmake.file(inputdir=$DATA/, type=$TYPE, prefix=shrimp)\n########################################################################\n#### had to fix shimp.files bc mothur splits name at first underscore (_)\n########################################################################\nmake.contigs(file=shrimp.files, processors=$PROC)\nsummary.seqs(fasta=shrimp.trim.contigs.fasta, count=shrimp.contigs.count_table, processors=$PROC)\ncount.groups(count=shrimp.contigs.count_table)\nscreen.seqs(fasta=current, count=current, maxambig=0, minlength=252, maxlength=254, maxhomop=6, processors=$PROC)\nsummary.seqs(fasta=current, count=current, processors=$PROC)\ncount.groups(count=current)\nunique.seqs(fasta=current, count=current)\nsummary.seqs(count=current, processors=$PROC)\n########################################################################\n#### Aligning reads\n########################################################################\n# https://mothur.org/wiki/silva_reference_files/\n#### Prep reference file\npcr.seqs(fasta=$REF_LOC/silva.seed_v138_2.align, start=13862, end=23445, keepdots=F, processors=$PROC)\nrename.file(input=pipelineFiles/silva.seed_v138_2.pcr.align, new=pipelineFiles/$ALIGNREF)\nsummary.seqs(fasta=pipelineFiles/$ALIGNREF, processors=$PROC)\n#### Align reads\nalign.seqs(fasta=shrimp.trim.contigs.good.unique.fasta, reference=pipelineFiles/$ALIGNREF, processors=$PROC)\nsummary.seqs(fasta=shrimp.trim.contigs.good.unique.align, count=shrimp.trim.contigs.good.count_table, processors=$PROC)\n########################################################################\n#### Further processing\n########################################################################\nscreen.seqs(fasta=current, count=current, start=1, end=9583, processors=$PROC)\nsummary.seqs(fasta=current, count=current, processors=$PROC)\ncount.groups(count=current)\nfilter.seqs(fasta=current, vertical=T, trump=., processors=$PROC)\nunique.seqs(fasta=current, count=current)\nsummary.seqs(fasta=current, count=current, processors=$PROC)\n########################################################################\n#### Copy files FOR MED\n########################################################################\nsystem(cp pipelineFiles/shrimp.trim.contigs.good.unique.good.filter.unique.fasta pipelineFiles_med/)\nsystem(cp pipelineFiles/shrimp.trim.contigs.good.unique.good.filter.count_table pipelineFiles_med/)\n########################################################################\n#### Proceed with mothur workflow\n########################################################################\npre.cluster(fasta=current, count=current, diffs=2, processors=$PROC)\nsummary.seqs(fasta=current, count=current, processors=$PROC)\ncount.groups(count=current)\n########################################################################\n#### Remove Negative Control samples (files generated in R)\n########################################################################\n\n########## IN R ########################################################\n#tmp_accnos &lt;- readr::read_delim(here(med_here, \"nc_screen/shrimp.files\"), delim = \"\\t\", col_names = FALSE)\n#tmp_accnos[, 2:3] &lt;- NULL\n#tmp_accnos &lt;- tmp_accnos[grepl(\"Control_\", tmp_accnos$X1), ]\n#readr::write_delim(tmp_accnos, file = here(med_here, \"nc_screen/nc_samples.accnos\"), col_names = FALSE)\n########################################################################\nget.groups(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.fasta, count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table, accnos=nc_samples.accnos)\nrename.file(input=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta, new=nc.fasta)\nrename.file(input=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table, new=nc.count_table)\nsummary.seqs(fasta=nc.fasta, count=nc.count_table, processors=$PROC)\nlist.seqs(count=nc.count_table)\ncount.seqs(count=nc.count_table, compress=f)\nget.seqs(accnos=nc.accnos, fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.fasta, count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table)\nrename.file(input=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta, new=subset.fasta)\nrename.file(input=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table, new=subset.count_table)\ncount.seqs(count=subset.count_table, compress=f)\n########################################################################\n########## IN R ########################################################\n########################################################################\n#full_count_tab &lt;- readr::read_delim(here(med_here, \"nc_screen/subset.full.count_table\"), delim = \"\\t\", col_names = TRUE)\n#control_cols     &lt;- grep(\"^Control_\", names(full_count_tab), value = TRUE)\n#noncontrol_cols  &lt;- setdiff(names(full_count_tab)[-(1:2)], control_cols)\n## now do the rowwise sums\n#read_totals &lt;- full_count_tab %&gt;%\n#  rowwise() %&gt;%\n#  mutate(\n#    total_reads_nc   = sum(c_across(all_of(control_cols)), na.rm = TRUE),\n#    total_reads_non_nc = sum(c_across(all_of(noncontrol_cols)), na.rm = TRUE)\n#  ) %&gt;%\n#  ungroup() %&gt;%\n#  select(1, 2, total_reads_nc, total_reads_non_nc)\n#\n#read_totals &lt;- read_totals %&gt;% dplyr::rename(\"total_reads\" = 2)\n#tmp_read_totals &lt;- read_totals %&gt;% dplyr::mutate(perc_reads_in_nc = 100*(total_reads_nc / (total_reads_nc + total_reads_non_nc)), .after = \"total_reads_non_nc\")\n#tmp_read_totals$perc_reads_in_nc &lt;- round(tmp_read_totals$perc_reads_in_nc, digits = 6)\n#\n#control_cols     &lt;- grep(\"^Control_\", names(full_count_tab), value = TRUE)\n#noncontrol_cols  &lt;- setdiff(names(full_count_tab)[-(1:2)], control_cols)\n## rowwise tally of non-zero columns\n#samp_totals &lt;- full_count_tab %&gt;%\n#  rowwise() %&gt;%\n#  mutate(\n#    num_nc_samp     = sum(c_across(all_of(control_cols)) != 0, na.rm = TRUE),\n#    num_non_nc_samp = sum(c_across(all_of(noncontrol_cols)) != 0, na.rm = TRUE)\n#  ) %&gt;%\n#  ungroup() %&gt;%\n#  select(1, num_nc_samp, num_non_nc_samp)\n#\n#samp_totals$total_samp &lt;- samp_totals$num_nc_samp + samp_totals$num_non_nc_samp\n#samp_totals &lt;- samp_totals %&gt;%  dplyr::relocate(\"total_samp\", .after = \"Representative_Sequence\")\n#samp_totals &lt;- samp_totals %&gt;% dplyr::mutate(perc_nc_samp = 100*( num_nc_samp / (num_nc_samp + num_non_nc_samp)), .after = \"num_non_nc_samp\")\n#\n#nc_check &lt;- dplyr::left_join(tmp_read_totals, samp_totals, by = \"Representative_Sequence\")\n#write_delim(nc_check, here(med_here, \"nc_screen/reads_in_nc_samples.txt\"),delim = \"\\t\")\n#\n#nc_remove &lt;- nc_check %&gt;% filter(perc_reads_in_nc &gt; 10 | perc_nc_samp &gt; 10)\n#nc_remain &lt;- dplyr::anti_join(nc_check, nc_remove)\n#rem_nc_reads &lt;- sum(nc_remove$total_reads_nc)\n#rem_sam_reads &lt;- sum(nc_remove$total_reads_non_nc)\n#per_reads_rem &lt;- round(100*( rem_nc_reads / (rem_nc_reads + rem_sam_reads)), digits = 3)\n#\n#ret_nc_reads &lt;- sum(nc_remain$total_reads_nc)\n#ret_sam_reads &lt;- sum(nc_remain$total_reads_non_nc)\n#per_reads_ret &lt;- round(100*( ret_nc_reads / (ret_nc_reads + ret_sam_reads)), digits = 3)\n########################################################################\nremove.seqs(accnos=nc_repseq_remove.accnos, fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.fasta, count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table)\ncount.groups(count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table)\n########################################################################\n########## IN R ########################################################\n########################################################################\n#tmp_before &lt;- read_tsv(here(med_here, \"nc_screen/shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count.summary\"), col_names = FALSE, col_select = 1)\n#tmp_after &lt;- read_tsv(here(med_here, \"nc_screen/shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count.summary\"), col_names = FALSE, col_select = 1)\n#tmp_nc_lost &lt;- anti_join(tmp_before, tmp_after)\n#tmp_nc_lost$X1\n#\n#nc_to_remove &lt;- semi_join(tmp_before, tmp_after)\n#nc_to_remove &lt;- nc_to_remove %&gt;% dplyr::filter(stringr::str_starts(X1, \"Control\"))\n# \n#readr::write_delim(nc_to_remove, file = here(med_here, \"nc_screen/nc_samples_remove.accnos\"), col_names = FALSE)\n########################################################################\nremove.groups(count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table, fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta, accnos=nc_samples_remove.accnos)\n\n########################################\n### NEGATIVE CONTROLS Should be GONE ###\n########################################\n\nsummary.seqs(fasta=current, count=current, processors=$PROC)\ncount.groups(count=current)\nchimera.vsearch(fasta=current, count=current, dereplicate=t, processors=$PROC)\nsummary.seqs(fasta=current, count=current, processors=$PROC)\ncount.groups(count=current)\nclassify.seqs(fasta=current, count=current, reference=reference_dbs/gsrdb.fasta, taxonomy=reference_dbs/gsrdb.tax, processors=3)\nremove.lineage(fasta=current, count=current, taxonomy=current, taxon=Chloroplast-Mitochondria-unknown-Eukaryota)\nsummary.tax(taxonomy=current, count=current)\ncount.groups(count=current)\n\nchimera.vsearch(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.fasta, count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.count_table, dereplicate=t, processors=$PROC)\nsummary.seqs(fasta=current, count=current, processors=$PROC)\ncount.groups(count=current, processors=$PROC)\n\nclassify.seqs(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.fasta, count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.count_table, processors=$PROC, reference=reference_dbs/gsrdb.fasta, taxonomy=reference_dbs/gsrdb.tax)\n\nremove.lineage(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.fasta, count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.count_table, taxonomy=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.taxonomy, taxon=Chloroplast-Mitochondria-unknown-Eukaryota)\n\nsummary.tax(taxonomy=current, count=current)\ncount.groups(count=current, processors=$PROC)\n\n##########################\nrename.file(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.pick.fasta, count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.pick.count_table, taxonomy=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.pick.taxonomy, prefix=final)\n\n##########################\n###    CLUSTERING      ###\n##########################\n\n##########################\n###    cluster.split   ###\n##########################\n\ncluster.split(fasta=final.fasta, count=final.count_table, taxonomy=final.taxonomy, taxlevel=4, cluster=f, processors=$PROC) \ncluster.split(file=final.file, count=final.count_table, processors=$PROC)\n\nsystem(mkdir pipelineFiles/cluster.split.gsrdb)\nsystem(mv pipelineFiles/final.opti_mcc.list pipelineFiles/cluster.split.gsrdb/)\nsystem(mv pipelineFiles/final.file pipelineFiles/cluster.split.gsrdb/)\nsystem(mv pipelineFiles/final.dist pipelineFiles/cluster.split.gsrdb/)\n\n##########################\n###    cluster         ###\n##########################\n\ndist.seqs(fasta=final.fasta, cutoff=0.03, processors=$PROC)\ncluster(column=final.dist, count=final.count_table)\n\nquit()\n\n\nOnce you have the script and data you simply run the pipeline like so.\n\nconda activate mothur\nmothur otu_batchfile_processing\n\nIn the resources listed above, we include a table that summarizes read changes for each sample through the pipeline."
  },
  {
    "objectID": "workflows/pub/archive_data/index.html",
    "href": "workflows/pub/archive_data/index.html",
    "title": "Submitting & Archiving Data/Code",
    "section": "",
    "text": "All sequence data is deposited at the European Nucleotide Archive (ENA). See below for instructions on submitting data to the ENA and the files we used to deposit the data.\nThe trimmed 16S rRNA data (primers removed) are deposited under Project Accession number PRJEB36632 (ERP119845), sample accession numbers ERS4291994-ERS4292031.\nThe metagenomic data from the four water column samples are also deposited under Project Accession number PRJEB36632 (ERP119845). The individual sample accession numbers for these data are ERS4578390-ERS4578393.",
    "crumbs": [
      "Home",
      "Publication Material",
      "Submitting & Archiving Data/Code"
    ]
  },
  {
    "objectID": "workflows/pub/archive_data/index.html#archived-sequence-data",
    "href": "workflows/pub/archive_data/index.html#archived-sequence-data",
    "title": "Submitting & Archiving Data/Code",
    "section": "",
    "text": "All sequence data is deposited at the European Nucleotide Archive (ENA). See below for instructions on submitting data to the ENA and the files we used to deposit the data.\nThe trimmed 16S rRNA data (primers removed) are deposited under Project Accession number PRJEB36632 (ERP119845), sample accession numbers ERS4291994-ERS4292031.\nThe metagenomic data from the four water column samples are also deposited under Project Accession number PRJEB36632 (ERP119845). The individual sample accession numbers for these data are ERS4578390-ERS4578393.",
    "crumbs": [
      "Home",
      "Publication Material",
      "Submitting & Archiving Data/Code"
    ]
  },
  {
    "objectID": "workflows/pub/archive_data/index.html#pipeline-data",
    "href": "workflows/pub/archive_data/index.html#pipeline-data",
    "title": "Submitting & Archiving Data/Code",
    "section": "Pipeline Data",
    "text": "Pipeline Data\nData for each individual pipeline are available through the Smithsonian figshare under a single collection at doi:10.25573/data.c.5025362. In addition, data from each pipeline are available for download from figshare using the links at the bottom of each page (where applicable).",
    "crumbs": [
      "Home",
      "Publication Material",
      "Submitting & Archiving Data/Code"
    ]
  },
  {
    "objectID": "workflows/pub/archive_data/index.html#submitting-sequence-data",
    "href": "workflows/pub/archive_data/index.html#submitting-sequence-data",
    "title": "Submitting & Archiving Data/Code",
    "section": "Submitting Sequence Data",
    "text": "Submitting Sequence Data\nWe submitted out data to the European Nucleotide Archive (ENA). The ENA does not like RAW data and prefers to have primers removed. So we submitted the trimmed Fastq files to the ENA. You can find these data under the study accession number PRJEB36632 (ERP119845). The RAW files on our figshare site (see above).\nTo submit to the ENA you need two data tables (plus your sequence data). One file describes the samples and the other file describes the sequencing data.\nYou can download these data tables here:\n16S rRNA\n\nDescription of sample data\nDescription of sequence data\nMetagenomic\n\nDescription of sample data\nDescription of sequence data\nSubmitting to the ENA\n\ngo to https://www.ebi.ac.uk/ena/submit and select Submit to ENA.\nLogin or Register.\nGo to New Submission tab and select Register study (project).\nHit Next\nEnter details and hit Submit.\nNext, Select Checklist. This will be specific to the type of samples you have and basically will create a template so you can add your sample metadata. For this study I chose GSC MIxS water, checklist accession number ERC000024\n\nNext\nNow go through and select/deselect fields as needed. Note, some fields are mandatory.\nOnce finished, hit Next to fill in any details that apply to All samples.\nFill in the sheet\nHit the Next button, change the number of samples, and download the sheet. (This is a little messy and you just need to wade through it)\nOnce everything looks good and uploaded, click Next to get to the Run page.\nSelect Two Fastq files (Paired) and Download the template.\nBefore filling out the form, gzip .gz all the trimmed fastq files (these are what you submit)\nThen run md5sum on all the tar.gz files.\nUpload all the fastq files. You must do this before submitting the experiment spreadsheet. There are different options for this step. I opened a terminal and typed ftp webin.ebi.ac.uk. I entered my username and password. Then typed mput *.gz. The problem was that I had to say yes for each file. Should be a way around this. Documentation can be found here. Probably need to type prompt first.\nFill in the sheet including md5 checksum values.\nUpload and submit the sheet.",
    "crumbs": [
      "Home",
      "Publication Material",
      "Submitting & Archiving Data/Code"
    ]
  },
  {
    "objectID": "workflows/pub/references/index.html",
    "href": "workflows/pub/references/index.html",
    "title": "Workflow References",
    "section": "",
    "text": "Order By\nDefault\n\n        Name\n      \n\n        Workflow\n      \n\n        Type\n      \n\n    \n      \n      \n\n\n\n\n\nName\n\n\n\nVersion\n\n\n\nWorkflow\n\n\n\nType\n\n\n\nReference\n\n\n\n\n\n\ndada2\n\n\nv1.26.0\n\n\n16S rRNA\n\n\nsoftware\n\n\n(Callahan et al. 2016)\n\n\n\n\n\nmothur\n\n\nv.1.43.0\n\n\n16S rRNA\n\n\nsoftware\n\n\n(Schloss et al. 2009)\n\n\n\n\n\nMED/Oligotyping\n\n\nv3.2-dev\n\n\n16S rRNA\n\n\nsoftware\n\n\n(Eren et al. 2015)\n\n\n\n\n\nLotuS3\n\n\nv3.03\n\n\n16S rRNA\n\n\nsoftware\n\n\n(Özkurt et al. 2022)\n\n\n\n\n\nMicroeco\n\n\nv1.9.1\n\n\n16S rRNA\n\n\nsoftware\n\n\n(Liu et al. 2021)\n\n\n\n\n\nSeqKit\n\n\nv2.8.2\n\n\n16S rRNA\n\n\nsoftware\n\n\n(Shen, Sipos, and Zhao 2024)\n\n\n\n\n\nvsearch\n\n\nv2.30.0\n\n\n16S rRNA\n\n\nsoftware\n\n\n(Rognes et al. 2016)\n\n\n\n\n\nGSRDB\n\n\n \n\n\n16S rRNA\n\n\ndatabase\n\n\n(Molano, Vega-Abellaneda, and Manichanh 2024)\n\n\n\n\n\nanvi’o\n\n\nv8(dev)\n\n\nmetagenomic\n\n\nsoftware\n\n\n(Eren et al. 2021)\n\n\n\n\n\nkraken2\n\n\nv2.1.3\n\n\nmetagenomic\n\n\nsoftware\n\n\n(Wood, Lu, and Langmead 2019)\n\n\n\n\n\nmegahit\n\n\nv1.2.9\n\n\nmetagenomic\n\n\nsoftware\n\n\n(D. Li et al. 2015)\n\n\n\n\n\nsnakemake\n\n\nv7.32.4\n\n\nmetagenomic\n\n\nsoftware\n\n\n(Köster and Rahmann 2012)\n\n\n\n\n\nBOWTIE2\n\n\nv2.5.2\n\n\nmetagenomic\n\n\nsoftware\n\n\n(Langmead and Salzberg 2012)\n\n\n\n\n\nSAMtools\n\n\nv1.18\n\n\nmetagenomic\n\n\nsoftware\n\n\n(H. Li et al. 2009)\n\n\n\n\n\nIU filterquality Minoche\n\n\nv2.13\n\n\nmetagenomic\n\n\nsoftware\n\n\n(Eren et al. 2013; Minoche, Dohm, and Himmelbauer 2011)\n\n\n\n\n\nKAIJU\n\n\nv1.10.1\n\n\nmetagenomic\n\n\nsoftware\n\n\n(Menzel, Ng, and Krogh 2016)\n\n\n\n\n\nCentrifuge\n\n\nv1.0.4.2\n\n\nmetagenomic\n\n\nsoftware\n\n\n(Kim et al. 2016)\n\n\n\n\n\nPRODIGAL\n\n\nV2.6.3\n\n\nmetagenomic\n\n\nsoftware\n\n\n(Hyatt et al. 2010)\n\n\n\n\n\nVirsorter2\n\n\nv2.2.4\n\n\nmetagenomic\n\n\nsoftware\n\n\n(Guo et al. 2021)\n\n\n\n\n\nCONCOCT\n\n\nv1.1.0\n\n\nmetagenomic\n\n\nsoftware\n\n\n(Alneberg et al. 2014)\n\n\n\n\n\nDASTOOL\n\n\nv1.1.7\n\n\nmetagenomic\n\n\nsoftware\n\n\n(Sieber et al. 2018)\n\n\n\n\n\nMETABAT2\n\n\nv2.17\n\n\nmetagenomic\n\n\nsoftware\n\n\n(Kang et al. 2019)\n\n\n\n\n\nMAXBIN2\n\n\n2.2.7\n\n\nmetagenomic\n\n\nsoftware\n\n\n(Wu, Simmons, and Singer 2016)\n\n\n\n\n\nCOG\n\n\nCOG20\n\n\nmetagenomic\n\n\ndatabase\n\n\n(Galperin et al. 2021)\n\n\n\n\n\nKEGG\n\n\n \n\n\nmetagenomic\n\n\ndatabase\n\n\n(Aramaki et al. 2020)\n\n\n\n\n\nCAZymes\n\n\nv11\n\n\nmetagenomic\n\n\ndatabase\n\n\n(Drula et al. 2022)\n\n\n\n\n\nPfams\n\n\nv37.0\n\n\nmetagenomic\n\n\ndatabase\n\n\n(Mistry et al. 2021)\n\n\n\n\n\nGTDB(SCG taxonomy)\n\n\nv214.1\n\n\nmetagenomic\n\n\ndatabase\n\n\n(Chaumeil et al. 2019)\n\n\n\n\n\nGTDB(tRNA taxonomy)\n\n\nv89\n\n\nmetagenomic\n\n\ndatabase\n\n\n(Chaumeil et al. 2019)\n\n\n\n\n\nArchaea_76 &Bacteria_71\n\n\n \n\n\nmetagenomic\n\n\nHMM database\n\n\n(Lee 2019)\n\n\n\n\n\nBacteria_71\n\n\n \n\n\nmetagenomic\n\n\nHMM database\n\n\n(Lee 2019)\n\n\n\n\n\nProtista_83\n\n\n0.9\n\n\nmetagenomic\n\n\nHMM database\n\n\n(Seemann 2018)\n\n\n\n\n\nRibosomal RNA (12S, 16S, 18S,23S, 28S, 5S)\n\n\n0.9\n\n\nmetagenomic\n\n\nHMM database\n\n\n(Seemann 2018)\n\n\n\n\n\nNo matching items\n\n\nReferences\nExpand to see the formatted references\n\nAlneberg, Johannes, Brynjar Smári Bjarnason, Ino De Bruijn, Melanie Schirmer, Joshua Quick, Umer Z Ijaz, Leo Lahti, Nicholas J Loman, Anders F Andersson, and Christopher Quince. 2014. “Binning Metagenomic Contigs by Coverage and Composition.” Nature Methods 11 (11): 11441146. https://doi.org/10.1038/nmeth.3103.\n\n\nAramaki, Takuya, Romain Blanc-Mathieu, Hisashi Endo, Koichi Ohkubo, Minoru Kanehisa, Susumu Goto, and Hiroyuki Ogata. 2020. “KofamKOALA: KEGG Ortholog Assignment Based on Profile HMM and Adaptive Score Threshold.” Bioinformatics 36 (7): 22512252. https://doi.org/10.1093/bioinformatics/btz859.\n\n\nCallahan, Benjamin J, Paul J McMurdie, Michael J Rosen, Andrew W Han, Amy Jo A Johnson, and Susan P Holmes. 2016. “DADA2: High-Resolution Sample Inference from Illumina Amplicon Data.” Nature Methods 13 (7): 581. https://doi.org/10.1038/nmeth.3869.\n\n\nChaumeil, Pierre-Alain, Aaron J Mussig, Philip Hugenholtz, and Donovan H Parks. 2019. “GTDB-Tk: A Toolkit to Classify Genomes with the Genome Taxonomy Database.” Bioinformatics 36 (6): 1925–27. https://doi.org/10.1093/bioinformatics/btz848.\n\n\nDrula, Elodie, Marie-Line Garron, Suzan Dogan, Vincent Lombard, Bernard Henrissat, and Nicolas Terrapon. 2022. “The Carbohydrate-Active Enzyme Database: Functions and Literature.” Nucleic Acids Research 50 (D1): D571D577. https://doi.org/10.1093/nar/gkab1045.\n\n\nEren, A Murat, E Kiefl, A Shaiber, I Veseli, SE Miller, MS Schechter, I Fink, et al. 2021. “Community-Led, Integrated, Reproducible Multi-Omics with Anvi’o. 6: 3–6.” Nature Microbiology. https://doi.org/10.1038/s41564-020-00834-3.\n\n\nEren, A Murat, Hilary G Morrison, Pamela J Lescault, Julie Reveillaud, Joseph H Vineis, and Mitchell L Sogin. 2015. “Minimum Entropy Decomposition: Unsupervised Oligotyping for Sensitive Partitioning of High-Throughput Marker Gene Sequences.” The ISME Journal 9 (4): 968–79. https://doi.org/10.1038/ismej.2014.195.\n\n\nEren, A Murat, Joseph H Vineis, Hilary G Morrison, and Mitchell L Sogin. 2013. “A Filtering Method to Generate High Quality Short Reads Using Illumina Paired-End Technology.” PLoS One 8 (6). https://doi.org/10.1371/journal.pone.0066643.\n\n\nGalperin, Michael Y, Yuri I Wolf, Kira S Makarova, Roberto Vera Alvarez, David Landsman, and Eugene V Koonin. 2021. “COG Database Update: Focus on Microbial Diversity, Model Organisms, and Widespread Pathogens.” Nucleic Acids Research 49 (D1): D274D281. https://doi.org/10.1093/nar/gkaa1018.\n\n\nGuo, Jiarong, Ben Bolduc, Ahmed A Zayed, Arvind Varsani, Guillermo Dominguez-Huerta, Tom O Delmont, Akbar Adjie Pratama, et al. 2021. “VirSorter2: A Multi-Classifier, Expert-Guided Approach to Detect Diverse DNA and RNA Viruses.” Microbiome 9: 113. https://doi.org/10.1186/s40168-020-00990-y.\n\n\nHyatt, Doug, Gwo-Liang Chen, Philip F LoCascio, Miriam L Land, Frank W Larimer, and Loren J Hauser. 2010. “Prodigal: Prokaryotic Gene Recognition and Translation Initiation Site Identification.” BMC Bioinformatics 11 (1): 119. https://doi.org/10.1186/1471-2105-11-119.\n\n\nKang, Dongwan D, Feng Li, Edward Kirton, Ashleigh Thomas, Rob Egan, Hong An, and Zhong Wang. 2019. “MetaBAT 2: An Adaptive Binning Algorithm for Robust and Efficient Genome Reconstruction from Metagenome Assemblies.” PeerJ 7: e7359. https://doi.org/10.7717%2Fpeerj.7359.\n\n\nKim, Daehwan, Li Song, Florian P Breitwieser, and Steven L Salzberg. 2016. “Centrifuge: Rapid and Sensitive Classification of Metagenomic Sequences.” Genome Research 26 (12): 1721–29. https://doi.org/10.1101/gr.210641.116.\n\n\nKöster, Johannes, and Sven Rahmann. 2012. “Snakemake—a Scalable Bioinformatics Workflow Engine.” Bioinformatics 28 (19): 2520–22. https://doi.org/10.1093/bioinformatics/bts480.\n\n\nLangmead, Ben, and Steven L Salzberg. 2012. “Fast Gapped-Read Alignment with Bowtie 2.” Nature Methods 9 (4): 357. https://doi.org/10.1038/nmeth.1923.\n\n\nLee, Michael D. 2019. “GToTree: A User-Friendly Workflow for Phylogenomics.” Bioinformatics 35 (20): 41624164. https://doi.org/10.1093/bioinformatics/btz188.\n\n\nLi, Dinghua, Chi-Man Liu, Ruibang Luo, Kunihiko Sadakane, and Tak-Wah Lam. 2015. “MEGAHIT: An Ultra-Fast Single-Node Solution for Large and Complex Metagenomics Assembly via Succinct de Bruijn Graph.” Bioinformatics 31 (10): 1674–76. https://doi.org/10.1093/bioinformatics/btv033.\n\n\nLi, Heng, Bob Handsaker, Alec Wysoker, Tim Fennell, Jue Ruan, Nils Homer, Gabor Marth, Goncalo Abecasis, and Richard Durbin. 2009. “The Sequence Alignment/Map Format and SAMtools.” Bioinformatics 25 (16): 2078–79. https://doi.org/10.1093/bioinformatics/btp352.\n\n\nLiu, Chi, Yaoming Cui, Xiangzhen Li, and Minjie Yao. 2021. “Microeco: An r Package for Data Mining in Microbial Community Ecology.” FEMS Microbiology Ecology 97 (2): fiaa255. https://doi.org/10.1038/s41596-025-01239-4.\n\n\nMenzel, Peter, Kim Lee Ng, and Anders Krogh. 2016. “Fast and Sensitive Taxonomic Classification for Metagenomics with Kaiju.” Nature Communications 7: 11257. https://doi.org/10.1038/ncomms11257.\n\n\nMinoche, André E, Juliane C Dohm, and Heinz Himmelbauer. 2011. “Evaluation of Genomic High-Throughput Sequencing Data Generated on Illumina HiSeq and Genome Analyzer Systems.” Genome Biology 12 (11): R112. https://doi.org/10.1186/gb-2011-12-11-r112.\n\n\nMistry, Jaina, Sara Chuguransky, Lowri Williams, Matloob Qureshi, Gustavo A Salazar, Erik LL Sonnhammer, Silvio CE Tosatto, et al. 2021. “Pfam: The Protein Families Database in 2021.” Nucleic Acids Research 49 (D1): D412D419. https://doi.org/10.1093/nar/gkaa913.\n\n\nMolano, Leidy-Alejandra G, Sara Vega-Abellaneda, and Chaysavanh Manichanh. 2024. “GSR-DB: A Manually Curated and Optimized Taxonomical Database for 16S rRNA Amplicon Analysis.” Msystems 9 (2): e00950–23. https://doi.org/10.1128/msystems.00950-23.\n\n\nÖzkurt, Ezgi, Joachim Fritscher, Nicola Soranzo, Duncan YK Ng, Robert P Davey, Mohammad Bahram, and Falk Hildebrand. 2022. “LotuS2: An Ultrafast and Highly Accurate Tool for Amplicon Sequencing Analysis.” Microbiome 10 (1): 176. https://doi.org/10.1186/s40168-022-01365-1.\n\n\nRognes, Torbjørn, Tomáš Flouri, Ben Nichols, Christopher Quince, and Frédéric Mahé. 2016. “VSEARCH: A Versatile Open Source Tool for Metagenomics.” PeerJ 4: e2584. https://doi.org/10.7717/peerj.2584.\n\n\nSchloss, Patrick D, Sarah L Westcott, Thomas Ryabin, Justine R Hall, Martin Hartmann, Emily B Hollister, Ryan A Lesniewski, et al. 2009. “Introducing Mothur: Open-Source, Platform-Independent, Community-Supported Software for Describing and Comparing Microbial Communities.” Applied and Environmental Microbiology 75 (23): 7537–41. https://doi.org/10.1128/AEM.01541-09.\n\n\nSeemann, T. 2018. “Barrnap 0.9 : Rapid ribosomal RNA Prediction.” https://github.com/tseemann/barrnap.\n\n\nShen, Wei, Botond Sipos, and Liuyang Zhao. 2024. “SeqKit2: A Swiss Army Knife for Sequence and Alignment Processing.” Imeta 3 (3): e191. https://doi.org/10.1002/imt2.191.\n\n\nSieber, Christian MK, Alexander J Probst, Allison Sharrar, Brian C Thomas, Matthias Hess, Susannah G Tringe, and Jillian F Banfield. 2018. “Recovery of Genomes from Metagenomes via a Dereplication, Aggregation and Scoring Strategy.” Nature Microbiology 3 (7): 836843. https://doi.org/10.1038/s41564-018-0171-1.\n\n\nWood, Derrick E, Jennifer Lu, and Ben Langmead. 2019. “Improved Metagenomic Analysis with Kraken 2.” Genome Biology 20: 113. https://doi.org/10.1186/s13059-019-1891-0.\n\n\nWu, Yu-Wei, Blake A Simmons, and Steven W Singer. 2016. “MaxBin 2.0: An Automated Binning Algorithm to Recover Genomes from Multiple Metagenomic Datasets.” Bioinformatics 32 (4): 605–7. https://doi.org/10.1093/bioinformatics/btv638.\n\n\nDetailed Session Info\nExpand to see Session Info\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.5.1 (2025-06-13)\n os       macOS Ventura 13.7.8\n system   x86_64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/Los_Angeles\n date     2025-10-02\n pandoc   3.6.3 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/x86_64/ (via rmarkdown)\n quarto   1.8.25 @ /Applications/quarto/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n sessioninfo * 1.2.3   2025-02-05 [1] CRAN (R 4.5.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.5-x86_64/Resources/library\n * ── Packages attached to the search path.\n\n──────────────────────────────────────────────────────────────────────────────\n\n\n\n======   Devtools Session info   ===================================\n\n\n package     * version date (UTC) lib source\n sessioninfo * 1.2.3   2025-02-05 [1] CRAN (R 4.5.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.5-x86_64/Resources/library\n * ── Packages attached to the search path.\n\n\nLast updated on\n2025-10-02\n\n\n\n\n\nCollected Links\n\nQuarto\nmetacrobe\nThe Istmobiome Project",
    "crumbs": [
      "Home",
      "Publication Material",
      "Workflow References"
    ]
  },
  {
    "objectID": "workflows/ssu/index.html",
    "href": "workflows/ssu/index.html",
    "title": "16S rRNA Workflows",
    "section": "",
    "text": "1. Sample Metadata\n\n\nWorkflow for wrangling sample metadata and generating informative sample names.\n\n\n\n\n\n9/29/25, 5:45:00 PM\n\n\n\n\n\n\n\n2. DADA2 ASV Workflow\n\n\nWorkflow for processing 16S rRNA samples for ASV analysis using DADA2. Workflow uses paired end reads, beginning with raw fastq files, ending with sequence and taxonomy tables. A Microtable Object…\n\n\n\n\n\n10/2/25, 2:19:39 PM\n\n\n\n\n\n\n\n4. OTU Workflow\n\n\nWorkflow for processing 16S rRNA samples for OTU analysis using mothur. Workflow uses paired end reads, beginning with raw fastq files, ending with sequence and taxonomy tables. A Microtable Object…\n\n\n\n\n\n10/1/25, 8:48:22 AM\n\n\n\n\n\n\n\n5. MED Workflow\n\n\nWorkflow for running MED analysis. Workflow begins with redundant, aligned fasta file from mothur and ends with the MED analysis. A Microtable Object is produced to collate the data for downstream…\n\n\n\n\n\n10/1/25, 8:48:20 AM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10/1/25, 1:19:30 PM\n\n\n\n\n\nNo matching items\nCollected Links\n\n\nQuarto\nmetacrobe\nThe Istmobiome Project",
    "crumbs": [
      "Home",
      "16S rRNA"
    ]
  },
  {
    "objectID": "workflows/ssu/otu/index.html",
    "href": "workflows/ssu/otu/index.html",
    "title": "4. OTU Workflow",
    "section": "",
    "text": "NoteData & Scripts\n\n\n\nFastq sequence files, scripts, and other assets for running this workflow can be found on the OTU Data Portal page.\nThe Data Portal page also contains a link to the curated output of this pipeline–-feature-sample count table, taxonomy table, sample metadata, & representative fasta sequences. Data is available as stand-alone text files, OR bundled in microtable and phyloseq objects. Archive also includes a table tracking sample read changes.\n\n\n\nTo run this workflow you need to have mothur installed and you will need the tidyverse package.",
    "crumbs": [
      "Home",
      "16S rRNA",
      "4. OTU Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/otu/index.html#workflow-input",
    "href": "workflows/ssu/otu/index.html#workflow-input",
    "title": "4. OTU Workflow",
    "section": "",
    "text": "NoteData & Scripts\n\n\n\nFastq sequence files, scripts, and other assets for running this workflow can be found on the OTU Data Portal page.\nThe Data Portal page also contains a link to the curated output of this pipeline–-feature-sample count table, taxonomy table, sample metadata, & representative fasta sequences. Data is available as stand-alone text files, OR bundled in microtable and phyloseq objects. Archive also includes a table tracking sample read changes.\n\n\n\nTo run this workflow you need to have mothur installed and you will need the tidyverse package.",
    "crumbs": [
      "Home",
      "16S rRNA",
      "4. OTU Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/otu/index.html#overview",
    "href": "workflows/ssu/otu/index.html#overview",
    "title": "4. OTU Workflow",
    "section": "Overview",
    "text": "Overview\nThis is a standard pipeline for generating OTUs using mothur(Schloss et al. 2009). We generally followed the mothur MiSeq SOP when building this pipeline. Since this SOP is heavily annotated we will keep our comments here to a minimum.\n\n\n\n\n\n\n\nflowchart TB\n  A(&lt;b&gt;&lt;a href='https://mothur.org/wiki/make.file/'&gt;make.file&lt;/a&gt;&lt;/b&gt;) --&gt; B(&lt;b&gt;&lt;a href='https://mothur.org/wiki/make.contigs/'&gt;make.contigs&lt;/a&gt;&lt;/b&gt;)\n  B --&gt; C(&lt;b&gt;&lt;b&gt;&lt;a href='https://mothur.org/wiki/screen.seqs/'&gt;screen.seqs&lt;/a&gt;&lt;/b&gt;&lt;br/&gt;&lt;a href='https://mothur.org/wiki/unique.seqs/'&gt;unique.seqs&lt;/a&gt;&lt;/b&gt;)\n  C --&gt; E(&lt;b&gt;&lt;a href='https://mothur.org/wiki/align.seqs/'&gt;align.seqs&lt;/a&gt;&lt;/b&gt;)\n  L(reference db&lt;br/&gt; e.g. silva seed) --&gt; F\n  F(&lt;b&gt;&lt;a href='https://mothur.org/wiki/pcr.seqs/'&gt;pcr.seqs&lt;/a&gt;&lt;/b&gt;&lt;/br&gt;trim ref db&lt;/br&gt;to V4 region) --&gt; E\n  E --&gt; I(&lt;b&gt;&lt;a href='https://mothur.org/wiki/screen.seqs/'&gt;screen.seqs&lt;/a&gt;&lt;br/&gt;&lt;a href='https://mothur.org/wiki/filter.seqs/'&gt;filter.seqs&lt;/a&gt;&lt;br/&gt;&lt;a href='https://mothur.org/wiki/unique.seqs/'&gt;unique.seqs&lt;/a&gt;&lt;/b&gt;)\n  I --&gt; K[Pass to MED]\n  I --&gt; J(&lt;b&gt;&lt;a href='https://mothur.org/wiki/pre.cluster/'&gt;pre.cluster&lt;/a&gt;&lt;/b&gt;)\n  J --&gt; M(remove&lt;/br&gt;negative controls)\n\n\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  L(\"&lt;b&gt;&lt;a href='https://mothur.org/wiki/get.groups/'&gt;get.groups&lt;/a&gt;&lt;/b&gt;&lt;br/&gt;(NC Samples)\")\n  L --&gt; M(&lt;b&gt;&lt;a href='https://mothur.org/wiki/remove.seqs/'&gt;remove.seqs&lt;/a&gt;&lt;/b&gt;)\n  L --&gt; N(&lt;b&gt;&lt;a href='https://mothur.org/wiki/remove.groups/'&gt;remove.groups&lt;/a&gt;&lt;/b&gt;)\n  M --&gt; O(&lt;b&gt;&lt;a href='https://mothur.org/wiki/chimera.vsearch/'&gt;chimera.vsearch&lt;/a&gt;&lt;/b&gt;)\n  N --&gt; O\n  O --&gt; P(&lt;b&gt;&lt;a href='https://mothur.org/wiki/classify.seqs/'&gt;classify.seqs&lt;/a&gt;&lt;/b&gt;)\n  P --&gt; Q(&lt;b&gt;&lt;a href='https://mothur.org/wiki/remove.lineage/'&gt;remove.lineage&lt;/a&gt;&lt;/b&gt;)\n  Q --&gt; R(&lt;b&gt;&lt;a href='https://mothur.org/wiki/cluster.split/'&gt;cluster.split&lt;/a&gt;&lt;/b&gt;)\n  Q --&gt; S(&lt;b&gt;&lt;a href='https://mothur.org/wiki/dist.seqs/'&gt;dist.seqs&lt;/a&gt;&lt;/b&gt;)\n\ntitle[Remove&lt;br/&gt;Negative&lt;br/&gt;Controls]\nstyle title stroke:#333,stroke-width:0px,mermaid-font-size:2.8rem",
    "crumbs": [
      "Home",
      "16S rRNA",
      "4. OTU Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/otu/index.html#getting-started",
    "href": "workflows/ssu/otu/index.html#getting-started",
    "title": "4. OTU Workflow",
    "section": "Getting Started",
    "text": "Getting Started\n\n\n\n\n\n\nNoteExpand for the MOTHUR batchfile\n\n\n\n\n\n########################################################################\n#$ export DATA=01_TRIMMED_DATA/\n#$ export TYPE=gz\n#$ export PROC=30\n\n#$ export REF_LOC=reference_dbs\n#$ export TAXREF_FASTA=gsrdb.fasta\n#$ export TAXREF_TAX=gsrdb.tax\n#$ export ALIGNREF=silva.v4.fasta\n\n#$ export CONTAMINENTS=Chloroplast-Mitochondria-unknown-Eukaryota\n########################################################################\nset.dir(output=pipelineFiles/)\nmake.file(inputdir=$DATA/, type=$TYPE, prefix=shrimp)\n########################################################################\n#### had to fix shimp.files bc mothur splits name at first underscore (_)\n########################################################################\nmake.contigs(file=shrimp.files, processors=$PROC)\nsummary.seqs(fasta=shrimp.trim.contigs.fasta, count=shrimp.contigs.count_table, processors=$PROC)\ncount.groups(count=shrimp.contigs.count_table)\nscreen.seqs(fasta=current, count=current, maxambig=0, minlength=252, maxlength=254, maxhomop=6, processors=$PROC)\nsummary.seqs(fasta=current, count=current, processors=$PROC)\ncount.groups(count=current)\nunique.seqs(fasta=current, count=current)\nsummary.seqs(count=current, processors=$PROC)\n########################################################################\n#### Aligning reads\n########################################################################\n# https://mothur.org/wiki/silva_reference_files/\n#### Prep reference file\npcr.seqs(fasta=$REF_LOC/silva.seed_v138_2.align, start=13862, end=23445, keepdots=F, processors=$PROC)\nrename.file(input=pipelineFiles/silva.seed_v138_2.pcr.align, new=pipelineFiles/$ALIGNREF)\nsummary.seqs(fasta=pipelineFiles/$ALIGNREF, processors=$PROC)\n#### Align reads\nalign.seqs(fasta=shrimp.trim.contigs.good.unique.fasta, reference=pipelineFiles/$ALIGNREF, processors=$PROC)\nsummary.seqs(fasta=shrimp.trim.contigs.good.unique.align, count=shrimp.trim.contigs.good.count_table, processors=$PROC)\n########################################################################\n#### Further processing\n########################################################################\nscreen.seqs(fasta=current, count=current, start=1, end=9583, processors=$PROC)\nsummary.seqs(fasta=current, count=current, processors=$PROC)\ncount.groups(count=current)\nfilter.seqs(fasta=current, vertical=T, trump=., processors=$PROC)\nunique.seqs(fasta=current, count=current)\nsummary.seqs(fasta=current, count=current, processors=$PROC)\n########################################################################\n#### Copy files FOR MED\n########################################################################\nsystem(cp pipelineFiles/shrimp.trim.contigs.good.unique.good.filter.unique.fasta pipelineFiles_med/)\nsystem(cp pipelineFiles/shrimp.trim.contigs.good.unique.good.filter.count_table pipelineFiles_med/)\n########################################################################\n#### Proceed with mothur workflow\n########################################################################\npre.cluster(fasta=current, count=current, diffs=2, processors=$PROC)\nsummary.seqs(fasta=current, count=current, processors=$PROC)\ncount.groups(count=current)\n########################################################################\n#### Remove Negative Control samples (files generated in R)\n########################################################################\n\n########## IN R ########################################################\n#tmp_accnos &lt;- readr::read_delim(here(med_here, \"nc_screen/shrimp.files\"), delim = \"\\t\", col_names = FALSE)\n#tmp_accnos[, 2:3] &lt;- NULL\n#tmp_accnos &lt;- tmp_accnos[grepl(\"Control_\", tmp_accnos$X1), ]\n#readr::write_delim(tmp_accnos, file = here(med_here, \"nc_screen/nc_samples.accnos\"), col_names = FALSE)\n########################################################################\nget.groups(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.fasta, count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table, accnos=nc_samples.accnos)\nrename.file(input=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta, new=nc.fasta)\nrename.file(input=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table, new=nc.count_table)\nsummary.seqs(fasta=nc.fasta, count=nc.count_table, processors=$PROC)\nlist.seqs(count=nc.count_table)\ncount.seqs(count=nc.count_table, compress=f)\nget.seqs(accnos=nc.accnos, fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.fasta, count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table)\nrename.file(input=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta, new=subset.fasta)\nrename.file(input=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table, new=subset.count_table)\ncount.seqs(count=subset.count_table, compress=f)\n########################################################################\n########## IN R ########################################################\n########################################################################\n#full_count_tab &lt;- readr::read_delim(here(med_here, \"nc_screen/subset.full.count_table\"), delim = \"\\t\", col_names = TRUE)\n#control_cols     &lt;- grep(\"^Control_\", names(full_count_tab), value = TRUE)\n#noncontrol_cols  &lt;- setdiff(names(full_count_tab)[-(1:2)], control_cols)\n## now do the rowwise sums\n#read_totals &lt;- full_count_tab %&gt;%\n#  rowwise() %&gt;%\n#  mutate(\n#    total_reads_nc   = sum(c_across(all_of(control_cols)), na.rm = TRUE),\n#    total_reads_non_nc = sum(c_across(all_of(noncontrol_cols)), na.rm = TRUE)\n#  ) %&gt;%\n#  ungroup() %&gt;%\n#  select(1, 2, total_reads_nc, total_reads_non_nc)\n#\n#read_totals &lt;- read_totals %&gt;% dplyr::rename(\"total_reads\" = 2)\n#tmp_read_totals &lt;- read_totals %&gt;% dplyr::mutate(perc_reads_in_nc = 100*(total_reads_nc / (total_reads_nc + total_reads_non_nc)), .after = \"total_reads_non_nc\")\n#tmp_read_totals$perc_reads_in_nc &lt;- round(tmp_read_totals$perc_reads_in_nc, digits = 6)\n#\n#control_cols     &lt;- grep(\"^Control_\", names(full_count_tab), value = TRUE)\n#noncontrol_cols  &lt;- setdiff(names(full_count_tab)[-(1:2)], control_cols)\n## rowwise tally of non-zero columns\n#samp_totals &lt;- full_count_tab %&gt;%\n#  rowwise() %&gt;%\n#  mutate(\n#    num_nc_samp     = sum(c_across(all_of(control_cols)) != 0, na.rm = TRUE),\n#    num_non_nc_samp = sum(c_across(all_of(noncontrol_cols)) != 0, na.rm = TRUE)\n#  ) %&gt;%\n#  ungroup() %&gt;%\n#  select(1, num_nc_samp, num_non_nc_samp)\n#\n#samp_totals$total_samp &lt;- samp_totals$num_nc_samp + samp_totals$num_non_nc_samp\n#samp_totals &lt;- samp_totals %&gt;%  dplyr::relocate(\"total_samp\", .after = \"Representative_Sequence\")\n#samp_totals &lt;- samp_totals %&gt;% dplyr::mutate(perc_nc_samp = 100*( num_nc_samp / (num_nc_samp + num_non_nc_samp)), .after = \"num_non_nc_samp\")\n#\n#nc_check &lt;- dplyr::left_join(tmp_read_totals, samp_totals, by = \"Representative_Sequence\")\n#write_delim(nc_check, here(med_here, \"nc_screen/reads_in_nc_samples.txt\"),delim = \"\\t\")\n#\n#nc_remove &lt;- nc_check %&gt;% filter(perc_reads_in_nc &gt; 10 | perc_nc_samp &gt; 10)\n#nc_remain &lt;- dplyr::anti_join(nc_check, nc_remove)\n#rem_nc_reads &lt;- sum(nc_remove$total_reads_nc)\n#rem_sam_reads &lt;- sum(nc_remove$total_reads_non_nc)\n#per_reads_rem &lt;- round(100*( rem_nc_reads / (rem_nc_reads + rem_sam_reads)), digits = 3)\n#\n#ret_nc_reads &lt;- sum(nc_remain$total_reads_nc)\n#ret_sam_reads &lt;- sum(nc_remain$total_reads_non_nc)\n#per_reads_ret &lt;- round(100*( ret_nc_reads / (ret_nc_reads + ret_sam_reads)), digits = 3)\n########################################################################\nremove.seqs(accnos=nc_repseq_remove.accnos, fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.fasta, count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table)\ncount.groups(count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table)\n########################################################################\n########## IN R ########################################################\n########################################################################\n#tmp_before &lt;- read_tsv(here(med_here, \"nc_screen/shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count.summary\"), col_names = FALSE, col_select = 1)\n#tmp_after &lt;- read_tsv(here(med_here, \"nc_screen/shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count.summary\"), col_names = FALSE, col_select = 1)\n#tmp_nc_lost &lt;- anti_join(tmp_before, tmp_after)\n#tmp_nc_lost$X1\n#\n#nc_to_remove &lt;- semi_join(tmp_before, tmp_after)\n#nc_to_remove &lt;- nc_to_remove %&gt;% dplyr::filter(stringr::str_starts(X1, \"Control\"))\n# \n#readr::write_delim(nc_to_remove, file = here(med_here, \"nc_screen/nc_samples_remove.accnos\"), col_names = FALSE)\n########################################################################\nremove.groups(count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table, fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta, accnos=nc_samples_remove.accnos)\nsummary.seqs(fasta=current, count=current, processors=30)\ncount.groups(count=current)\nchimera.vsearch(fasta=current, count=current, dereplicate=t, processors=$PROC)\nsummary.seqs(fasta=current, count=current, processors=30)\ncount.groups(count=current)\nclassify.seqs(fasta=current, count=current, reference=reference_dbs/gsrdb.fasta, taxonomy=reference_dbs/gsrdb.tax, processors=3)\nremove.lineage(fasta=current, count=current, taxonomy=current, taxon=Chloroplast-Mitochondria-unknown-Eukaryota)\nsummary.tax(taxonomy=current, count=current)\ncount.groups(count=current)\n\n##########################\n##rename.file(\nfasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.pick.fasta, count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.pick.count_table, taxonomy=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.pick.taxonomy, prefix=final)\n\n##########################\n###    CLUSTERING      ###\n##########################\n\n##########################\n###    cluster.split   ###\n##########################\n\ncluster.split(fasta=final.fasta, count=final.count_table, taxonomy=final.taxonomy, taxlevel=4, cluster=f, processors=30) \ncluster.split(file=final.file, count=final.count_table, processors=30)\n\nsystem(mkdir pipelineFiles/cluster.split.gsrdb)\nsystem(mv pipelineFiles/final.opti_mcc.list pipelineFiles/cluster.split.gsrdb/)\nsystem(mv pipelineFiles/final.file pipelineFiles/cluster.split.gsrdb/)\nsystem(mv pipelineFiles/final.dist pipelineFiles/cluster.split.gsrdb/)\n\n##########################\n###    cluster         ###\n##########################\n\ndist.seqs(fasta=final.fasta, cutoff=0.03, processors=30)\ncluster(column=final.dist, count=final.count_table)\n\nquit()\n\n\n\nset.dir(output=pipelineFiles/)\nMothur's directories:\noutputDir=pipelineFiles/\nmake.file(inputdir=$DATA, type=$TYPE, prefix=shrimp)\n# OR\nmake.file(inputdir=01_TRIMMED_DATA/, type=gz, prefix=shrimp)\nSetting input directories to: \n    01_TRIMMED_DATA/\n\nOutput File Names: \nshrimp.files",
    "crumbs": [
      "Home",
      "16S rRNA",
      "4. OTU Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/otu/index.html#reducing-sequencing-pcr-errors",
    "href": "workflows/ssu/otu/index.html#reducing-sequencing-pcr-errors",
    "title": "4. OTU Workflow",
    "section": "Reducing Sequencing & PCR Errors",
    "text": "Reducing Sequencing & PCR Errors\nmake.contigs(file=current, processors=$PROC)\n# OR\nmake.contigs(file=shrimp.files, processors=30)\nWe will get the following message if sample names contain dash (-) characters. Mothur will change this for us.\n[WARNING]: group Control-10 contains illegal characters in the name. \nGroup names should not include :, -, or / characters.  The ':' character \nis a special character used in trees. Using ':' will result in your tree \nbeing unreadable by tree reading software.  The '-' character is a special \ncharacter used by mothur to parse group names.  Using the '-' character \nwill prevent you from selecting groups. The '/' character will created \nunreadable filenames when mothur includes the group in an output filename.\n\n[NOTE] Updating Control-10 to Control_10 to avoid downstream issues.\n\n...\n\nTotal of all groups is 44710450\n\nIt took 1257 secs to process 44710450 sequences.\n\nOutput File Names: \nshrimp.trim.contigs.fasta\nshrimp.scrap.contigs.fasta\nshrimp.contigs_report\nshrimp.contigs.count_table\nsummary.seqs(fasta=current, count=current, processors=$PROC)\nsummary.seqs(fasta=shrimp.trim.contigs.fasta, count=shrimp.contigs.count_table, processors=30)\n\n\n\n\n\n\nNoteExpand to see data set summary\n\n\n\n\n\n\n\n\nStart\nEnd\nNBases\nAmbigs\nPolymer\nNumSeqs\n\n\n\nMinimum\n1\n226\n226\n0\n2\n1\n\n\n2.5%-tile:\n1\n252\n252\n0\n3\n1117762\n\n\n25%-tile:\n1\n253\n253\n0\n4\n11177613\n\n\nMedian:\n1\n253\n253\n0\n4\n22355226\n\n\n75%-tile:\n1\n253\n253\n0\n5\n33532838\n\n\n97.5%-tile:\n1\n254\n254\n6\n6\n43592689\n\n\nMaximum:\n1\n480\n480\n95\n233\n44710450\n\n\nMean:\n1\n254\n254\n0\n4\n\n\n\n\n# of unique seqs:   44710450\ntotal # of seqs:    44710450\n\nIt took 782 secs to summarize 44710450 sequences.\n\nOutput File Names:\nshrimp.trim.contigs.summary\n\n\n\ncount.groups(count=current)\ncount.groups(count=shrimp.contigs.count_table)\nSize of smallest group: 58.\n\nTotal seqs: 44710450.\nscreen.seqs(fasta=current, count=current, maxambig=0, minlength=252, maxlength=254, maxhomop=6, processors=$PROC)\nscreen.seqs(fasta=current, count=current, maxambig=0, minlength=252, maxlength=254, maxhomop=6, processors=30)\nUsing 30 processors.\n\nIt took 107 secs to screen 44710450 sequences, removed 8308318.\n/******************************************/\nRunning command: remove.seqs(accnos=shrimp.trim.contigs.bad.accnos.temp, count=shrimp.contigs.count_table)\nRemoved 8308318 sequences from shrimp.contigs.count_table.\n\nOutput File Names:\nshrimp.contigs.pick.count_table\n\n/******************************************/\n\nOutput File Names:\nshrimp.trim.contigs.good.fasta\nshrimp.trim.contigs.bad.accnos\nshrimp.contigs.good.count_table\n\nIt took 557 secs to screen 44710450 sequences.\nsummary.seqs(fasta=current, count=current, processors=$PROC)\nsummary.seqs(fasta=shrimp.trim.contigs.good.fasta, count=shrimp.contigs.good.count_table, processors=30)\n\n\n\n\n\n\nNoteExpand to see data set summary\n\n\n\n\n\n\n\n\nStart\nEnd\nNBases\nAmbigs\nPolymer\nNumSeqs\n\n\n\nMinimum\n1\n252\n252\n0\n3\n1\n\n\n2.5%-tile:\n1\n252\n252\n0\n3\n910054\n\n\n25%-tile:\n1\n253\n253\n0\n4\n9100534\n\n\nMedian:\n1\n253\n253\n0\n4\n18201067\n\n\n75%-tile:\n1\n253\n253\n0\n5\n27301600\n\n\n97.5%-tile:\n1\n254\n254\n6\n6\n35492079\n\n\nMaximum:\n1\n254\n254\n0\n6\n36402132\n\n\nMean:\n1\n252\n252\n0\n4\n\n\n\n\n# of unique seqs:   36402132\ntotal # of seqs:    36402132\n\nIt took 610 secs to summarize 36402132 sequences.\n\nOutput File Names:\nshrimp.trim.contigs.good.summary\n\n\n\ncount.groups(count=current)\ncount.groups(count=shrimp.contigs.good.count_table)\nSize of smallest group: 57.\n\nTotal seqs: 36402132.\nOutput File Names: \nshrimp.contigs.good.count.summary",
    "crumbs": [
      "Home",
      "16S rRNA",
      "4. OTU Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/otu/index.html#processing-improved-reads",
    "href": "workflows/ssu/otu/index.html#processing-improved-reads",
    "title": "4. OTU Workflow",
    "section": "Processing Improved Reads",
    "text": "Processing Improved Reads\nunique.seqs(fasta=current, count=current)\nunique.seqs(fasta=shrimp.trim.contigs.good.fasta, count=shrimp.contigs.good.count_table)\n36402132    4224192\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.fasta\nshrimp.trim.contigs.good.count_table\nsummary.seqs(count=current, processors=$PROC)\nsummary.seqs(count=shrimp.trim.contigs.good.count_table, processors=30)\n\n\n\n\n\n\nNoteExpand to see data set summary\n\n\n\n\n\n\n\n\nStart\nEnd\nNBases\nAmbigs\nPolymer\nNumSeqs\n\n\n\nMinimum\n1\n252\n252\n0\n3\n1\n\n\n2.5%-tile:\n1\n252\n252\n0\n3\n910054\n\n\n25%-tile:\n1\n253\n253\n0\n4\n9100534\n\n\nMedian:\n1\n253\n253\n0\n4\n18201067\n\n\n75%-tile:\n1\n253\n253\n0\n5\n27301600\n\n\n97.5%-tile:\n1\n254\n254\n6\n6\n35492079\n\n\nMaximum:\n1\n254\n254\n0\n6\n36402132\n\n\nMean:\n1\n252\n252\n0\n4\n\n\n\n\n# of unique seqs:   4224192\ntotal # of seqs:    36402132\n\nIt took 78 secs to summarize 36402132 sequences.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.summary",
    "crumbs": [
      "Home",
      "16S rRNA",
      "4. OTU Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/otu/index.html#aligning-reads",
    "href": "workflows/ssu/otu/index.html#aligning-reads",
    "title": "4. OTU Workflow",
    "section": "Aligning Reads",
    "text": "Aligning Reads\nSince we are using the silva.nr_v132.align to align sequences, we can check where our reads start and end with the ARB-SILVA web aligner. After uploading a few sequences we find they start at postion 13862 and end at position 23445. Neat. We will pad these numbers to make sure we do not miss anything.\npcr.seqs(fasta=$REF_LOC/silva.seed_v138_2.align, start=13862, end=23445, keepdots=F, processors=$PROC)\npcr.seqs(fasta=reference_dbs/silva.seed_v138_2.align, start=13862, end=23445, keepdots=F, processors=30)\nUsing 30 processors.\n[NOTE]: no sequences were bad, removing silva.nr_v132.bad.accnos\n\nIt took 10 secs to screen 8696 sequences.\n\nOutput File Names: \nsilva.seed_v138_2.pcr.align\nrename.file(input=pipelineFiles/silva.nr_v132.pcr.align, new=pipelineFiles/$ALIGNREF)\nrename.file(input=pipelineFiles/silva.seed_v138_2.pcr.align, new=pipelineFiles/silva.v4.fasta)\n\nsummary.seqs(fasta=pipelineFiles/$ALIGNREF, processors=$PROC)\nsummary.seqs(fasta=pipelineFiles/silva.v4.fasta, processors=30)\n\n\n\n\n\n\nNoteExpand to see data set summary\n\n\n\n\n\n\n\n\nStart\nEnd\nNBases\nAmbigs\nPolymer\nNumSeqs\n\n\n\nMinimum\n1\n8722\n221\n0\n3\n1\n\n\n2.5%-tile:\n1\n9583\n252\n0\n4\n218\n\n\n25%-tile:\n1\n9583\n253\n0\n4\n2175\n\n\nMedian:\n1\n9583\n253\n0\n5\n4349\n\n\n75%-tile:\n1\n9583\n254\n0\n6\n6523\n\n\n97.5%-tile:\n1\n9583\n421\n1\n6\n8479\n\n\nMaximum:\n15\n9584\n1082\n5\n10\n8696\n\n\nMean:\n1\n9582\n288\n0\n4\n\n\n\n\n# of Seqs:  8696\n\nIt took 1 secs to summarize 8696 sequences.\n\nOutput File Names:\nsilva.v4.summary\n\n\n\nalign.seqs(fasta=current, reference=$REF_LOC/$ALIGNREF, processors=$PROC)\nalign.seqs(fasta=shrimp.trim.contigs.good.unique.fasta, reference=pipelineFiles/silva.v4.fasta, processors=30)\nUsing 30 processors.\n\nReading in the silva.v4.fasta template sequences... DONE.\nIt took 34 to read  213119 sequences.\n\nAligning sequences from shrimp.trim.contigs.good.unique.fasta ...\nIt took 2 to read  8696 sequences.\n\n[WARNING]: 854 of your sequences generated alignments that \neliminated too many bases, a list is provided in \nshrimp.trim.contigs.good.unique.flip.accnos.\n[NOTE]: 444 of your sequences were reversed to produce a better alignment.\n\nIt took 780 seconds to align 4224192 sequences.\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.align\nshrimp.trim.contigs.good.unique.align_report\nshrimp.trim.contigs.good.unique.flip.accnos\nsummary.seqs(fasta=current, count=current, processors=$PROC)\nsummary.seqs(fasta=shrimp.trim.contigs.good.unique.align, count=shrimp.trim.contigs.good.count_table, processors=30)\n\n\n\n\n\n\nNoteExpand to see data set summary\n\n\n\n\n\n\n\n\nStart\nEnd\nNBases\nAmbigs\nPolymer\nNumSeqs\n\n\n\nMinimum\n0\n0\n0\n0\n1\n1\n\n\n2.5%-tile:\n1\n9583\n252\n0\n3\n910054\n\n\n25%-tile:\n1\n9583\n253\n0\n4\n9100534\n\n\nMedian:\n1\n9583\n253\n0\n4\n18201067\n\n\n75%-tile:\n1\n9583\n253\n0\n5\n27301600\n\n\n97.5%-tile:\n1\n9583\n253\n0\n6\n35492079\n\n\nMaximum:\n9583\n9584\n254\n0\n6\n36402132\n\n\nMean:\n1\n9581\n252\n0\n4\n\n\n\n\n# of unique seqs:   4224192\ntotal # of seqs:    36402132\n\nIt took 141 secs to summarize 36402132 sequences.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.summary\n\n\n\nscreen.seqs(fasta=current, count=current, start=1968, end=11550, processors=$PROC)\nscreen.seqs(fasta=shrimp.trim.contigs.good.unique.align, count=shrimp.trim.contigs.good.count_table, start=1, end=9583, processors=30)\nUsing 30 processors.\n\nIt took 66 secs to screen 4224192 sequences, removed 21326.\n\n/******************************************/\nRunning command: \nremove.seqs(accnos=shrimp.trim.contigs.good.unique.bad.accnos.temp, \ncount=shrimp.trim.contigs.good.count_table)\nRemoved 79070 sequences from shrimp.trim.contigs.good.count_table.\n\nOutput File Names:\nshrimp.trim.contigs.good.pick.count_table\n\n/******************************************/\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.align\nshrimp.trim.contigs.good.unique.bad.accnos\nshrimp.trim.contigs.good.good.count_table\nsummary.seqs(fasta=current, count=current, processors=$PROC)\nsummary.seqs(fasta=shrimp.trim.contigs.good.unique.good.align, count=shrimp.trim.contigs.good.good.count_table, processors=30)\n\n\n\n\n\n\nNoteExpand to see data set summary\n\n\n\n\n\n\n\n\nStart\nEnd\nNBases\nAmbigs\nPolymer\nNumSeqs\n\n\n\nMinimum\n1\n9583\n208\n0\n3\n1\n\n\n2.5%-tile:\n1\n9583\n252\n0\n3\n908077\n\n\n25%-tile:\n1\n9583\n253\n0\n4\n9080766\n\n\nMedian:\n1\n9583\n253\n0\n4\n18161532\n\n\n75%-tile:\n1\n9583\n253\n0\n5\n27242297\n\n\n97.5%-tile:\n1\n9583\n253\n0\n6\n35414986\n\n\nMaximum:\n1\n9584\n254\n0\n6\n36323062\n\n\nMean:\n1\n9583\n252\n0\n4\n\n\n\n\n# of unique seqs:   4202866\ntotal # of seqs:    36323062\n\nIt took 152 secs to summarize 36323062 sequences.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.summary\n\n\n\ncount.groups(count=current)\ncount.groups(count=shrimp.trim.contigs.good.good.count_table)\nSize of smallest group: 57.\n\nTotal seqs: 36323062.\n\nOutput File Names: \nshrimp.trim.contigs.good.good.count.summary\nfilter.seqs(fasta=current, vertical=T, trump=., processors=$PROC)\nfilter.seqs(fasta=shrimp.trim.contigs.good.unique.good.align, vertical=T, trump=., processors=30)\nUsing 30 processors.\nCreating Filter...\nIt took 31 secs to create filter for 4202866 sequences.\n\n\nRunning Filter...\nIt took 26 secs to filter 4202866 sequences.\n\nLength of filtered alignment: 554\nNumber of columns removed: 9030\nLength of the original alignment: 9584\nNumber of sequences used to construct filter: 4202866\n\nOutput File Names: \nshrimp.filter\nshrimp.trim.contigs.good.unique.good.filter.fasta\nunique.seqs(fasta=current, count=current)\nunique.seqs(fasta=shrimp.trim.contigs.good.unique.good.filter.fasta, count=shrimp.trim.contigs.good.good.count_table)\n4202866 4178668\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.good.filter.unique.fasta\nshrimp.trim.contigs.good.unique.good.filter.count_table\nsummary.seqs(fasta=current, count=current, processors=$PROC)\nsummary.seqs(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.fasta, count=shrimp.trim.contigs.good.unique.good.filter.count_table, processors=30)\n\n\n\n\n\n\nNoteExpand to see data set summary\n\n\n\n\n\n\n\n\nStart\nEnd\nNBases\nAmbigs\nPolymer\nNumSeqs\n\n\n\nMinimum\n1\n554\n208\n0\n3\n1\n\n\n2.5%-tile:\n1\n554\n252\n0\n3\n908077\n\n\n25%-tile:\n1\n554\n253\n0\n4\n9080766\n\n\nMedian:\n1\n554\n253\n0\n4\n18161532\n\n\n75%-tile:\n1\n554\n253\n0\n5\n27242297\n\n\n97.5%-tile:\n1\n554\n253\n0\n6\n35414986\n\n\nMaximum:\n1\n554\n254\n0\n6\n36323062\n\n\nMean:\n1\n554\n252\n0\n4\n\n\n\n\n# of unique seqs:   4178668\ntotal # of seqs:    36323062\n\nIt took 10 secs to summarize 36323062 sequences.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.summary\n\n\n\nWe now have a fully aligned and curated dataset that we can now pass off to the MED pipeline.\nsystem(cp pipelineFiles/shrimp.trim.contigs.good.unique.good.filter.unique.fasta pipelineFiles_med/)\nsystem(cp pipelineFiles/shrimp.trim.contigs.good.unique.good.filter.count_table pipelineFiles_med/)\nMoving on, to the next step of the mothur OTU pipeline.",
    "crumbs": [
      "Home",
      "16S rRNA",
      "4. OTU Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/otu/index.html#precluster",
    "href": "workflows/ssu/otu/index.html#precluster",
    "title": "4. OTU Workflow",
    "section": "Precluster",
    "text": "Precluster\npre.cluster(fasta=current, count=current, diffs=2, processors=$PROC)\n\n\n\n\n\n\nNoteExpand to see partial output of pre.cluster\n\n\n\n\n\nUsing 30 processors.\n\n/******************************************/\nSplitting by sample: \n\nUsing 30 processors.\n\nSelecting sequences for groups Control_1-Control_10-Control_11-Control_12\n-Control_13-Control_14-Control_15-Control_16-Control_17-Control_18\n\nSelected 828 sequences from WA_A_FORM_EG_7457.\nSelected 2958 sequences from WA_A_FORM_EG_7752.\nSelected 5589 sequences from WA_A_FORM_EG_9400.\nSelected 4632 sequences from WA_A_FORM_GL_7402.\nSelected 3860 sequences from WA_A_FORM_GL_7403.\nSelected 5518 sequences from WA_A_FORM_GL_7455.\n\n/******************************************/\n\nDeconvoluting count table results...\nIt took 17 secs to merge 2465292 sequences group data.\n/******************************************/\nRunning get.seqs: \nSelected 1675449 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.fasta.\n/******************************************/\nIt took 8102 secs to run pre.cluster.\n\nUsing 3 processors.\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.fasta\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table\n\n\n\n\nsummary.seqs(fasta=current, count=current, processors=$PROC)\n\n\n\n\n\n\nNoteExpand to see data set summary\n\n\n\n\n\n\n\n\nStart\nEnd\nNBases\nAmbigs\nPolymer\nNumSeqs\n\n\n\nMinimum\n1\n554\n208\n0\n3\n1\n\n\n2.5%-tile:\n1\n554\n252\n0\n3\n908077\n\n\n25%-tile:\n1\n554\n253\n0\n4\n9080766\n\n\nMedian:\n1\n554\n253\n0\n4\n18161532\n\n\n75%-tile:\n1\n554\n253\n0\n5\n27242297\n\n\n97.5%-tile:\n1\n554\n253\n0\n6\n35414986\n\n\nMaximum:\n1\n554\n254\n0\n6\n36323062\n\n\nMean:\n1\n554\n252\n0\n4\n\n\n\n\n# of unique seqs:   1675449\ntotal # of seqs:    36323062\n\nIt took 35 secs to summarize 36323062 sequences.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.summary\n\n\n\ncount.groups(count=current)\nSize of smallest group: 57.\n\nTotal seqs: 36323062.\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.count.summary",
    "crumbs": [
      "Home",
      "16S rRNA",
      "4. OTU Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/otu/index.html#remove-negative-controls",
    "href": "workflows/ssu/otu/index.html#remove-negative-controls",
    "title": "4. OTU Workflow",
    "section": "Remove Negative Controls",
    "text": "Remove Negative Controls\nAs with the OTU workflow, we remove NC samples, but in this case we skip the pre.cluster step. Our goal here to remove all NC samples from the dataset, as well as any reads that are predominantly found in NC samples. More on what that means in a minute. Before we do anything we need to identify all reads present in NC samples.\nHere is what we are going to do:\n\nSubset the NC samples (and associated reads) from the fasta and count.table. To do this in mothur we need all of the NC sample names collected in an .accnos file, which is a text file used in mothur that contains a single column of names–these can be sequences, OTU labels, or sample names. This list is used to select or remove specific items from other mothur files. That way we can process subsets of the data without having to retype long lists of names.\n\nTo generate the .accnos file of NC samples we can use the shrimp.files file generated at the beginning of the mothur pipeline.\n\ntmp_accnos &lt;- readr::read_delim(here(work_here, \"nc_screen/shrimp.files\"), \n                                delim = \"\\t\", col_names = FALSE)\ntmp_accnos[, 2:3] &lt;- NULL\ntmp_accnos &lt;- tmp_accnos[grepl(\"Control_\", tmp_accnos$X1), ]\nreadr::write_delim(tmp_accnos, file = here(work_here, \"nc_screen/nc_samples.accnos\"), \n                   col_names = FALSE)\n\n\nNow we have a list of all NC sample names. The mothur command get.groups in conjunction with accnos file allows us to subset the full fasta and count_table\n\n\nget.groups(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.fasta, count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table, accnos=nc_samples.accnos)\nSelected 192842 sequences from your count file.\nSelected 4148 sequences from your fasta file.\n\nOutput File names:\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta\n\n\nNext we rename the new files to something more informative (and shorter).\n\nrename.file(input=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta, new=nc.fasta)\nrename.file(input=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table, new=nc.count_table)\n\nAnd a quick summary of the NC subset.\n\nsummary.seqs(fasta=nc.fasta, count=nc.count_table, processors=$PROC)\n\n\n\n\n\n\nNoteExpand to see negative control summary\n\n\n\n\n\n\n\n\nStart\nEnd\nNBases\nAmbigs\nPolymer\nNumSeqs\n\n\n\nMinimum\n1\n554\n248\n0\n3\n1\n\n\n2.5%-tile:\n1\n554\n252\n0\n4\n4822\n\n\n25%-tile:\n1\n554\n253\n0\n4\n48211\n\n\nMedian:\n1\n554\n253\n0\n5\n96422\n\n\n75%-tile:\n1\n554\n253\n0\n5\n144632\n\n\n97.5%-tile:\n1\n554\n254\n0\n6\n188021\n\n\nMaximum:\n1\n554\n254\n0\n6\n192842\n\n\nMean:\n1\n554\n253\n0\n4\n\n\n\n\n# of unique seqs:   4148\ntotal # of seqs:    192842\n\nIt took 0 secs to summarize 192842 sequences.\n\nOutput File Names:\nneg_control.summary\n\n\n\n\nSweet. We use the command list.seqs to get a complete list of all repseq names in the NC subset.\n\nlist.seqs(count=nc.count_table)\nOutput File Names: \nnc.accnos\nThis gives us all repseq IDs in the NC samples.\n\nWe could simply use the nc.accnos file from the list.seqs command to remove repseqs found in negative control (NC) samples from the main dataset. This seems reasonable enough. Except mothur will remove all repseqs found in a NC sample, in other words, any repseq in the accnos file. For example, let’s consider the following scenario where we have two repseqs:\n\nrepseq01 is abundant in many NC samples but not found in any other samples.repseq02 on the other hand is represented by say one read in a single NC sample but very abundant in other samples.\nIt makes sense to remove repseq01 but not necessarily repseq02. Essentially, for each repseq in the nc.accnos file we want to calculate:\n\nThe total number of reads in NC samples.\n\nThe total number of reads in non-NC samples.\n\nThe percent of reads in the NC samples.\nThe total number of NC samples containing at least 1 read.\n\nThe total number of non-NC samples containing at least 1 read.\n\nThe percent of NC samples containing reads.\n\nWhere a final data table might look something like this\n\n\n\n\n\n\n\n\n\n\n\nrepseq\nrc_nc\nrc_samps\n%in_nc\nnc_samp\nno_nc_samp\n%_in_nc_samp\n\n\n\nrepseq001\n3\n5\n37.5\n1\n2\n33.31\n\n\nrepseq002\n196\n308\n38.9\n17\n38\n30.7\n\n\nrepseq003\n3\n23\n11.1\n3\n18\n14.5\n\n\n\nTo accomplish this we will parse out relevant data from the .count_table files. We got the idea on how best to do this from a discussion on the mothur forum.\nTo save space and minimize file size, mothur formats the .count_table using a sparse format by storing only non zero sample counts. However, we need the full format which lists each repseq and its abundance counts for each sample. Using the command count.seqs in conjunction with the .count_table will return a full format table.\ncount.seqs(count=nc.count_table, compress=f)\nOutput File Names:\nnc.full.count_table\nThen we use the accnos file (nc.accnos)–containing repseqs found in NC samples, to generate a subset of the complete dataset containing only the reseqs found in NC samples.\nget.seqs(accnos=nc.accnos, fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.fasta, count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table)\nSelected 4148 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.precluster.fasta.\nSelected 15029155 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table\nrename.file(input=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta, new=subset.fasta)\nrename.file(input=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table, new=subset.count_table)\nAnd again run count.seqs to get a full format table of the complete dataset, which we subsetted earlier to contain only repseqs found in NC samples.\ncount.seqs(count=subset.count_table, compress=f)\nOutput File Names:\nsubset.full.count_table\nFinally we can parse out read count data from the two subset.full.count_table files.\n\nfull_count_tab &lt;- readr::read_delim(here(work_here, \"nc_screen/subset.full.count_table\"), \n                                    delim = \"\\t\", col_names = TRUE)\n# figure out which columns to use\ncontrol_cols     &lt;- grep(\"^Control_\", names(full_count_tab), value = TRUE)\nnoncontrol_cols  &lt;- setdiff(names(full_count_tab)[-(1:2)], control_cols)\n\n# now do the rowwise sums\nread_totals &lt;- full_count_tab %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    total_reads_nc   = sum(c_across(all_of(control_cols)), na.rm = TRUE),\n    total_reads_non_nc = sum(c_across(all_of(noncontrol_cols)), na.rm = TRUE)\n  ) %&gt;%\n  ungroup() %&gt;%\n  select(1, 2, total_reads_nc, total_reads_non_nc)\n\nread_totals &lt;- read_totals %&gt;% dplyr::rename(\"total_reads\" = 2)\n\nAnd here is what the new dataframe looks like. Three columns where the first is the repseq name, the second the total number of reads in NC samples, and the third the total number of reads in the entire dataset (this includes NC samples).\n\n\n\n-------------------------------------------------------------------------------------------------\n           Representative_Sequence             total_reads   total_reads_nc   total_reads_non_nc \n--------------------------------------------- ------------- ---------------- --------------------\n M06508_12_000000000-CJG44_1_1101_16846_2451      51955          31810              20145        \n\n M06508_18_000000000-CNPPR_1_1101_15534_2103     304220            2                304218       \n\n M06508_12_000000000-CJG44_1_1101_15015_3135      9474            900                8574        \n\n M06508_12_000000000-CJG44_1_1101_15072_3643      39666           4326              35340        \n\n M06508_9_000000000-JTBW3_1_1106_8860_16108      509857            2                509855       \n\n M06508_9_000000000-JTBW3_1_1106_6953_16246      997443            9                997434       \n-------------------------------------------------------------------------------------------------\n\n\nIn total there are 4148 repseqs that were potential contaminants.\nNow we add in a column that calculates the percent of reads in the NC samples.\n\ntmp_read_totals &lt;- read_totals %&gt;%\n  dplyr::mutate(perc_reads_in_nc = 100*(\n    total_reads_nc / (total_reads_nc + total_reads_non_nc)),\n                .after = \"total_reads_non_nc\")\ntmp_read_totals$perc_reads_in_nc &lt;- \n  round(tmp_read_totals$perc_reads_in_nc, digits = 6)\n\nAnd then we calculate row sums to get the number of NC and non-NC samples containing these reads. For this we can use the subset.full.count_table we read in above.\n\ncontrol_cols     &lt;- grep(\"^Control_\", names(full_count_tab), value = TRUE)\nnoncontrol_cols  &lt;- setdiff(names(full_count_tab)[-(1:2)], control_cols)\n# rowwise tally of non-zero columns\nsamp_totals &lt;- full_count_tab %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    num_nc_samp     = sum(c_across(all_of(control_cols)) != 0, na.rm = TRUE),\n    num_non_nc_samp = sum(c_across(all_of(noncontrol_cols)) != 0, na.rm = TRUE)\n  ) %&gt;%\n  ungroup() %&gt;%\n  select(1, num_nc_samp, num_non_nc_samp)\n\nFinally add a column with the total number of samples and calculate the percent of NC samples containing these reads.\n\nsamp_totals$total_samp &lt;- samp_totals$num_nc_samp + samp_totals$num_non_nc_samp\nsamp_totals &lt;- samp_totals %&gt;%  \n  dplyr::relocate(\"total_samp\", .after = \"Representative_Sequence\")\nsamp_totals &lt;- samp_totals %&gt;%\n  dplyr::mutate(perc_nc_samp = \n                  100*( num_nc_samp / (num_nc_samp + num_non_nc_samp)),\n                  .after = \"num_non_nc_samp\")\n\nAfter all of this wrangling here is a snippet of the table showing the distribution of NC reads across samples.\n\n\n\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n           Representative_Sequence             total_reads   total_reads_nc   total_reads_non_nc   perc_reads_in_nc   total_samp   num_nc_samp   num_non_nc_samp   perc_nc_samp \n--------------------------------------------- ------------- ---------------- -------------------- ------------------ ------------ ------------- ----------------- --------------\n M06508_12_000000000-CJG44_1_1101_16846_2451      51955          31810              20145               61.23            641           41              600            6.396     \n\n M06508_18_000000000-CNPPR_1_1101_15534_2103     304220            2                304218             0.000657          1099           2             1097            0.182     \n\n M06508_12_000000000-CJG44_1_1101_15015_3135      9474            900                8574                9.5             386            8              378            2.073     \n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n\nNow we remove any repseqs where:\n\nThe number of reads found in NC samples accounted for more than 10% of total reads OR\nThe percent of NC samples containing the repseq was greater than 10% of total samples.\n\n\nnc_remove &lt;- nc_check %&gt;% \n  filter(perc_reads_in_nc &gt; 10 | perc_nc_samp &gt; 10)\n\n\n\n\n\n\n\n\n\n\n\nTotal rep seqs\nNC reads\nnon NC reads\n% NC reads\n\n\n\nRemoved\n3886\n156935\n218220\n41.832\n\n\nRetained\n262\n35907\n14618093\n0.245\n\n\n\nWe identified a total of 4148 representative sequences (repseqs) that were present in at least 1 NC sample by at least 1 read. We removed any repseq where more than 10% of total reads were found in NC samples OR any repseq found in more than 10% of NC samples. Based on these criteria we removed 3886 repseqs from the data set, which accounted for 156935 total reads in NC samples and 218220 total reads in non-NC samples. Of the total reads removed 41.832% came from NC samples. Of all repseqs identified in NC samples, 262 were retained because they fell below the threshold criteria. These repseqs accounted for 35907 reads in NC samples and 14618093 reads in non-NC samples. NC samples accounted for 0.245% of these reads.\nOK, now we can create a new neg_control.accnos containing only repseqs abundant in NC samples.\n\nwrite_delim(\n  data.frame(nc_remove$Representative_Sequence), \n  here(work_here, \"nc_screen/nc_repseq_remove.accnos\"), \n  col_names = FALSE)\n\nAnd then use this file in conjunction with the mothur command remove.seqs.\nremove.seqs(accnos=nc_repseq_remove.accnos, fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.fasta, count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table)\nRemoved 3886 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.precluster.fasta.\nRemoved 375155 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table\n\ncount.groups(count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table)\nSize of smallest group: 1.\n\nTotal seqs: 35947907.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count.summary\nBefore we remove the NC samples we need to check whether some NC samples were already removed. When mothur runs the remove.seqs command it will automatically remove any samples where the read count has fallen to zero. If mothur did remove samples and we try to remove all NC samples, we will get an error. To check we can compare the count.summary files before and after the previous remove.seqs command.\n\ntmp_before &lt;- read_tsv(\n  here(work_here, \n       \"nc_screen/shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count.summary\"),\n  col_names = FALSE,\n  col_select = 1\n)\n\ntmp_after &lt;- read_tsv(\n  here(work_here, \n       \"nc_screen/shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count.summary\"),\n  col_names = FALSE,\n  col_select = 1\n)\ntmp_nc_lost &lt;- anti_join(tmp_before, tmp_after)\ntmp_nc_lost$X1\n\nThese are the samples that were removed when we ran remove.seqs. We need to eliminate these sample IDs from our list of NC samples to remove or mothur will throw an error.\n[1] \"Control_15\" \"Control_18\" \"Control_21\" \"Control_29\" \"Control_5\"   \nAs before, we can generate a list of NC samples to use in conjunction with the remove.groups command to eliminate all NC samples.\n\nnc_to_remove &lt;- semi_join(tmp_before, tmp_after)\nnc_to_remove &lt;- nc_to_remove %&gt;%\n  dplyr::filter(\n    stringr::str_starts(X1, \"Control\")\n    )\nreadr::write_delim(nc_to_remove, \n                   file = here(work_here, \"nc_screen/nc_samples_remove.accnos\"), \n                   col_names = FALSE)\n\nIn total the following mothur command should remove 55 negative control samples.\nremove.groups(count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table, fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta, accnos=nc_samples_remove.accnos)\nRemoved 35907 sequences from your count file.\nRemoved 0 sequences from your fasta file.\n\nOutput File names:\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.count_table\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.fasta\nsummary.seqs(fasta=current, count=current, processors=30)\n\n\n\n\n\n\nNoteExpand to see data set summary\n\n\n\n\n\n\n\n\nStart\nEnd\nNBases\nAmbigs\nPolymer\nNumSeqs\n\n\n\nMinimum\n1\n554\n208\n0\n3\n1\n\n\n2.5%-tile:\n1\n554\n252\n0\n3\n897801\n\n\n25%-tile:\n1\n554\n253\n0\n4\n8978001\n\n\nMedian:\n1\n554\n253\n0\n4\n17956001\n\n\n75%-tile:\n1\n554\n253\n0\n5\n26934001\n\n\n97.5%-tile:\n1\n554\n253\n0\n6\n35014201\n\n\nMaximum:\n1\n554\n254\n0\n6\n35912000\n\n\nMean:\n1\n554\n252\n0\n4\n\n\n\n\n# of unique seqs:   1671563\ntotal # of seqs:    35912000\n\nIt took 74 secs to summarize 35912000 sequences.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.summary\n\n\n\ncount.groups(count=current)\nSize of smallest group: 14.\n\nTotal seqs: 35912000.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.count.summary",
    "crumbs": [
      "Home",
      "16S rRNA",
      "4. OTU Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/otu/index.html#remove-chimeras",
    "href": "workflows/ssu/otu/index.html#remove-chimeras",
    "title": "4. OTU Workflow",
    "section": "Remove Chimeras",
    "text": "Remove Chimeras\nchimera.vsearch(fasta=current, count=current, dereplicate=t, processors=$PROC)\nUsing vsearch version v2.30.0.\nChecking sequences from shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.fasta ...\n/******************************************/\nSplitting by sample: \n\n...\n\nRemoving chimeras from your input files:\n/******************************************/\nRunning command: remove.seqs(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.fasta, accnos=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.accnos)\nRemoved 619952 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.fasta.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.pick.fasta\n\n/******************************************/\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.count_table\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.chimeras\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.accnos\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.fasta\nsummary.seqs(fasta=current, count=current, processors=30)\n\n\n\n\n\n\nNoteExpand to see data set summary\n\n\n\n\n\n\n\n\nStart\nEnd\nNBases\nAmbigs\nPolymer\nNumSeqs\n\n\n\nMinimum\n1\n554\n208\n0\n3\n1\n\n\n2.5%-tile:\n1\n554\n252\n0\n3\n863358\n\n\n25%-tile:\n1\n554\n253\n0\n4\n8633575\n\n\nMedian:\n1\n554\n253\n0\n4\n17267149\n\n\n75%-tile:\n1\n554\n253\n0\n5\n25900723\n\n\n97.5%-tile:\n1\n554\n253\n0\n6\n33670940\n\n\nMaximum:\n1\n554\n254\n0\n6\n34534297\n\n\nMean:\n1\n554\n252\n0\n4\n\n\n\n\n# of unique seqs:   1051611\ntotal # of seqs:    34534297\n\nIt took 22 secs to summarize 34534297 sequences.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.summary\n\n\n\ncount.groups(count=current)\nSize of smallest group: 14.\n\nTotal seqs: 34534297.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.count.summary",
    "crumbs": [
      "Home",
      "16S rRNA",
      "4. OTU Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/otu/index.html#assign-taxonomy",
    "href": "workflows/ssu/otu/index.html#assign-taxonomy",
    "title": "4. OTU Workflow",
    "section": "Assign Taxonomy",
    "text": "Assign Taxonomy\nThe classify.seqs command requires properly formatted reference and taxonomy databases. For taxonomic assignment, we are using the GSR database (Molano, Vega-Abellaneda, and Manichanh 2024). The developers of mothur maintain formatted versions of popular databases, however the GSR-DB has not been formatted by the developers yet.\n\n\n\n\n\n\nNote\n\n\n\nHere can download an appropriate version of the GSR database.\n\n\nTo create a mothur formatted version GSR-DB1, we perform the following steps.\nMake a custom DB\nHere we are using the GSR V4 database.\n\nwget https://manichanh.vhir.org/gsrdb/GSR-DB_V4_cluster-1.tar.gz\ntar -xvzf GSR-DB_V4_cluster-1.tar.gz\n\nFirst (in the command line) we remove first line of the taxonomy file.\n\ncp GSR-DB_V4_cluster-1_taxa.txt tmp0.txt\nsed '1d' tmp0.txt &gt; tmp1.txt\n\nNext, delete species and remove leading [a-z]__ from taxa names\n\nsed -E 's/s__.*//g' tmp1.txt &gt; tmp2.txt\nsed -E 's/[a-zA-Z]__//g' tmp2.txt &gt; gsrdb.tax\ncp GSR-DB_V4_cluster-1_seqs.fasta gsrdb.fasta\n\nclassify.seqs(fasta=current, count=current, reference=reference_dbs/gsrdb.fasta, taxonomy=reference_dbs/gsrdb.tax, processors=3)\nUsing 30 processors.\nGenerating search database...    DONE.\nIt took 2 seconds generate search database.\n\nReading in the reference_dbs/gsrdb.txt taxonomy...  DONE.\nCalculating template taxonomy tree...     DONE.\nCalculating template probabilities...     DONE.\nIt took 6 seconds get probabilities.\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.fasta ...\n[WARNING]: M06508_9_000000000-JTBW3_1_1102_26159_16839 could not be classified. \nYou can use the remove.lineage command with taxon=unknown; \nto remove such sequences.\n...\n\nIt took 348 secs to classify 1051611 sequences.\n\nIt took 503 secs to create the summary file for 1051611 sequences.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.taxonomy\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.tax.summary",
    "crumbs": [
      "Home",
      "16S rRNA",
      "4. OTU Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/otu/index.html#remove-contaminants",
    "href": "workflows/ssu/otu/index.html#remove-contaminants",
    "title": "4. OTU Workflow",
    "section": "Remove Contaminants",
    "text": "Remove Contaminants\nremove.lineage(fasta=current, count=current, taxonomy=current, taxon=$CONTAMINENTS)\nRunning command: \nremove.seqs(accnos=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.accnos, \ncount=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.count_table, \nfasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.fasta)\n\nRemoved 617 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.fasta.\nRemoved 2776 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.count_table.\n\n/******************************************/\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.pick.taxonomy\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.accnos\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.pick.count_table\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.pick.fasta\nsummary.tax(taxonomy=current, count=current)\nUsing shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.pick.count_table \nas input file for the count parameter.\nUsing shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.pick.taxonomy \nas input file for the taxonomy parameter.\n\nIt took 489 secs to create the summary file for 34531521 sequences.\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.pick.tax.summary\ncount.groups(count=current)\nSize of smallest group: 14.\n\nTotal seqs: 34531521.\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.pick.count.summary",
    "crumbs": [
      "Home",
      "16S rRNA",
      "4. OTU Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/otu/index.html#track-reads-through-workflow",
    "href": "workflows/ssu/otu/index.html#track-reads-through-workflow",
    "title": "4. OTU Workflow",
    "section": "Track Reads through Workflow",
    "text": "Track Reads through Workflow\nAt this point we can look at the number of reads that made it through each step of the workflow for every sample.\n\nread_change &lt;- read_tsv(\n  here(work_here, \"mothur_pipeline_read_changes.txt\"),\n  col_names = TRUE\n)",
    "crumbs": [
      "Home",
      "16S rRNA",
      "4. OTU Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/otu/index.html#preparing-for-analysis",
    "href": "workflows/ssu/otu/index.html#preparing-for-analysis",
    "title": "4. OTU Workflow",
    "section": "Preparing for analysis",
    "text": "Preparing for analysis\nrename.file(fasta=current, count=current, taxonomy=current, prefix=final)\nCurrent files saved by mothur:\nfasta=final.fasta\ntaxonomy=final.taxonomy\ncount=final.count_table",
    "crumbs": [
      "Home",
      "16S rRNA",
      "4. OTU Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/otu/index.html#clustering",
    "href": "workflows/ssu/otu/index.html#clustering",
    "title": "4. OTU Workflow",
    "section": "Clustering",
    "text": "Clustering\ncluster.split(fasta=final.fasta, count=final.count_table, taxonomy=final.taxonomy, taxlevel=4, cluster=f, processors=$PROC)\ncluster.split(file=final.file, count=final.count_table, processors=$PROC)\nUsing 30 processors.\nSplitting the file...\n/******************************************/\nSelecting sequences for group Vibrionales (1 of 364)\nNumber of unique sequences: 92783\n\nSelected 5390956 sequences from final.count_table.\n\nCalculating distances for group Vibrionales (1 of 364):\n\nSequence    Time    Num_Dists_Below_Cutoff\n\nIt took 902 secs to find distances for 92783 sequences. \n477552179 distances below cutoff 0.03.\n\nOutput File Names:\nfinal.0.dist\n\n...\n\nIt took 8671 seconds to cluster\nMerging the clustered files...\nIt took 14 seconds to merge.\n[WARNING]: Cannot run sens.spec analysis without a column file, \nskipping.\nOutput File Names: \nfinal.opti_mcc.list\nsystem(mkdir cluster.split.gsrdb) \nsystem(mv final.opti_mcc.list cluster.split.gsrdb/) \nsystem(mv final.file cluster.split.gsrdb/) \nsystem(mv final.dist cluster.split.gsrdb/)\ndist.seqs(fasta=final.fasta, cutoff=0.03, processors=\\$PROC) cluster(column=final.dist, count=final.count_table)\nSequence    Time    Num_Dists_Below_Cutoff\n\nIt took 91935 secs to find distances for 1022766 sequences. \n1096480673 distances below cutoff 0.03.\n\nOutput File Names: \nfinal.dist\n\nYou did not set a cutoff, using 0.03.\n\nClustering final.dist\n\niter    time    label   num_otus    cutoff  tp  tn  fp  fn  sensitivity specificity ppv npv fdr accuracy    mcc f1score\n\n0.03\n0   0   0.03    1022766 0.03    0   5.21928e+11 0   1.09648e+09 0   1   0   0.997904    1   0.997904    0   0   \n1   3187    0.03    130371  0.03    7.80436e+08 5.21829e+11 9.9517e+07  3.16045e+08 0.711764    0.999809    0.886906    0.999395    0.886906    0.999205    0.794146    0.789741    \n2   3706    0.03    119919  0.03    7.82225e+08 5.21828e+11 9.99504e+07 3.14256e+08 0.713396    0.999808    0.8867  0.999398    0.8867  0.999208    0.794965    0.790663    \n3   3712    0.03    119453  0.03    7.82257e+08 5.21828e+11 9.99331e+07 3.14224e+08 0.713425    0.999809    0.886722    0.999398    0.886722    0.999208    0.794991    0.790689    \n\nIt took 21013 seconds to cluster\n\nOutput File Names: \nfinal.opti_mcc.list\nfinal.opti_mcc.steps\nfinal.opti_mcc.sensspec",
    "crumbs": [
      "Home",
      "16S rRNA",
      "4. OTU Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/otu/index.html#getting-files-from-mothur",
    "href": "workflows/ssu/otu/index.html#getting-files-from-mothur",
    "title": "4. OTU Workflow",
    "section": "Getting Files from Mothur",
    "text": "Getting Files from Mothur\nTo create a microtable object we need a a sequence table, taxonomy table, and a sample data table. To generate the sequence table we need a shared file from mothur, which we can generate using the command make.shared. The data in a shared file represent the number of times that an OTU is observed in multiple samples.\nmake.shared(list=final.opti_mcc.list, count=final.count_table, label=0.03)\n0.03\n\nOutput File Names:\nfinal.opti_mcc.shared\nNext we use classify.otu to get the OTU taxonomy table.\nclassify.otu(list=final.opti_mcc.list, count=final.count_table, taxonomy=final.taxonomy, label=0.03)\n0.03\n\nOutput File Names: \nfinal.opti_mcc.0.03.cons.taxonomy\nfinal.opti_mcc.0.03.cons.tax.summary\ncount.groups(shared=final.opti_mcc.shared)\nSize of smallest group: 14.\n\nTotal seqs: 34611554.\n\nOutput File Names: \nfinal.opti_mcc.count.summary",
    "crumbs": [
      "Home",
      "16S rRNA",
      "4. OTU Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/otu/index.html#prep-data-for-microeco",
    "href": "workflows/ssu/otu/index.html#prep-data-for-microeco",
    "title": "4. OTU Workflow",
    "section": "Prep Data for microeco\n",
    "text": "Prep Data for microeco\n\nLike any tool, the microeco package needs the data in a specific form. I formatted our data to match the mock data in the microeco tutorial, specifically this section.\nA. Taxonomy Table\nHere is what the taxonomy table looks like in the mock data.\n\npandoc.table(taxonomy_table_16S[10:12, 1:3], emphasize.rownames = FALSE)\n\nOur taxonomy file (below) needs a little wrangling to be properly formatted.\n\ntmp_tax &lt;- read_delim(here(work_here, \"final.opti_mcc.0.03.cons.taxonomy\"), \n                      delim = \"\\t\")\n\n\n\n\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    OTU       Size                                                                                      Taxonomy                                                                                    \n----------- -------- -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n Otu000011   460531                           Bacteria(100);Proteobacteria(100);Gammaproteobacteria(100);Vibrionales(100);Vibrionaceae(100);Vibrionaceae_unclassified(84);                          \n\n Otu000012   445273   Bacteria(100);Proteobacteria(100);Alphaproteobacteria(100);Alphaproteobacteria_unclassified(100);Alphaproteobacteria_unclassified(100);Alphaproteobacteria_unclassified(100); \n\n Otu000013   369620               Bacteria(100);Bacteria_unclassified(100);Bacteria_unclassified(100);Bacteria_unclassified(100);Bacteria_unclassified(100);Bacteria_unclassified(100);             \n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n\nSome fancy string manipulation…\n\ntmp_tax &lt;- data.frame(sapply(tmp_tax, \n                             gsub, \n                             pattern = \"\\\\(\\\\d+\\\\)\", \n                             replacement = \"\"))\ntmp_tax &lt;- data.frame(sapply(tmp_tax, \n                             gsub, \n                             pattern = \";$\", \n                             replacement = \"\"))\ntmp_tax &lt;- separate_wider_delim(tmp_tax, \n                              cols = Taxonomy, \n                              delim = \";\", names = c(\n                                \"Kingdom\", \"Phylum\", \n                                \"Class\", \"Order\", \n                                \"Family\", \"Genus\" \n                                )\n                              )\ntmp_tax &lt;- data.frame(sapply(tmp_tax, gsub, \n                           pattern = \"^.*_unclassified$\", \n                           replacement = \"\"))\ntmp_tax$Size &lt;- NULL\ntmp_tax &lt;- tibble::column_to_rownames(tmp_tax, \"OTU\")\n\nAnd we get this …\n\n\n\n-------------------------------------------------------------\n  &nbsp;     Kingdom        Phylum              Class        \n----------- ---------- ---------------- ---------------------\n Otu000011   Bacteria   Proteobacteria   Gammaproteobacteria \n\n Otu000012   Bacteria   Proteobacteria   Alphaproteobacteria \n\n Otu000013   Bacteria                                        \n-------------------------------------------------------------\n\n\n\nrank_prefixes &lt;- c(\n  Kingdom = \"k\", \n  Phylum  = \"p\", \n  Class   = \"c\", \n  Order   = \"o\", \n  Family  = \"f\", \n  Genus   = \"g\"\n)\n\ntmp_tax &lt;- tmp_tax %&gt;%\n  mutate(across(everything(), ~replace_na(., \"\"))) %&gt;% \n  mutate(across(names(rank_prefixes), \n                ~ paste0(rank_prefixes[cur_column()], \"__\", .))) %&gt;%\ntidy_taxonomy()\n\nAnd then this. Excatly like the mock data.\n\n\n\n----------------------------------------------------------------------\n  &nbsp;       Kingdom          Phylum                 Class          \n----------- ------------- ------------------- ------------------------\n Otu000011   k__Bacteria   p__Proteobacteria   c__Gammaproteobacteria \n\n Otu000012   k__Bacteria   p__Proteobacteria   c__Alphaproteobacteria \n\n Otu000013   k__Bacteria          p__                   c__           \n----------------------------------------------------------------------\n\n\nB. Sequence Table\nHere is what the sequence table looks like in the mock data.\nHere is what the sequence table looks like in the mock data.\n\n\n\n--------------------------------------------------------------------\n  &nbsp;    S1   S2   S3   S4   S5   S6   S7   S9   S10   S11   S12 \n---------- ---- ---- ---- ---- ---- ---- ---- ---- ----- ----- -----\n OTU_4272   1    0    1    1    0    0    1    1     0     1     1  \n\n OTU_236    1    4    0    2    35   5    94   0    177   14    27  \n\n OTU_399    9    2    2    4    4    0    3    6     0     1     2  \n--------------------------------------------------------------------\n\n\nThese code block will return a properly formatted sequence table.\n\ntmp_st &lt;- readr::read_delim(\n  here(work_here, \"final.opti_mcc.shared\"),  delim = \"\\t\")\n\n\ntmp_st$numOtus &lt;- NULL\ntmp_st$label &lt;- NULL\ntmp_st &lt;- tmp_st %&gt;%\n  tidyr::pivot_longer(cols = c(-1), names_to = \"tmp\") %&gt;%\n  tidyr::pivot_wider(names_from = c(1))\n\ntmp_st &lt;- tibble::column_to_rownames(tmp_st, \"tmp\")\n\n\n\n\n-----------------------------------------------------------------------\n  &nbsp;     EP_A_AREN_EG_8651   EP_A_AREN_EG_8654   EP_A_AREN_EG_8698 \n----------- ------------------- ------------------- -------------------\n Otu000001          16                 1598                 49         \n\n Otu000002           0                   0                   0         \n\n Otu000003           0                   4                   1         \n\n Otu000004           0                   1                   0         \n\n Otu000005           1                  216                1330        \n-----------------------------------------------------------------------\n\n\n\n# only need to run this if reading in processed files\n# code adds a tab to the beginning of first line\nsed '1s/^/\\t/' tmp_final.opti_mcc.fixed.shared &gt; final.opti_mcc.fixed.shared\n\nC. Sample Table\n\n\n\n----------------------------------------------------\n &nbsp;   SampleID   Group   Type       Saline      \n-------- ---------- ------- ------ -----------------\n   S1        S1       IW      NE    Non-saline soil \n\n   S2        S2       IW      NE    Non-saline soil \n\n   S3        S3       IW      NE    Non-saline soil \n----------------------------------------------------\n\n\nNo problem.\n\nsamdf &lt;- readRDS(here(\"working_files/ssu/sampledata\", \"sample_data.rds\"))\n\nsamdf &lt;- samdf %&gt;% tibble::column_to_rownames(\"SampleID\")\nsamdf$SampleID &lt;- rownames(samdf)\nsamdf &lt;- samdf %&gt;% dplyr::relocate(SampleID)\n\nsamdf &lt;- samdf %&gt;%\n  dplyr::filter(\n    stringr::str_starts(SampleID, \"Control\", negate = TRUE)\n    )\n\n\n\n\n------------------------------------------------------------------\n      &nbsp;             SampleID        OCEAN   SPECIES   TISSUE \n------------------- ------------------- ------- --------- --------\n EP_A_AREN_EG_8651   EP_A_AREN_EG_8651    EP     A_AREN      EG   \n\n EP_A_AREN_EG_8654   EP_A_AREN_EG_8654    EP     A_AREN      EG   \n\n EP_A_AREN_EG_8698   EP_A_AREN_EG_8698    EP     A_AREN      EG   \n------------------------------------------------------------------",
    "crumbs": [
      "Home",
      "16S rRNA",
      "4. OTU Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/otu/index.html#experiment-level-objects",
    "href": "workflows/ssu/otu/index.html#experiment-level-objects",
    "title": "4. OTU Workflow",
    "section": "Experiment-level Objects",
    "text": "Experiment-level Objects\nIn the following section we create microtable and phyloseq objects. These are single, consolidated experiment-level data objects that organize various data types, such as OTU tables (taxa abundance), sample metadata, taxonomic classifications, as well as phylogenetic trees and reference sequences, into one structured format. This unified approach facilitates reproducible, interactive analysis and visualization by bringing all related information together.\nWe begin by creating the microtable and then use the function meco2phyloseq from the file2meco package to create the phyloseq object. This way all of the underlying data is identical across the two objects.\n\n\n\n\n\n\nNote\n\n\n\nThese objects contain an OTU table (taxa abundances), sample metadata, and taxonomy table (mapping between OTUs and higher-level taxonomic classifications).\n\n\n\nsample_info &lt;- samdf\ntax_tab &lt;- tmp_tax\notu_tab &lt;- tmp_st\n\n\ntmp_me &lt;- microtable$new(sample_table = sample_info, \n                         otu_table = otu_tab, \n                         tax_table = tax_tab)\ntmp_me\n\nmicrotable-class object:\nsample_table have 1849 rows and 13 columns\notu_table have 119453 rows and 1849 columns\ntax_table have 119453 rows and 6 columns\nD. Add Representative Sequence\nWe can also add representative sequences for each OTU/ASV. For this step, we use the mothur command get.oturep.\nget.oturep(column=final.dist, list=final.opti_mcc.list, count=final.count_table, fasta=final.fasta)\nThe fasta file it returns needs a little T.L.C.\nYou did not provide a label, using 0.03.\n0.03    119453\n\nOutput File Names: \nfinal.opti_mcc.0.03.rep.count_table\nfinal.opti_mcc.0.03.rep.fasta\nFor that we use a tool called SeqKit (Shen, Sipos, and Zhao 2024) for fasta defline manipulation.\n\nseqkit replace -p \"\\|.*\" -r '' final.opti_mcc.0.03.rep.fasta &gt; tmp2.fa\nseqkit replace -p \".*\\\\t\" -r '' tmp2.fa &gt; tmp3.fa\nseqkit replace -p \"-\" -r '$1' -s -w 0 tmp3.fa &gt; otu_reps.fasta\nrm tmp*\n\n\nrep_fasta &lt;- Biostrings::readDNAStringSet(here(work_here, \"otu_reps.fasta\"))\ntmp_me$rep_fasta &lt;- rep_fasta\ntmp_me$tidy_dataset()\ntmp_me\n\nmicrotable-class object:\nsample_table have 1849 rows and 14 columns\notu_table have 119453 rows and 1849 columns\ntax_table have 119453 rows and 6 columns\nrep_fasta have 119453 sequence",
    "crumbs": [
      "Home",
      "16S rRNA",
      "4. OTU Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/otu/index.html#curate-the-data-set",
    "href": "workflows/ssu/otu/index.html#curate-the-data-set",
    "title": "4. OTU Workflow",
    "section": "Curate the Data Set",
    "text": "Curate the Data Set\nPretty much the last thing to do is remove low-count samples.\nRemove Low-Count Samples\n\nthreshold &lt;- 1000\ntmp_no_low &lt;- microeco::clone(me_raw)\ntmp_no_low$otu_table &lt;- me_raw$otu_table %&gt;%\n          dplyr::select(where(~ is.numeric(.) && sum(.) &gt;= threshold))\ntmp_no_low$tidy_dataset()\ntmp_no_low\n\n41 taxa with 0 abundance are removed from the otu_table ...\n\n\nmicrotable-class object:\nsample_table have 1838 rows and 14 columns\notu_table have 119412 rows and 1838 columns\ntax_table have 119412 rows and 6 columns\nrep_fasta have 119412 sequences\n\n\n\nme_final &lt;- microeco::clone(tmp_no_low)\n\nLastly, we can use the package file2meco to generate a phyloseq object.\n\nps_final &lt;- file2meco::meco2phyloseq(me_final)",
    "crumbs": [
      "Home",
      "16S rRNA",
      "4. OTU Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/otu/index.html#summary",
    "href": "workflows/ssu/otu/index.html#summary",
    "title": "4. OTU Workflow",
    "section": "Summary",
    "text": "Summary\nNow time to summarize the data. For this we use the R package miaverse (Borman et al. 2024).\n\n\n\n\nme_dataset\ntotal_asvs\ntotal_reads\ntotal_samples\n\n\n\noriginal\n119453\n34611554\n1849\n\n\nfinal\n119412\n34606505\n1838\n\n\n\n\n\n\n\n\nDataset metrics before and after curation.\n\nMetric\nStart\nEnd\n\n\n\nMin. no. of reads\n14\n1160\n\n\nMax. no. of reads\n244661\n244661\n\n\nTotal no. of reads\n34611554\n34606505\n\n\nAvg. no. of reads\n18719\n18828\n\n\nMedian no. of reads\n14834\n14885\n\n\nTotal ASVs\n119453\n119412\n\n\nNo. of singleton ASVs\n71988\n71959\n\n\n% of singleton ASVs\n60.265\n60.261\n\n\nSparsity\n0.996\n0.996\n\n\n\n\n\nWe started off with 119453 ASVs and 1849 samples. After removing low-count samples, there were 119412 ASVs and 1838 samples remaining.\nWe lost a total of 71 samples after curating the dataset. This includes 60 NC samples and 11 non-NC samples.\nHere is a list of non-NC samples that were removed.\n\n\n [1] \"WA_A_BAHA_GL_7474\" \"WA_A_CRIS_GL_9624\" \"WA_A_CRIS_HP_7536\"\n [4] \"WA_A_FLOR_GL_9659\" \"WA_A_NUTT_EG_9410\" \"WA_A_PARA_GL_9474\"\n [7] \"WA_A_PARA_MG_9364\" \"WA_A_PARA_ST_9477\" \"WA_A_PCNS_MG_9513\"\n[10] \"WA_A_THOM_ST_9389\" \"WA_A_WEBS_HP_9411\"",
    "crumbs": [
      "Home",
      "16S rRNA",
      "4. OTU Workflow"
    ]
  },
  {
    "objectID": "workflows/ssu/otu/index.html#footnotes",
    "href": "workflows/ssu/otu/index.html#footnotes",
    "title": "4. OTU Workflow",
    "section": "Footnotes",
    "text": "Footnotes\n\nFrom the developers: GSR database (Greengenes, SILVA, and RDP database) is an integrated and manually curated database for bacterial and archaeal 16S amplicon taxonomy analysis. Unlike previous integration approaches, this database creation pipeline includes a taxonomy unification step to ensure consistency in taxonomical annotations. The database was validated with three mock communities and two real datasets and compared with existing 16S databases such as Greengenes, GTDB, ITGDB, SILVA, RDP, and MetaSquare. Results showed that the GSR database enhances taxonomical annotations of 16S sequences, outperforming current 16S databases at the species level. The GSR database is available for full-length 16S sequences and the most commonly used hypervariable regions: V4, V1-V3, V3-V4, and V3-V5.↩︎",
    "crumbs": [
      "Home",
      "16S rRNA",
      "4. OTU Workflow"
    ]
  }
]