::: {.callout-note appearance="simple" collapse="true"}

### Expand to see environment variables

We are running mothur in [Batch Mode](https://mothur.org/wiki/batch_mode/) and using Environment Variables to generalize the batch commands for reuse. The format of environmental variables is `[tag]=[value]`. mothur will automatically pull in the systems environment variable, which we can set in bash and then run the batch file:

```         
$ export DATA=01_TRIMMED_DATA/
$ export TYPE=gz
$ export PROC=30

$ export REF_LOC=reference_dbs
$ export TAXREF_FASTA=gsrdb.fasta
$ export TAXREF_TAX=gsrdb.tax
$ export ALIGNREF=silva.v4.fasta

$ export CONTAMINENTS=Chloroplast-Mitochondria-unknown-Eukaryota
```
:::

```{r}
#| echo: false
#| eval: true
load(here("page_build", "otu_part1.rdata"))
```

## Getting Started

::: {.callout-note appearance="simple" collapse="true"}

### Expand for the MOTHUR batchfile
{{< include include/_mothur_batchfile.qmd >}}

:::

```{verbatim}
set.dir(output=pipelineFiles/)
```

```         
Mothur's directories:
outputDir=pipelineFiles/
```

```{verbatim}
make.file(inputdir=$DATA, type=$TYPE, prefix=shrimp)
# OR
make.file(inputdir=01_TRIMMED_DATA/, type=gz, prefix=shrimp)
```

```         
Setting input directories to: 
    01_TRIMMED_DATA/

Output File Names: 
shrimp.files
```

## Reducing Sequencing & PCR Errors

```{verbatim}
make.contigs(file=current, processors=$PROC)
# OR
make.contigs(file=shrimp.files, processors=30)
```

We will get the following message if sample names contain dash (`-`) characters. Mothur will change this for us.

```         
[WARNING]: group Control-10 contains illegal characters in the name. 
Group names should not include :, -, or / characters.  The ':' character 
is a special character used in trees. Using ':' will result in your tree 
being unreadable by tree reading software.  The '-' character is a special 
character used by mothur to parse group names.  Using the '-' character 
will prevent you from selecting groups. The '/' character will created 
unreadable filenames when mothur includes the group in an output filename.

[NOTE] Updating Control-10 to Control_10 to avoid downstream issues.

...

Total of all groups is 44710450

It took 1257 secs to process 44710450 sequences.

Output File Names: 
shrimp.trim.contigs.fasta
shrimp.scrap.contigs.fasta
shrimp.contigs_report
shrimp.contigs.count_table
```

```{verbatim}
summary.seqs(fasta=current, count=current, processors=$PROC)
summary.seqs(fasta=shrimp.trim.contigs.fasta, count=shrimp.contigs.count_table, processors=30)
```

::: {.callout-note appearance="simple" collapse="true"}

### Expand to see data set summary

|             | Start | End | NBases | Ambigs | Polymer | NumSeqs  |
|:------------|:-----:|:---:|:------:|:------:|:-------:|:--------:|
| Minimum     |   1   | 226 |  226   |   0    |    2    |    1     |
| 2.5%-tile:  |   1   | 252 |  252   |   0    |    3    | 1117762  |
| 25%-tile:   |   1   | 253 |  253   |   0    |    4    | 11177613 |
| Median:     |   1   | 253 |  253   |   0    |    4    | 22355226 |
| 75%-tile:   |   1   | 253 |  253   |   0    |    5    | 33532838 |
| 97.5%-tile: |   1   | 254 |  254   |   6    |    6    | 43592689 |
| Maximum:    |   1   | 480 |  480   |   95   |   233   | 44710450 |
| Mean:       |   1   | 254 |  254   |   0    |    4    |          |

```         
# of unique seqs:   44710450
total # of seqs:    44710450

It took 782 secs to summarize 44710450 sequences.

Output File Names:
shrimp.trim.contigs.summary
```
:::

```{verbatim}
count.groups(count=current)
count.groups(count=shrimp.contigs.count_table)
```

```         
Size of smallest group: 58.

Total seqs: 44710450.
```

```{verbatim}
screen.seqs(fasta=current, count=current, maxambig=0, minlength=252, maxlength=254, maxhomop=6, processors=$PROC)
screen.seqs(fasta=current, count=current, maxambig=0, minlength=252, maxlength=254, maxhomop=6, processors=30)
```

```         
Using 30 processors.

It took 107 secs to screen 44710450 sequences, removed 8308318.
/******************************************/
Running command: remove.seqs(accnos=shrimp.trim.contigs.bad.accnos.temp, count=shrimp.contigs.count_table)
Removed 8308318 sequences from shrimp.contigs.count_table.

Output File Names:
shrimp.contigs.pick.count_table

/******************************************/

Output File Names:
shrimp.trim.contigs.good.fasta
shrimp.trim.contigs.bad.accnos
shrimp.contigs.good.count_table

It took 557 secs to screen 44710450 sequences.
```

```{verbatim}
summary.seqs(fasta=current, count=current, processors=$PROC)
summary.seqs(fasta=shrimp.trim.contigs.good.fasta, count=shrimp.contigs.good.count_table, processors=30)
```

::: {.callout-note appearance="simple" collapse="true"}

### Expand to see data set summary

|             | Start | End | NBases | Ambigs | Polymer | NumSeqs  |
|:------------|:-----:|:---:|:------:|:------:|:-------:|:--------:|
| Minimum     |   1   | 252 |  252   |   0    |    3    |    1     |
| 2.5%-tile:  |   1   | 252 |  252   |   0    |    3    |  910054  |
| 25%-tile:   |   1   | 253 |  253   |   0    |    4    | 9100534  |
| Median:     |   1   | 253 |  253   |   0    |    4    | 18201067 |
| 75%-tile:   |   1   | 253 |  253   |   0    |    5    | 27301600 |
| 97.5%-tile: |   1   | 254 |  254   |   6    |    6    | 35492079 |
| Maximum:    |   1   | 254 |  254   |   0    |    6    | 36402132 |
| Mean:       |   1   | 252 |  252   |   0    |    4    |          |

```         
# of unique seqs:   36402132
total # of seqs:    36402132

It took 610 secs to summarize 36402132 sequences.

Output File Names:
shrimp.trim.contigs.good.summary
```
:::

```{verbatim}
count.groups(count=current)
count.groups(count=shrimp.contigs.good.count_table)
```

```         
Size of smallest group: 57.

Total seqs: 36402132.
Output File Names: 
shrimp.contigs.good.count.summary
```

## Processing Improved Reads

```{verbatim}
unique.seqs(fasta=current, count=current)
unique.seqs(fasta=shrimp.trim.contigs.good.fasta, count=shrimp.contigs.good.count_table)
```

```         
36402132    4224192

Output File Names: 
shrimp.trim.contigs.good.unique.fasta
shrimp.trim.contigs.good.count_table
```

```{verbatim}
summary.seqs(count=current, processors=$PROC)
summary.seqs(count=shrimp.trim.contigs.good.count_table, processors=30)
```

::: {.callout-note appearance="simple" collapse="true"}

### Expand to see data set summary

|             | Start | End | NBases | Ambigs | Polymer | NumSeqs  |
|:------------|:-----:|:---:|:------:|:------:|:-------:|:--------:|
| Minimum     |   1   | 252 |  252   |   0    |    3    |    1     |
| 2.5%-tile:  |   1   | 252 |  252   |   0    |    3    |  910054  |
| 25%-tile:   |   1   | 253 |  253   |   0    |    4    | 9100534  |
| Median:     |   1   | 253 |  253   |   0    |    4    | 18201067 |
| 75%-tile:   |   1   | 253 |  253   |   0    |    5    | 27301600 |
| 97.5%-tile: |   1   | 254 |  254   |   6    |    6    | 35492079 |
| Maximum:    |   1   | 254 |  254   |   0    |    6    | 36402132 |
| Mean:       |   1   | 252 |  252   |   0    |    4    |          |

```         
# of unique seqs:   4224192
total # of seqs:    36402132

It took 78 secs to summarize 36402132 sequences.

Output File Names:
shrimp.trim.contigs.good.unique.summary
```
:::

## Aligning Reads

Since we are using the `silva.nr_v132.align` to align sequences, we can check where our reads start and end with the [ARB-SILVA web aligner](https://www.arb-silva.de/aligner/). After uploading a few sequences we find they start at postion 13862 and end at position 23445. Neat. We will pad these numbers to make sure we do not miss anything. 

```{verbatim}
pcr.seqs(fasta=$REF_LOC/silva.seed_v138_2.align, start=13862, end=23445, keepdots=F, processors=$PROC)
pcr.seqs(fasta=reference_dbs/silva.seed_v138_2.align, start=13862, end=23445, keepdots=F, processors=30)
```

```         
Using 30 processors.
[NOTE]: no sequences were bad, removing silva.nr_v132.bad.accnos

It took 10 secs to screen 8696 sequences.

Output File Names: 
silva.seed_v138_2.pcr.align
```

```{verbatim}
rename.file(input=pipelineFiles/silva.nr_v132.pcr.align, new=pipelineFiles/$ALIGNREF)
rename.file(input=pipelineFiles/silva.seed_v138_2.pcr.align, new=pipelineFiles/silva.v4.fasta)

summary.seqs(fasta=pipelineFiles/$ALIGNREF, processors=$PROC)
summary.seqs(fasta=pipelineFiles/silva.v4.fasta, processors=30)
```

::: {.callout-note appearance="simple" collapse="true"}

### Expand to see data set summary

|             | Start |  End | NBases | Ambigs | Polymer | NumSeqs |
|:------------|:-----:|:----:|:------:|:------:|:-------:|:-------:|
| Minimum     |   1   | 8722 |  221   |   0    |    3    |    1    |
| 2.5%-tile:  |   1   | 9583 |  252   |   0    |    4    |   218   |
| 25%-tile:   |   1   | 9583 |  253   |   0    |    4    |   2175  |
| Median:     |   1   | 9583 |  253   |   0    |    5    |   4349  |
| 75%-tile:   |   1   | 9583 |  254   |   0    |    6    |   6523  |
| 97.5%-tile: |   1   | 9583 |  421   |   1    |    6    |   8479  |
| Maximum:    |   15  | 9584 |  1082  |   5    |   10    |   8696  |
| Mean:       |   1   | 9582 |  288   |   0    |    4    |         |

```         
# of Seqs:  8696

It took 1 secs to summarize 8696 sequences.

Output File Names:
silva.v4.summary
```
:::

```{verbatim}
align.seqs(fasta=current, reference=$REF_LOC/$ALIGNREF, processors=$PROC)
align.seqs(fasta=shrimp.trim.contigs.good.unique.fasta, reference=pipelineFiles/silva.v4.fasta, processors=30)
```

```         
Using 30 processors.

Reading in the silva.v4.fasta template sequences... DONE.
It took 34 to read  213119 sequences.

Aligning sequences from shrimp.trim.contigs.good.unique.fasta ...
It took 2 to read  8696 sequences.

[WARNING]: 854 of your sequences generated alignments that 
eliminated too many bases, a list is provided in 
shrimp.trim.contigs.good.unique.flip.accnos.
[NOTE]: 444 of your sequences were reversed to produce a better alignment.

It took 780 seconds to align 4224192 sequences.

Output File Names: 
shrimp.trim.contigs.good.unique.align
shrimp.trim.contigs.good.unique.align_report
shrimp.trim.contigs.good.unique.flip.accnos
```

```{verbatim}
summary.seqs(fasta=current, count=current, processors=$PROC)
summary.seqs(fasta=shrimp.trim.contigs.good.unique.align, count=shrimp.trim.contigs.good.count_table, processors=30)
```

::: {.callout-note appearance="simple" collapse="true"}

### Expand to see data set summary

|             | Start |  End | NBases | Ambigs | Polymer | NumSeqs  |
|:------------|:-----:|:----:|:------:|:------:|:-------:|:--------:|
| Minimum     |   0   |   0  |   0    |   0    |    1    |    1     |
| 2.5%-tile:  |   1   | 9583 |  252   |   0    |    3    |  910054  |
| 25%-tile:   |   1   | 9583 |  253   |   0    |    4    | 9100534  |
| Median:     |   1   | 9583 |  253   |   0    |    4    | 18201067 |
| 75%-tile:   |   1   | 9583 |  253   |   0    |    5    | 27301600 |
| 97.5%-tile: |   1   | 9583 |  253   |   0    |    6    | 35492079 |
| Maximum:    | 9583  | 9584 |  254   |   0    |    6    | 36402132 |
| Mean:       |   1   | 9581 |  252   |   0    |    4    |          |

```         
# of unique seqs:   4224192
total # of seqs:    36402132

It took 141 secs to summarize 36402132 sequences.

Output File Names:
shrimp.trim.contigs.good.unique.summary
```
:::

```{verbatim}
screen.seqs(fasta=current, count=current, start=1968, end=11550, processors=$PROC)
screen.seqs(fasta=shrimp.trim.contigs.good.unique.align, count=shrimp.trim.contigs.good.count_table, start=1, end=9583, processors=30)
```

```         
Using 30 processors.

It took 66 secs to screen 4224192 sequences, removed 21326.

/******************************************/
Running command: 
remove.seqs(accnos=shrimp.trim.contigs.good.unique.bad.accnos.temp, 
count=shrimp.trim.contigs.good.count_table)
Removed 79070 sequences from shrimp.trim.contigs.good.count_table.

Output File Names:
shrimp.trim.contigs.good.pick.count_table

/******************************************/

Output File Names:
shrimp.trim.contigs.good.unique.good.align
shrimp.trim.contigs.good.unique.bad.accnos
shrimp.trim.contigs.good.good.count_table
```

```{verbatim}
summary.seqs(fasta=current, count=current, processors=$PROC)
summary.seqs(fasta=shrimp.trim.contigs.good.unique.good.align, count=shrimp.trim.contigs.good.good.count_table, processors=30)
```

::: {.callout-note appearance="simple" collapse="true"}

### Expand to see data set summary

|             | Start |  End | NBases | Ambigs | Polymer | NumSeqs  |
|:------------|:-----:|:----:|:------:|:------:|:-------:|:--------:|
| Minimum     |   1   | 9583 |  208   |   0    |    3    |    1     |
| 2.5%-tile:  |   1   | 9583 |  252   |   0    |    3    |  908077  |
| 25%-tile:   |   1   | 9583 |  253   |   0    |    4    | 9080766  |
| Median:     |   1   | 9583 |  253   |   0    |    4    | 18161532 |
| 75%-tile:   |   1   | 9583 |  253   |   0    |    5    | 27242297 |
| 97.5%-tile: |   1   | 9583 |  253   |   0    |    6    | 35414986 |
| Maximum:    |   1   | 9584 |  254   |   0    |    6    | 36323062 |
| Mean:       |   1   | 9583 |  252   |   0    |    4    |          |

```         
# of unique seqs:   4202866
total # of seqs:    36323062

It took 152 secs to summarize 36323062 sequences.

Output File Names:
shrimp.trim.contigs.good.unique.good.summary
```
:::

```{verbatim}
count.groups(count=current)
count.groups(count=shrimp.trim.contigs.good.good.count_table)
```

```         
Size of smallest group: 57.

Total seqs: 36323062.

Output File Names: 
shrimp.trim.contigs.good.good.count.summary
```

```{verbatim}
filter.seqs(fasta=current, vertical=T, trump=., processors=$PROC)
filter.seqs(fasta=shrimp.trim.contigs.good.unique.good.align, vertical=T, trump=., processors=30)
```

```         
Using 30 processors.
Creating Filter...
It took 31 secs to create filter for 4202866 sequences.


Running Filter...
It took 26 secs to filter 4202866 sequences.

Length of filtered alignment: 554
Number of columns removed: 9030
Length of the original alignment: 9584
Number of sequences used to construct filter: 4202866

Output File Names: 
shrimp.filter
shrimp.trim.contigs.good.unique.good.filter.fasta
```

```{verbatim}
unique.seqs(fasta=current, count=current)
unique.seqs(fasta=shrimp.trim.contigs.good.unique.good.filter.fasta, count=shrimp.trim.contigs.good.good.count_table)
```

```         
4202866	4178668

Output File Names: 
shrimp.trim.contigs.good.unique.good.filter.unique.fasta
shrimp.trim.contigs.good.unique.good.filter.count_table
```

```{verbatim}
summary.seqs(fasta=current, count=current, processors=$PROC)
summary.seqs(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.fasta, count=shrimp.trim.contigs.good.unique.good.filter.count_table, processors=30)
```

::: {.callout-note appearance="simple" collapse="true"}

### Expand to see data set summary

|             | Start | End | NBases | Ambigs | Polymer | NumSeqs  |
|:------------|:-----:|:---:|:------:|:------:|:-------:|:--------:|
| Minimum     |   1   | 554 |  208   |   0    |    3    |    1     |
| 2.5%-tile:  |   1   | 554 |  252   |   0    |    3    |  908077  |
| 25%-tile:   |   1   | 554 |  253   |   0    |    4    | 9080766  |
| Median:     |   1   | 554 |  253   |   0    |    4    | 18161532 |
| 75%-tile:   |   1   | 554 |  253   |   0    |    5    | 27242297 |
| 97.5%-tile: |   1   | 554 |  253   |   0    |    6    | 35414986 |
| Maximum:    |   1   | 554 |  254   |   0    |    6    | 36323062 |
| Mean:       |   1   | 554 |  252   |   0    |    4    |          |

```         
# of unique seqs:   4178668
total # of seqs:    36323062

It took 10 secs to summarize 36323062 sequences.

Output File Names:
shrimp.trim.contigs.good.unique.good.filter.unique.summary
```
:::

We now have a fully aligned and curated dataset that we can now pass off to the [MED pipeline](workflows/ssu/med/). 

```{verbatim}
system(cp pipelineFiles/shrimp.trim.contigs.good.unique.good.filter.unique.fasta pipelineFiles_med/)
system(cp pipelineFiles/shrimp.trim.contigs.good.unique.good.filter.count_table pipelineFiles_med/)
```

Moving on, to the next step of the mothur OTU pipeline. 

## Precluster

```{verbatim}
pre.cluster(fasta=current, count=current, diffs=2, processors=$PROC)
```

::: {.callout-note appearance="simple" collapse="true"}

### Expand to see partial output of `pre.cluster`

```         
Using 30 processors.

/******************************************/
Splitting by sample: 

Using 30 processors.

Selecting sequences for groups Control_1-Control_10-Control_11-Control_12
-Control_13-Control_14-Control_15-Control_16-Control_17-Control_18

Selected 828 sequences from WA_A_FORM_EG_7457.
Selected 2958 sequences from WA_A_FORM_EG_7752.
Selected 5589 sequences from WA_A_FORM_EG_9400.
Selected 4632 sequences from WA_A_FORM_GL_7402.
Selected 3860 sequences from WA_A_FORM_GL_7403.
Selected 5518 sequences from WA_A_FORM_GL_7455.

/******************************************/

Deconvoluting count table results...
It took 17 secs to merge 2465292 sequences group data.
/******************************************/
Running get.seqs: 
Selected 1675449 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.fasta.
/******************************************/
It took 8102 secs to run pre.cluster.

Using 3 processors.

Output File Names: 
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.fasta
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table

```
:::

```{verbatim}
summary.seqs(fasta=current, count=current, processors=$PROC)
```

::: {.callout-note appearance="simple" collapse="true"}

### Expand to see data set summary

|             | Start | End | NBases | Ambigs | Polymer | NumSeqs  |
|:------------|:-----:|:---:|:------:|:------:|:-------:|:--------:|
| Minimum     |   1   | 554 |  208   |   0    |    3    |    1     |
| 2.5%-tile:  |   1   | 554 |  252   |   0    |    3    |  908077  |
| 25%-tile:   |   1   | 554 |  253   |   0    |    4    | 9080766  |
| Median:     |   1   | 554 |  253   |   0    |    4    | 18161532 |
| 75%-tile:   |   1   | 554 |  253   |   0    |    5    | 27242297 |
| 97.5%-tile: |   1   | 554 |  253   |   0    |    6    | 35414986 |
| Maximum:    |   1   | 554 |  254   |   0    |    6    | 36323062 |
| Mean:       |   1   | 554 |  252   |   0    |    4    |          |

```         
# of unique seqs:   1675449
total # of seqs:    36323062

It took 35 secs to summarize 36323062 sequences.

Output File Names:
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.summary
```
:::

```{verbatim}
count.groups(count=current)
```

```         
Size of smallest group: 57.

Total seqs: 36323062.

Output File Names: 
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count.summary
```

## Remove Negative Controls

As with the OTU workflow, we remove NC samples, but in this case we skip the `pre.cluster` step. Our goal here to remove all NC samples from the dataset, as well as any reads that are predominantly found in NC samples. More on what that means in a minute. Before we do anything we need to identify all reads present in NC samples. 

Here is what we are going to do:

1. Subset the NC samples (and associated reads) from the `fasta` and `count.table`. To do this in mothur we need all of the NC sample names collected in an `.accnos` file, which is a text file used in  mothur  that contains a single column of names--these can be sequences, OTU labels, or sample names. This list is used to select or remove specific items from other mothur files. That way we can process subsets of the data without having to retype long lists of names. 

To generate the  `.accnos` file of NC samples we can use the `shrimp.files` file generated at the beginning of the mothur pipeline.

```{r}
#| eval: false
#| echo: true
tmp_accnos <- readr::read_delim(here(work_here, "nc_screen/shrimp.files"), 
                                delim = "\t", col_names = FALSE)
tmp_accnos[, 2:3] <- NULL
tmp_accnos <- tmp_accnos[grepl("Control_", tmp_accnos$X1), ]
readr::write_delim(tmp_accnos, file = here(work_here, "nc_screen/nc_samples.accnos"), 
                   col_names = FALSE)
```

2. Now we have a list of all NC sample names. The mothur command `get.groups` in conjunction with `accnos` file allows us to subset the full `fasta` and `count_table`


```{verbatim}
get.groups(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.fasta, count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table, accnos=nc_samples.accnos)
```

```         
Selected 192842 sequences from your count file.
Selected 4148 sequences from your fasta file.

Output File names:
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta

```

3. Next we rename the new files to something more informative (and shorter).

```{verbatim}
rename.file(input=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta, new=nc.fasta)
rename.file(input=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table, new=nc.count_table)
```

4. And a quick summary of the NC subset. 

```{verbatim}
summary.seqs(fasta=nc.fasta, count=nc.count_table, processors=$PROC)
```

::: {.callout-note appearance="simple" collapse="true"}

### Expand to see negative control summary

|             | Start | End | NBases | Ambigs | Polymer | NumSeqs |
|:------------|:-----:|:---:|:------:|:------:|:-------:|:-------:|
| Minimum     |   1   | 554 |  248   |   0    |    3    |    1    |
| 2.5%-tile:  |   1   | 554 |  252   |   0    |    4    |  4822   |
| 25%-tile:   |   1   | 554 |  253   |   0    |    4    |  48211  |
| Median:     |   1   | 554 |  253   |   0    |    5    |  96422  |
| 75%-tile:   |   1   | 554 |  253   |   0    |    5    | 144632  |
| 97.5%-tile: |   1   | 554 |  254   |   0    |    6    | 188021  |
| Maximum:    |   1   | 554 |  254   |   0    |    6    | 192842  |
| Mean:       |   1   | 554 |  253   |   0    |    4    |         |

```         
# of unique seqs:   4148
total # of seqs:    192842

It took 0 secs to summarize 192842 sequences.

Output File Names:
neg_control.summary
```
:::

5. Sweet. We use the command `list.seqs` to get a complete list of all repseq names in the NC subset.

```{verbatim}
list.seqs(count=nc.count_table)
```

```         
Output File Names: 
nc.accnos
```

This gives us all repseq IDs in the NC samples. 

6.  We could simply use the `nc.accnos` file from the `list.seqs` command to remove repseqs found in negative control (NC) samples from the main dataset. This seems reasonable enough. Except mothur will remove **all** repseqs found in a NC sample, in other words, any repseq in the accnos file. For example, let's consider the following scenario where we have two repseqs:

`repseq01` is abundant in many NC samples but not found in any other samples.   
`repseq02` on the other hand is represented by say one read in a single NC sample but very abundant in other samples.   

It makes sense to remove `repseq01` but not necessarily `repseq02`. Essentially, for each `repseq` in the `nc.accnos` file we want to calculate:

-   The total number of reads in NC samples.\
-   The total number of reads in non-NC samples.\
-   The percent of reads in the NC samples.
-   The total number of NC samples containing at least 1 read.\
-   The total number of non-NC samples containing at least 1 read.\
-   The percent of NC samples containing reads.

Where a final data table might look something like this

| repseq     | rc_nc | rc_samps | %in_nc  | nc_samp | no_nc_samp | %_in_nc_samp   |
|:-----------|:-----:|:--------:|:-------:|:-------:|:----------:|:--------------:|
| repseq001  |   3   | 5        |  37.5   |   1     |    2       |    33.31       |
| repseq002  |   196 | 308      |  38.9   |   17    |    38      |    30.7        |
| repseq003  |   3   | 23       |  11.1   |   3     |    18      |    14.5        |

To accomplish this we will parse out relevant data from the `.count_table` files. We got the idea on how best to do this from a [discussion on the mothur forum](https://forum.mothur.org/t/negative-control/2754).

To save space and minimize file size, mothur formats the [`.count_table`](https://mothur.org/wiki/count_file/) using a sparse format by storing only non zero sample counts. However, we need the full format which lists each repseq and its abundance counts for each sample. Using the command `count.seqs` in conjunction with the `.count_table` will return a full format table. 

```{verbatim}
count.seqs(count=nc.count_table, compress=f)
```

```
Output File Names:
nc.full.count_table
```

Then we use the accnos file (`nc.accnos`)--containing repseqs found in NC samples, to generate a subset of the complete dataset containing only the reseqs found in NC samples. 

```{verbatim}
get.seqs(accnos=nc.accnos, fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.fasta, count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table)
```

```
Selected 4148 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.precluster.fasta.
Selected 15029155 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table.

Output File Names:
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table
```
```{verbatim}
rename.file(input=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta, new=subset.fasta)
rename.file(input=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table, new=subset.count_table)
```

And again run `count.seqs` to get a full format table of the complete dataset, which we subsetted earlier to contain only repseqs found in NC samples. 

```{verbatim}
count.seqs(count=subset.count_table, compress=f)
```

```
Output File Names:
subset.full.count_table
```

Finally we can parse out read count data from the  two `subset.full.count_table` files. 

```{r}
full_count_tab <- readr::read_delim(here(work_here, "nc_screen/subset.full.count_table"), 
                                    delim = "\t", col_names = TRUE)
# figure out which columns to use
control_cols     <- grep("^Control_", names(full_count_tab), value = TRUE)
noncontrol_cols  <- setdiff(names(full_count_tab)[-(1:2)], control_cols)

# now do the rowwise sums
read_totals <- full_count_tab %>%
  rowwise() %>%
  mutate(
    total_reads_nc   = sum(c_across(all_of(control_cols)), na.rm = TRUE),
    total_reads_non_nc = sum(c_across(all_of(noncontrol_cols)), na.rm = TRUE)
  ) %>%
  ungroup() %>%
  select(1, 2, total_reads_nc, total_reads_non_nc)

read_totals <- read_totals %>% dplyr::rename("total_reads" = 2)
```

And here is what the new dataframe looks like. Three columns where the first is the repseq name, the second the total number of reads in NC samples, and the third the total number of reads in the entire dataset (this includes NC samples).

```{r}
#| echo: false
#| eval: true
#head_read_totals <- head(read_totals)
#nc_dim <- dim(read_totals)[1]
pandoc.table(read_totals[1:6, 1:4], 
             emphasize.rownames = FALSE, 
             split.tables = Inf)
```

In total there are `r nc_dim` repseqs that were potential contaminants.

Now we add in a column that calculates the percent of reads in the NC samples. 

```{r}
#| echo: true
#| eval: false
tmp_read_totals <- read_totals %>%
  dplyr::mutate(perc_reads_in_nc = 100*(
    total_reads_nc / (total_reads_nc + total_reads_non_nc)),
                .after = "total_reads_non_nc")
tmp_read_totals$perc_reads_in_nc <- 
  round(tmp_read_totals$perc_reads_in_nc, digits = 6)
```

And then we calculate row sums to get the number of NC and non-NC samples containing these reads. For this we can use the `subset.full.count_table` we read in above.

```{r}
#| echo: true
#| eval: false
control_cols     <- grep("^Control_", names(full_count_tab), value = TRUE)
noncontrol_cols  <- setdiff(names(full_count_tab)[-(1:2)], control_cols)
# rowwise tally of non-zero columns
samp_totals <- full_count_tab %>%
  rowwise() %>%
  mutate(
    num_nc_samp     = sum(c_across(all_of(control_cols)) != 0, na.rm = TRUE),
    num_non_nc_samp = sum(c_across(all_of(noncontrol_cols)) != 0, na.rm = TRUE)
  ) %>%
  ungroup() %>%
  select(1, num_nc_samp, num_non_nc_samp)
```

Finally add a column with the total number of samples and calculate the percent of NC samples containing these reads. 

```{r}
#| echo: true
#| eval: false
samp_totals$total_samp <- samp_totals$num_nc_samp + samp_totals$num_non_nc_samp
samp_totals <- samp_totals %>%  
  dplyr::relocate("total_samp", .after = "Representative_Sequence")
samp_totals <- samp_totals %>%
  dplyr::mutate(perc_nc_samp = 
                  100*( num_nc_samp / (num_nc_samp + num_non_nc_samp)),
                  .after = "num_non_nc_samp")
```

```{r}
#| echo: false
#| eval: false
nc_check <- dplyr::left_join(tmp_read_totals, samp_totals, by = "Representative_Sequence")
write_delim(nc_check, here(work_here, "nc_screen/reads_in_nc_samples.txt"),
    delim = "\t")
```

After all of this wrangling here is a snippet of the table showing the distribution of NC reads across samples. 

```{r}
#| echo: false
#| eval: true
pandoc.table(nc_check[1:3, 1:9], 
             emphasize.rownames = FALSE, 
             split.tables = Inf)
```

Now we remove any repseqs where:

-   The number of reads found in NC samples accounted for more than 10% of total reads OR
-   The percent of NC samples containing the repseq was greater than 10% of total samples.

```{r}
#| echo: true
#| eval: false
nc_remove <- nc_check %>% 
  filter(perc_reads_in_nc > 10 | perc_nc_samp > 10)
```

```{r}
#| echo: false
#| eval: false
nc_remain <- dplyr::anti_join(nc_check, nc_remove)

rem_nc_reads <- sum(nc_remove$total_reads_nc)
rem_sam_reads <- sum(nc_remove$total_reads_non_nc)
per_reads_rem <- round(100*( rem_nc_reads / (rem_nc_reads + rem_sam_reads)), 
                       digits = 3)

ret_nc_reads <- sum(nc_remain$total_reads_nc)
ret_sam_reads <- sum(nc_remain$total_reads_non_nc)
per_reads_ret <- round(100*( ret_nc_reads / (ret_nc_reads + ret_sam_reads)), 
                       digits = 3)
```

|          | Total rep seqs      | NC reads         | non NC reads      | \% NC reads       |
|----------|---------------------|------------------|-------------------|-------------------|
| Removed  | `r nrow(nc_remove)` | `r rem_nc_reads` | `r rem_sam_reads` | `r per_reads_rem` |
| Retained | `r nrow(nc_remain)` | `r ret_nc_reads` | `r ret_sam_reads` | `r per_reads_ret` |

We identified a total of **`r nrow(nc_check)`** representative sequences (`repseqs`) that were present in at least 1 NC sample by at least 1 read. We removed any repseq where more than 10% of total reads were found in NC samples OR any repseq found in more than 10% of NC samples. Based on these criteria we removed **`r nrow(nc_remove)` **repseqs from the data set, which accounted for **`r rem_nc_reads`** total reads in NC samples and **`r rem_sam_reads`** total reads in non-NC samples. Of the total reads removed **`r per_reads_rem`%** came from NC samples. Of all repseqs identified in NC samples, **`r nrow(nc_remain)`** were retained because they fell below the threshold criteria. These repseqs accounted for **`r ret_nc_reads`** reads in NC samples and **`r ret_sam_reads`** reads in non-NC samples. NC samples accounted for **`r per_reads_ret`%** of these reads.

OK, now we can create a new `neg_control.accnos` containing only repseqs abundant in NC samples.

```{r}
#| echo: true
#| eval: false
write_delim(
  data.frame(nc_remove$Representative_Sequence), 
  here(work_here, "nc_screen/nc_repseq_remove.accnos"), 
  col_names = FALSE)
```

And then use this file in conjunction with the mothur command `remove.seqs`. 

```{verbatim}
remove.seqs(accnos=nc_repseq_remove.accnos, fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.fasta, count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table)
```

```         
Removed 3886 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.precluster.fasta.
Removed 375155 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table.

Output File Names:
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table

```

```{verbatim}
count.groups(count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table)
```

```         
Size of smallest group: 1.

Total seqs: 35947907.

Output File Names:
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count.summary
```

Before we remove the NC samples we need to check whether some NC samples were already removed. When mothur runs the `remove.seqs` command it will automatically remove any samples where the read count has fallen to zero. If mothur did remove samples and we try to remove all NC samples, we will get an error. To check we can compare the `count.summary` files before and after the previous `remove.seqs` command.

```{r}
tmp_before <- read_tsv(
  here(work_here, 
       "nc_screen/shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count.summary"),
  col_names = FALSE,
  col_select = 1
)

tmp_after <- read_tsv(
  here(work_here, 
       "nc_screen/shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count.summary"),
  col_names = FALSE,
  col_select = 1
)
tmp_nc_lost <- anti_join(tmp_before, tmp_after)
tmp_nc_lost$X1
```

These are the samples that were removed when we ran `remove.seqs`. We need to eliminate these sample IDs from our list of NC samples to remove or mothur will throw an error. 

```         
[1] "Control_15" "Control_18" "Control_21" "Control_29" "Control_5"   
```

As before, we can generate a list of NC samples to use in conjunction with the  `remove.groups` command to eliminate all NC samples. 

```{r}
#| echo: true
#| eval: false 
nc_to_remove <- semi_join(tmp_before, tmp_after)
nc_to_remove <- nc_to_remove %>%
  dplyr::filter(
    stringr::str_starts(X1, "Control")
    )
readr::write_delim(nc_to_remove, 
                   file = here(work_here, "nc_screen/nc_samples_remove.accnos"), 
                   col_names = FALSE)
```

```{r}
#| echo: false
#| eval: false 
nc_to_remove
```

In total the following mothur command should remove `r dim(nc_to_remove)[1]` negative control samples.

```{verbatim}
remove.groups(count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table, fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta, accnos=nc_samples_remove.accnos)
```

```         
Removed 35907 sequences from your count file.
Removed 0 sequences from your fasta file.

Output File names:
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.count_table
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.fasta
```

```{verbatim}
summary.seqs(fasta=current, count=current, processors=30)
```

::: {.callout-note appearance="simple" collapse="true"}

### Expand to see data set summary

|             | Start | End | NBases | Ambigs | Polymer | NumSeqs  |
|:------------|:-----:|:---:|:------:|:------:|:-------:|:--------:|
| Minimum     |   1   | 554 |  208   |   0    |    3    |    1     |
| 2.5%-tile:  |   1   | 554 |  252   |   0    |    3    |  897801  |
| 25%-tile:   |   1   | 554 |  253   |   0    |    4    | 8978001  |
| Median:     |   1   | 554 |  253   |   0    |    4    | 17956001 |
| 75%-tile:   |   1   | 554 |  253   |   0    |    5    | 26934001 |
| 97.5%-tile: |   1   | 554 |  253   |   0    |    6    | 35014201 |
| Maximum:    |   1   | 554 |  254   |   0    |    6    | 35912000 |
| Mean:       |   1   | 554 |  252   |   0    |    4    |          |

```         
# of unique seqs:   1671563
total # of seqs:    35912000

It took 74 secs to summarize 35912000 sequences.

Output File Names:
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.summary
```

:::

```{verbatim}
count.groups(count=current)
```

```         
Size of smallest group: 14.

Total seqs: 35912000.

Output File Names:
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.count.summary
```

## Remove Chimeras

```{verbatim}
chimera.vsearch(fasta=current, count=current, dereplicate=t, processors=$PROC)
```

```         
Using vsearch version v2.30.0.
Checking sequences from shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.fasta ...
/******************************************/
Splitting by sample: 

...

Removing chimeras from your input files:
/******************************************/
Running command: remove.seqs(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.fasta, accnos=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.accnos)
Removed 619952 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.fasta.

Output File Names:
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.pick.fasta

/******************************************/

Output File Names:
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.count_table
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.chimeras
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.accnos
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.fasta
```

```{verbatim}
summary.seqs(fasta=current, count=current, processors=30)
```

::: {.callout-note appearance="simple" collapse="true"}

### Expand to see data set summary

|             | Start | End | NBases | Ambigs | Polymer | NumSeqs  |
|:------------|:-----:|:---:|:------:|:------:|:-------:|:--------:|
| Minimum     |   1   | 554 |  208   |   0    |    3    |    1     |
| 2.5%-tile:  |   1   | 554 |  252   |   0    |    3    |  863358  |
| 25%-tile:   |   1   | 554 |  253   |   0    |    4    | 8633575  |
| Median:     |   1   | 554 |  253   |   0    |    4    | 17267149 |
| 75%-tile:   |   1   | 554 |  253   |   0    |    5    | 25900723 |
| 97.5%-tile: |   1   | 554 |  253   |   0    |    6    | 33670940 |
| Maximum:    |   1   | 554 |  254   |   0    |    6    | 34534297 |
| Mean:       |   1   | 554 |  252   |   0    |    4    |          |

```         
# of unique seqs:   1051611
total # of seqs:    34534297

It took 22 secs to summarize 34534297 sequences.

Output File Names:
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.summary
```
:::

```{verbatim}
count.groups(count=current)
```

```         
Size of smallest group: 14.

Total seqs: 34534297.

Output File Names:
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.count.summary
```

## Assign Taxonomy

The `classify.seqs` command requires properly formatted reference and taxonomy databases. For taxonomic assignment, we are using the GSR database [@molano2024gsr]. The developers of mothur maintain [formatted versions of popular databases](https://mothur.org/wiki/taxonomy_outline/), however the GSR-DB has not been formatted by the developers yet.

::: callout-note
Here can download an appropriate version of the [GSR database](https://manichanh.vhir.org/gsrdb/download_db_links2.php).
:::

To create a mothur formatted version GSR-DB[^_merge_runs-1], we perform the following steps.

[^_merge_runs-1]: From the developers: GSR database (Greengenes, SILVA, and RDP database) is an integrated and manually curated database for bacterial and archaeal 16S amplicon taxonomy analysis. Unlike previous integration approaches, this database creation pipeline includes a taxonomy unification step to ensure consistency in taxonomical annotations. The database was validated with three mock communities and two real datasets and compared with existing 16S databases such as Greengenes, GTDB, ITGDB, SILVA, RDP, and MetaSquare. Results showed that the GSR database enhances taxonomical annotations of 16S sequences, outperforming current 16S databases at the species level. The GSR database is available for full-length 16S sequences and the most commonly used hypervariable regions: V4, V1-V3, V3-V4, and V3-V5.

### Make a custom DB

Here we are using the [GSR V4 database](https://manichanh.vhir.org/gsrdb/GSR-DB_V4_cluster-1.tar.gz). 

```{zsh}
#| echo: true
#| eval: false
wget https://manichanh.vhir.org/gsrdb/GSR-DB_V4_cluster-1.tar.gz
tar -xvzf GSR-DB_V4_cluster-1.tar.gz
```

First (in the command line) we remove first line of the taxonomy file. 

```{zsh}
cp GSR-DB_V4_cluster-1_taxa.txt tmp0.txt
sed '1d' tmp0.txt > tmp1.txt
```

Next, delete species and remove leading \[a-z\]\_\_ from taxa names

```{zsh}
sed -E 's/s__.*//g' tmp1.txt > tmp2.txt
sed -E 's/[a-zA-Z]__//g' tmp2.txt > gsrdb.tax
cp GSR-DB_V4_cluster-1_seqs.fasta gsrdb.fasta
```

```{verbatim}
classify.seqs(fasta=current, count=current, reference=reference_dbs/gsrdb.fasta, taxonomy=reference_dbs/gsrdb.tax, processors=3)
```

```         
Using 30 processors.
Generating search database...    DONE.
It took 2 seconds generate search database.

Reading in the reference_dbs/gsrdb.txt taxonomy...  DONE.
Calculating template taxonomy tree...     DONE.
Calculating template probabilities...     DONE.
It took 6 seconds get probabilities.
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.fasta ...
[WARNING]: M06508_9_000000000-JTBW3_1_1102_26159_16839 could not be classified. 
You can use the remove.lineage command with taxon=unknown; 
to remove such sequences.
...

It took 348 secs to classify 1051611 sequences.

It took 503 secs to create the summary file for 1051611 sequences.

Output File Names:
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.taxonomy
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.tax.summary
```

## Remove Contaminants

```{verbatim}
remove.lineage(fasta=current, count=current, taxonomy=current, taxon=$CONTAMINENTS)
```

```         
Running command: 
remove.seqs(accnos=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.accnos, 
count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.count_table, 
fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.fasta)

Removed 617 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.fasta.
Removed 2776 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.count_table.

/******************************************/

Output File Names:
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.pick.taxonomy
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.accnos
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.pick.count_table
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.pick.fasta
```

```{verbatim}
summary.tax(taxonomy=current, count=current)
```

```         
Using shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.pick.count_table 
as input file for the count parameter.
Using shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.pick.taxonomy 
as input file for the taxonomy parameter.

It took 489 secs to create the summary file for 34531521 sequences.

Output File Names: 
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.pick.tax.summary
```

```{verbatim}
count.groups(count=current)
```

```         
Size of smallest group: 14.

Total seqs: 34531521.

Output File Names: 
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.pick.count.summary
```

## Track Reads through Workflow

At this point we can look at the number of reads that made it through each step of the workflow for every sample.

```{r}
#| echo: false
#| eval: false
# this is to generate a read change table DO NOT RUN HERE
# gather all count.summary file together in one directory
library(dplyr)
library(purrr)
library(readr)
files <- file.info(list.files("read_change_summary", pattern = "\\.summary$", full.names = TRUE))
files <- files[order(files$mtime), ]
files <- as.character(row.names(files))
col_names <- c("SampleID", 
               "raw_reads", 
               "initial_screen", 
               "screen_after_align", 
               "remove_nc_reads", 
               "remove_nc", 
               "nochim", 
               "no_contam")  

df <- files %>%
  map(~ read_tsv(.x, col_names = c("SampleID", "value"), col_types = "ci")) %>% 
  reduce(left_join, by = "SampleID")

# Rename columns
names(df) <- col_names

# Arrange rows (optional: if you want custom order of samples)
# For example, put controls first:
df <- df %>%
  arrange(factor(SampleID, levels = sort(unique(SampleID))))  
# Write to tab-delimited file
write_tsv(df, "mothur_pipeline_read_changes.txt")
```

```{r}
#| echo: true
#| eval: false
read_change <- read_tsv(
  here(work_here, "mothur_pipeline_read_changes.txt"),
  col_names = TRUE
)
```

## Preparing for analysis

```{verbatim}
rename.file(fasta=current, count=current, taxonomy=current, prefix=final)
```

```         
Current files saved by mothur:
fasta=final.fasta
taxonomy=final.taxonomy
count=final.count_table
```

## Clustering

```{verbatim}
cluster.split(fasta=final.fasta, count=final.count_table, taxonomy=final.taxonomy, taxlevel=4, cluster=f, processors=$PROC)
cluster.split(file=final.file, count=final.count_table, processors=$PROC)
```

```         
Using 30 processors.
Splitting the file...
/******************************************/
Selecting sequences for group Vibrionales (1 of 364)
Number of unique sequences: 92783

Selected 5390956 sequences from final.count_table.

Calculating distances for group Vibrionales (1 of 364):

Sequence    Time    Num_Dists_Below_Cutoff

It took 902 secs to find distances for 92783 sequences. 
477552179 distances below cutoff 0.03.

Output File Names:
final.0.dist

...

It took 8671 seconds to cluster
Merging the clustered files...
It took 14 seconds to merge.
[WARNING]: Cannot run sens.spec analysis without a column file, 
skipping.
Output File Names: 
final.opti_mcc.list
```

```{verbatim}
system(mkdir cluster.split.gsrdb) 
system(mv final.opti_mcc.list cluster.split.gsrdb/) 
system(mv final.file cluster.split.gsrdb/) 
system(mv final.dist cluster.split.gsrdb/)
```

```{verbatim}
dist.seqs(fasta=final.fasta, cutoff=0.03, processors=\$PROC) cluster(column=final.dist, count=final.count_table)
```

```         
Sequence    Time    Num_Dists_Below_Cutoff

It took 91935 secs to find distances for 1022766 sequences. 
1096480673 distances below cutoff 0.03.

Output File Names: 
final.dist

You did not set a cutoff, using 0.03.

Clustering final.dist

iter    time    label   num_otus    cutoff  tp  tn  fp  fn  sensitivity specificity ppv npv fdr accuracy    mcc f1score

0.03
0   0   0.03    1022766 0.03    0   5.21928e+11 0   1.09648e+09 0   1   0   0.997904    1   0.997904    0   0   
1   3187    0.03    130371  0.03    7.80436e+08 5.21829e+11 9.9517e+07  3.16045e+08 0.711764    0.999809    0.886906    0.999395    0.886906    0.999205    0.794146    0.789741    
2   3706    0.03    119919  0.03    7.82225e+08 5.21828e+11 9.99504e+07 3.14256e+08 0.713396    0.999808    0.8867  0.999398    0.8867  0.999208    0.794965    0.790663    
3   3712    0.03    119453  0.03    7.82257e+08 5.21828e+11 9.99331e+07 3.14224e+08 0.713425    0.999809    0.886722    0.999398    0.886722    0.999208    0.794991    0.790689    

It took 21013 seconds to cluster

Output File Names: 
final.opti_mcc.list
final.opti_mcc.steps
final.opti_mcc.sensspec
```

<!------------------------------------------------------------------------>
<!-------------------- Use this area to save things ---------------------->
<!------------------------------------------------------------------------>

```{r}
#| echo: false
#| eval: false
readr::write_delim(read_change,
  here(share_here, "mothur_pipeline_read_changes.txt"),
  col_names = TRUE, delim = "\t"
)
file.copy(here(work_here,  "nc_screen/reads_in_nc_samples.txt"), 
          here(share_here), 
          overwrite = TRUE, recursive = FALSE, 
          copy.mode = TRUE, copy.date = FALSE)
```

```{r}
#| echo: false
#| eval: false
objects()
gdata::keep(nc_check, nc_remain, nc_remove, nc_to_remove, nc_dim,
            no_read_sum, per_reads_rem, per_reads_ret, 
            rem_nc_reads, rem_sam_reads, ret_nc_reads, 
            ret_sam_reads, read_change, nc_tmp1, nc_tmp2,
            read_totals, 
            sure = TRUE)
save.image(here("page_build", "otu_part1.rdata"))
```

