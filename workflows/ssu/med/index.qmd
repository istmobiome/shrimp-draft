---
title: "5. MED Workflow"
description: |
  Workflow for running MED analysis. Workflow begins with redundant, aligned fasta file from mothur and ends with the MED analysis. A Microtable Object is produced to collate the data for downstream analysis.
listing: 
    id: med-listing
    contents: data-med.yml
    type: table
    sort-ui: false
    filter-ui: false
    fields: 
      - filename
      - description
    field-links: 
      - filename
    field-display-names: 
      filename: File Name
      description: Description
---

{{< include /include/_setup.qmd >}}

```{r}
#| echo: false
#| eval: true
#| comment: set wd for here
remove(list = ls())
workflow_name <- "med" # e.g., med, dada2, etc
work_here <- paste0("working_files/ssu/", workflow_name)
share_here <- paste0("share/ssu/", workflow_name)
source(here("assets/scripts", "summarize_objs.R"))
source(here("assets/scripts", "mia_metrics.R"))
```

## Workflow Input

::: {.callout-note icon=false}
## Data & Scripts

Fastq sequence files, scripts, and other assets for running this workflow can be found on the [MED Data Portal](/workflows/portal/data-med.qmd) page. 

The Data Portal page also contains a link to the curated output of this pipelineâ€“-feature-sample count table, taxonomy table, sample metadata, & representative fasta sequences. Data is available as stand-alone text files, OR bundled in microtable and phyloseq objects. Archive also includes a table tracking sample read changes.
:::

#### Required Packages & Software

To run this workflow you need to  [install mothur](https://mothur.org/wiki/installation/) and the [Oligotyping/MED](https://merenlab.org/2014/08/16/installing-the-oligotyping-pipeline/). You will need the [tidyverse](https://www.tidyverse.org) package. 

## Overview

With the mothur pipeline finished, we can turn our attention to [Minimum Entropy Decomposition (MED)](https://merenlab.org/2014/11/04/med/) [@eren2015minimum]. MED is a novel, information theory-based clustering algorithm for sensitive partitioning of high-throughput marker gene sequences. 

(From the Meren Lab website) MED:

- Does not perform pairwise sequence comparison,  
- Does not rely on arbitrary sequence similarity thresholds,  
- Does not require user supervision,  
- Does not require preliminary classification or clustering results,  
- Is agnostic to sampling strategy or how long your sequences are,  
- Gives you 1 nucleotide resolution over any sequencing length with computational efficiency and minimal computational heuristics.

MED needs a redundant alignment file of read data. This means **all** identical sequences need to be included.  We again use mothur but this pipeline starts with the currated output of the [`align.seqs`](/workflows/ssu/otu/#aligning-reads) portion of our mothur OTU pipeline. 

We set up our run in the same way as the mothur pipeline. 

{{< include include/_med_flowchart_1.qmd >}}

# Read Processing

{{< include include/_MED_Part1.qmd >}}

# MED Analysis

```{r}
#| eval: true
#| echo: false
#| message: false 
#| results: hide
load(here("page_build", "med_part2.rdata"))
```

::: {style="text-align: center;"}
{{< include include/_med_flowchart_2.qmd >}}
:::

Now that we have the necessary files we are almost ready to run the MED pipeline. First though we need to properly format the mothur files. For this we will use a custom script called [`mothur2oligo`](https://github.com/michberr/MicrobeMiseq) written by [Michelle Berry](https://github.com/michberr).

```{bash}
bash mothur2oligo.sh
```

::: {.callout-note appearance="simple" collapse="true"}
### Expand to see mothur2oligo workflow

These steps are run automatically by the `mothur2oligo` script.

```{verbatim}
get.lineage(taxonomy=final_med.taxonomy, taxon='Bacteria;-Archaea;', count=final_med.count_table)
```

```         
/******************************************/
Running command: get.seqs(accnos=final_med.pick.taxonomy.accnos, count=final_med.count_table)
Selected 34628081 sequences from final_med.count_table.

Output File Names:
final_med.pick.count_table

/******************************************/

Output File Names:
final_med.pick.taxonomy
final_med.pick.taxonomy.accnos
final_med.pick.count_table
```

```{verbatim}
list.seqs(count=current)
```

```         
Using final_med.pick.count_table as input file for the count parameter.

Output File Names: 
final_med.pick.accnos
```

```{verbatim}
get.seqs(accnos=current, fasta=final_med.fasta)
```

```         
Using pipelineFiles_med/final_med.pick.accnos as input file for the accnos parameter.
Selected 3433440 sequences from pipelineFiles_med/final_med.fasta.

Output File Names:
final_med.pick.fasta
```

```{verbatim}
deunique.seqs(fasta=current, count=current)
```

```         
Using final_med.pick.count_table as input file for the count parameter.
Using final_med.pick.fasta as input file for the fasta parameter.

Output File Names: 
final_med.pick.redundant.fasta
final_med.pick.redundant.groups
```

The eventual output of the `mothur2oligo` workflow is a fasta file with reformatted headers. The script `renamer.pl` adds sample names to fasta headers and appends the extension `_headers-replaced.fasta` to a new file with these reformatted data. 

```
final_med.pick.redundant.fasta_headers-replaced.fasta
```

:::

```{zsh}
o-trim-uninformative-columns-from-alignment \
        final_med.pick.redundant.renamed.fasta
decompose final_med.pick.redundant.renamed.fasta-TRIMMED \
        --sample-mapping med_mapping.txt \
        --output-directory MED \
        --number-of-threads 24 \
        --skip-gen-figures
```


To run the `decompose` command you will need this [sample mapping file](/share/ssu/med/med_mapping.txt).

::: {.callout-tip appearance="default"}

## MED Results

Once this is finished, the MED software will output a [summary of the run](/share/ssu/med/med_results/index.html). 
:::

# Data Set Prep

In this next part of the workflow our main goal is to create a *microtable object* using the R package [microeco](https://joey711.github.io/phyloseq/) [@liu2021microeco]. The microtable will be used to store the OTU by sample data as well the taxonomic, fasta, and sample data in a single object. More on that in a moment.

## Assign Taxonomy

Our first step is to classify the node representatives from the MED output. The `classify.seqs` command requires properly formatted reference and taxonomy databases. For taxonomic assignment, we are using the GSR database [@molano2024gsr]. The developers of mothur maintain [formatted versions of popular databases](https://mothur.org/wiki/taxonomy_outline/), however the GSR-DB has not been formatted by the developers yet.

::: callout-note
Here you can download an appropriate version of the [GSR database](https://manichanh.vhir.org/gsrdb/download_db_links2.php).
:::

To create a mothur formatted version GSR-DB, we perform the following steps (we went through this process above prior to removing contaminants but will repeat here for posterity). 

#### Download a data base

Here we are using the [GSR V4 database](https://manichanh.vhir.org/gsrdb/GSR-DB_V4_cluster-1.tar.gz).

```{zsh}
#| echo: true
#| eval: false
wget https://manichanh.vhir.org/gsrdb/GSR-DB_V4_cluster-1.tar.gz
tar -xvzf GSR-DB_V4_cluster-1.tar.gz
```

First (in the command line) we remove first line of the taxonomy file.

```{zsh}
cp GSR-DB_V4_cluster-1_taxa.txt tmp0.txt
sed '1d' tmp0.txt > tmp1.txt
```

Next, delete species and remove leading \[a-z\]\_\_ from taxa names

```{zsh}
sed -E 's/s__.*//g' tmp1.txt > tmp2.txt
sed -E 's/[a-zA-Z]__//g' tmp2.txt > gsrdb.tax
cp GSR-DB_V4_cluster-1_seqs.fasta gsrdb.fasta
```

The next thing we need to do is grab the `node-representatives.fa.txt` from the MED output so that we can classify these sequences. Of course, proper formatting is required.

```{zsh}
seqkit replace --pattern ^ --replacement MED node-representatives.fa.txt > tmp1.fa
seqkit replace --pattern "\|.*" --replacement '' tmp1.fa > med_nodes.fasta
rm tmp1.fa
```

Great, the reference database is formatted. Now we need to make a few files that mimics the normal mothur output files because the MED pipeline does not exactly generate the files we need to create a `microtable` object. First we use the `matrix_counts.txt` file from the MED analysis to create a mothur-styled `count.table`.

```{r}
#| echo: true
#| eval: false
tmp_med_counts <- read_tsv(
  here(work_here, "med_results/matrix_counts.txt"),
    col_names = TRUE)

tmp_med_counts <- tmp_med_counts %>% 
  dplyr::rename_with( ~ paste0("MED", .x)) 

tmp_med_counts <- tmp_med_counts %>%
  tidyr::pivot_longer(cols = c(-1), names_to = "tmp") %>%
  tidyr::pivot_wider(names_from = c(1))

tmp_med_counts <- tibble::column_to_rownames(tmp_med_counts, "tmp")

tmp_med_counts <- tmp_med_counts %>%
                  mutate(total = rowSums(.), .before = 1)

tmp_med_counts <- tmp_med_counts %>% 
     tibble::rownames_to_column("Representative_Sequence")
med_counts <- tmp_med_counts
```

```{r}
#| echo: false
#| eval: false
write.table(med_counts, here(work_here, "node_taxonomy/med_nodes.count.table"), 
            row.names = FALSE, quote = FALSE, sep = "\t")
```

Now we can actually classify the representative sequences. 

```{verbatim}
classify.seqs(fasta=med_nodes.fasta, count=med_nodes.count.table, reference=reference_dbs/gsrdb.fasta, taxonomy=reference_dbs/gsrdb.tax)
```

Now we make a mothur styled `shared` file. 

```{r}
#| echo: true
#| eval: false
tmp_med_counts <- read_tsv(
  here(work_here, "med_results/matrix_counts.txt"),
    col_names = TRUE)

tmp_n_meds <- ncol(tmp_med_counts) - 1
tmp_med_counts <- tmp_med_counts %>% 
  dplyr::rename_with( ~ paste0("MED", .x)) %>% 
  dplyr::rename("Group" = "MEDsamples")

tmp_med_counts <- tmp_med_counts %>% 
  tibble::add_column(label = 0.03, .before = "Group") %>% 
  tibble::add_column(numOtus = tmp_n_meds, .after = "Group")

med_shared <- tmp_med_counts
```

```{r}
#| echo: false
#| eval: false
write.table(med_shared, here(work_here, "node_taxonomy/med_nodes.shared"),
            row.names = FALSE, quote = FALSE, sep = "\t")
```

```{r}
#| echo: false
#| eval: false
tmp_med_tax <- read_tsv(
    here(work_here, "node_taxonomy/med_nodes.gsrdb.wang.taxonomy"),
    col_names = FALSE)

tmp_1 <- med_counts[1:2]

med_cons_tax <- dplyr::left_join(
  tmp_1, tmp_med_tax, 
  by = c("Representative_Sequence" = "X1")) %>% 
  dplyr::rename(c("OTU" = 1, "Size" = 2, "Taxonomy" = 3))  

write.table(med_cons_tax,
            here(work_here, "node_taxonomy/med_nodes.cons.taxonomy"),
            row.names = FALSE, quote = FALSE, sep = "\t")
```

### A. Taxonomy Table

Here is what the taxonomy table looks like in the microeco mock data.

```{r}
#| eval: true
#| echo: false
pandoc.table(taxonomy_table_16S[10:12, 1:3], emphasize.rownames = FALSE)
```

Our taxonomy file (below) needs a little wrangling to be properly formatted. 

```{r}
#| echo: true
#| eval: false
tmp_tax <- read_delim(
             here(work_here, "node_taxonomy/med_nodes.cons.taxonomy"),
             delim = "\t")
```

```{r}
#| echo: false
#| eval: true
#tax1 <- tmp_tax
pandoc.table(tax1[11:13, 1:3], emphasize.rownames = FALSE, split.tables = Inf)
```

Some fancy string manipulation...

```{r}
tmp_tax <- data.frame(sapply(tmp_tax, 
                             gsub, 
                             pattern = "\\(\\d+\\)", 
                             replacement = ""))
tmp_tax <- data.frame(sapply(tmp_tax, 
                             gsub, 
                             pattern = ";$", 
                             replacement = ""))
tmp_tax <- separate_wider_delim(tmp_tax, 
                              cols = Taxonomy, 
                              delim = ";", names = c(
                                "Kingdom", "Phylum", 
                                "Class", "Order", 
                                "Family", "Genus"))
tmp_tax <- data.frame(sapply(tmp_tax, gsub, 
                           pattern = "^.*_unclassified$", 
                           replacement = ""))
tmp_tax$Size <- NULL
tmp_tax <- tibble::column_to_rownames(tmp_tax, "OTU")
```

And we get this ...

```{r}
#| echo: false
#| eval: true
#tax2 <- tmp_tax
pandoc.table(tax2[11:14, 1:4], emphasize.rownames = FALSE, split.tables = Inf)
```

```{r}
#| echo: true
#| eval: false
rank_prefixes <- c(
  Kingdom = "k", 
  Phylum  = "p", 
  Class   = "c", 
  Order   = "o", 
  Family  = "f", 
  Genus   = "g"
)

tmp_tax <- tmp_tax %>%
  mutate(across(everything(), ~replace_na(., ""))) %>% 
  mutate(across(names(rank_prefixes), 
                ~ paste0(rank_prefixes[cur_column()], "__", .))) %>%
tidy_taxonomy()
```

And then this. Exactly like the mock data. 

```{r}
#| echo: false
#| eval: true
#tax3 <- tmp_tax
pandoc.table(tax3[11:14, 1:4], emphasize.rownames = FALSE, split.tables = Inf)
```

### B. Sequence Table

Here is what the sequence table looks like in the mock data.

```{r}
#| eval: true
#| echo: false
pandoc.table(otu_table_16S[1:3, 1:11], emphasize.rownames = FALSE)
```

These code block will return a properly formatted sequence table. 

```{r}
#| echo: true
#| eval: false
tmp_st <- readr::read_delim(
  here(work_here, "node_taxonomy/med_nodes.shared"),
  delim = "\t")
```

```{r}
#| echo: true
#| eval: false
tmp_st$numOtus <- NULL
tmp_st$label <- NULL
tmp_st <- tmp_st %>%
  tidyr::pivot_longer(cols = c(-1), names_to = "tmp") %>%
  tidyr::pivot_wider(names_from = c(1))

tmp_st <- tibble::column_to_rownames(tmp_st, "tmp")
```

```{r}
#| eval: true
#| echo: false
# st <- tmp_st
pandoc.table(st[1:4, 1:3], emphasize.rownames = FALSE)
```

### C. Sample Table

Here is what the sample table looks like in the mock data.

```{r}
#| eval: true
#| echo: false
pandoc.table(sample_info_16S[1:3, 1:4], emphasize.rownames = FALSE, split.tables = Inf)
```

No problem. 

```{r}
#| echo: true
#| eval: false
samdf <- read.table(
  here("working_files/ssu/sampledata", "sample_data.txt"),
  header = TRUE, sep = "\t")

samdf <- samdf %>% tibble::column_to_rownames("SampleID")
samdf$SampleID <- rownames(samdf)
samdf <- samdf %>% relocate(SampleID)

samdf <- samdf %>%
  dplyr::filter(
    stringr::str_starts(SampleID, "Control", negate = TRUE))
```

```{r}
#| echo: false
#| eval: true
pandoc.table(samdf[11:13, 1:5], emphasize.rownames = FALSE, split.tables = Inf)
```

## Experiment-level Objects

In the following section we create `microtable` and `phyloseq` objects. These are single, consolidated experiment-level data objects that organize various data types, such as OTU tables (taxa abundance), sample metadata, taxonomic classifications, as well as phylogenetic trees and reference sequences, into one structured format. This unified approach facilitates reproducible, interactive analysis and visualization by bringing all related information together. 

We begin by creating the `microtable` and then use the function `meco2phyloseq` from the [file2meco](https://github.com/ChiLiubio/file2meco) package to create the `phyloseq` object. This way all of the underlying data is identical across the two objects. 

::: callout-note
These objects contain an OTU table (taxa abundances), sample metadata, and taxonomy table (mapping between OTUs and higher-level taxonomic classifications).
:::

```{r}
#| echo: true
#| eval: false
sample_info <- samdf
tax_tab <- tmp_tax
otu_tab <- tmp_st
```

```{r}
#| echo: true
#| eval: false
tmp_me <- microtable$new(sample_table = sample_info, 
                         otu_table = otu_tab, 
                         tax_table = tax_tab)
tmp_me
```

```         
microtable-class object:
sample_table have 1849 rows and 14 columns
otu_table have 436 rows and 1849 columns
tax_table have 436 rows and 6 columns
```

### Add Representative Sequence

The fasta file returned by MED needs a little T.L.C. For that we use a tool called SeqKit [@shen2024seqkit2] for fasta defline manipulation.

```{zsh}
seqkit replace -p ^ -r MED node-representatives.fa.txt > tmp1.fa
seqkit replace -p "\|.*" -r '' tmp1.fa > tmp2.fa 
seqkit replace -p "-" -r '$1' -s -w 0 tmp2.fa > med_rep.fasta
rm tmp1.fa
rm tmp2.fa
```

```{r}
#| echo: true
#| eval: false
rep_fasta <- Biostrings::readDNAStringSet(here(work_here, "med_rep.fasta"))
```

```{r}
#| echo: true
#| eval: false
tmp_me$rep_fasta <- rep_fasta
tmp_me$tidy_dataset()
tmp_me
me_raw <- microeco::clone(tmp_me)
```

## Curate the Data Set

Pretty much the last thing to do is remove low-count samples.

### Remove Low-Count Samples

```{r}
#| echo: true
#| eval: false
threshold <- 1000
tmp_no_low <- microeco::clone(me_raw)
tmp_no_low$otu_table <- me_raw$otu_table %>%
          dplyr::select(where(~ is.numeric(.) && sum(.) >= threshold))
tmp_no_low$tidy_dataset()
tmp_no_low
```

```{r}
#| echo: false
#| eval: false
me_no_low <- microeco::clone(tmp_no_low)
```

```{r}
#| echo: false
#| eval: true
me_no_low
```

```{r}
#| echo: true
#| eval: false
me_final <- microeco::clone(tmp_no_low)
```

Lastly, we can use the package [`file2meco`](https://github.com/ChiLiubio/file2meco) to generate a [phyloseq](https://joey711.github.io/phyloseq/) object. 

```{r}
#| echo: true
#| eval: false
ps_final <- file2meco::meco2phyloseq(me_final)
```

## Summary

Now time to summarize the data. For this we use the R package [miaverse](https://microbiome.github.io) [@felix2024mia].

```{r}
#| echo: false
#| eval: false
identical(rownames(me_raw$sample_table), colnames(me_raw$otu_table))
identical(rownames(me_final$sample_table), colnames(me_final$otu_table))
```

```{r}
#| echo: false
#| eval: false
objs <- c("me_raw", "me_final")
pipe_summary <- summarize_objs(objs)
pipe_summary$me_dataset <- c(
  "original", "no low count samps")
print(pipe_summary)
```

```{r}
#| echo: false
#| eval: true
knitr::kable(pipe_summary)
```

```{r}
#| echo: false
#| eval: false
tmp_raw_ps <- file2meco::meco2phyloseq(me_raw)
tmp_final_ps <- file2meco::meco2phyloseq(me_final)
tmp_mia_raw <- mia_metrics(tmp_raw_ps)
tmp_mia_final <- mia_metrics(tmp_final_ps)

tmp_metrics_final <- rbind(tmp_mia_raw, tmp_mia_final)
rownames(tmp_metrics_final) <- c("Start", "End")

# suppose ds_metrics_final has rownames "Start" and "End"
ds_metrics_tbl <- tmp_metrics_final %>%
  t() %>%                         # transpose: metrics in rows
  as.data.frame() %>%
  tibble::rownames_to_column("Metric") %>%
  dplyr::rename(Start = Start, End = End)

ds_metrics_tbl <- ds_metrics_tbl %>%
  mutate(
    Start = ifelse(Start %% 1 == 0,
                   formatC(Start, format = "f", digits = 0),
                   formatC(Start, format = "f", digits = 3)),
    End   = ifelse(End %% 1 == 0,
                   formatC(End, format = "f", digits = 0),
                   formatC(End, format = "f", digits = 3))
  )

ds_metrics_tbl$Metric <- c(
  "Min. no. of reads",
  "Max. no. of reads",
  "Total no. of reads",
  "Avg. no. of reads",
  "Median no. of reads",
  "Total ASVs",
  "No. of singleton ASVs",
  "% of singleton ASVs",
  "Sparsity"
)
```

```{r}
#| echo: false
#| eval: true
#| tbl-cap: "Dataset metrics before and after curation."
knitr::kable(ds_metrics_tbl, format = "markdown")
```

We started off with `r nrow(me_raw$tax_table)` ASVs and `r nrow(me_raw$sample_table)` samples. After removing low-count samples, there were `r nrow(me_final$tax_table)` ASVs and `r nrow(me_final$sample_table)` samples remaining.

```{r}
#| echo: false
#| eval: false
# helper functions
make_tables <- function(obj, prefix) {
  rc <- obj$sample_sums() %>%
    as.data.frame() %>%
    tibble::rownames_to_column("SampleID") %>%
    dplyr::rename(!!paste0(prefix, "_rc") := 2)

  asv <- obj$otu_table %>%
    t() %>%
    as.data.frame() %>%
    { rowSums(. > 0) } %>%
    as.data.frame() %>%
    tibble::rownames_to_column("SampleID") %>%
    dplyr::rename(!!paste0(prefix, "_med") := 2)

  list(rc = rc, asv = asv)
}
```

```{r}
#| echo: false
#| eval: false
#tmp_start <- microeco::clone(me_raw)
#tmp_final <- microeco::clone(me_final)
# --- input rc file ---
tmp_rc <- readr::read_delim(here(work_here, "mothur_med_pipeline_read_changes.txt")) 

# --- generate tables ---
start_tbls <- make_tables(microeco::clone(me_raw), "start")
final_tbls <- make_tables(microeco::clone(me_final), "final")

# --- summary ---
curate_summary <- tmp_rc %>%
  dplyr::full_join(start_tbls$rc,  by = "SampleID") %>%
  dplyr::left_join(start_tbls$asv, by = "SampleID") %>%
  dplyr::left_join(final_tbls$rc,  by = "SampleID") %>%
  dplyr::left_join(final_tbls$asv, by = "SampleID")

removed_samples <- curate_summary %>%
  dplyr::filter(is.na(final_rc)) %>%
  dplyr::pull(SampleID)
```

We lost a total of `r length(removed_samples)` samples after curating the dataset. This includes `r length(removed_samples[grepl("^Control_", removed_samples)])` NC samples and `r length(removed_samples[!grepl("^Control_", removed_samples)])` non-NC samples.

Here is a list of non-NC samples that were removed. 

```{r}
#| message: false
#| echo: false
#| eval: true
removed_samples[!grepl("^Control_", removed_samples)]
```

# Download Results {#download-results}

Quick access to read changes through the pipeline and repseqs detected in negative control samples.

::: {#med-listing .column-body}
:::

<!------------------------------------------------------------------------>
<!-------------------- Use this area to save things ---------------------->
<!------------------------------------------------------------------------>

<!--------------------------------------->
<!-- These chunks are for curated data -->
<!--------------------------------------->
```{r}
#| echo: false
#| eval: false
workflow_name <- "med"
dir.create(here(share_here, paste0(workflow_name, "_curated_data")), 
           recursive = FALSE, 
           showWarnings = TRUE)
```

```{r}
#| echo: false
#| eval: false
tmp_path <- here(share_here, paste0(workflow_name, "_curated_data/"))

files <- list(
  otu = paste0(workflow_name, "_otu_table.txt"),
  tax = paste0(workflow_name, "_tax_table.txt"),
  sample = paste0(workflow_name, "_sample_table.txt"),
  rep = paste0(workflow_name, "_rep.fasta"),
  me = paste0("me_", workflow_name, ".rds"),
  ps = paste0("ps_", workflow_name, ".rds"),
  counts = paste0(workflow_name, "_track_read_counts.txt")
)
```

```{r}
#| echo: false
#| eval: false
#---------------OTU Table--------------#
tmp_otu <- me_final$otu_table %>% 
  tibble::rownames_to_column(paste0(toupper(workflow_name), "_ID"))
write_delim(tmp_otu, paste0(tmp_path, files$otu), delim = "\t")
#---------------TAX Table--------------#
tmp_tax <- me_final$tax_table %>% 
  tibble::rownames_to_column(paste0(toupper(workflow_name), "_ID"))
write_delim(tmp_tax, paste0(tmp_path, files$tax), delim = "\t")
#---------------SAMP Table--------------#
write_delim(me_final$sample_table, paste0(tmp_path, files$sample), delim = "\t")
#---------------REP FASTA--------------#
write.fasta(
  sequences = as.list(as.character(me_final$rep_fasta)),
  names = names(me_final$rep_fasta),
  file.out = paste0(tmp_path, files$rep)
)
#---------------        --------------#
saveRDS(me_final, paste0(tmp_path, files$me))
ps_final <- file2meco::meco2phyloseq(me_final)
saveRDS(ps_final, paste0(tmp_path, files$ps))
#---------------read count changes--------------#
write_delim(curate_summary, paste0(tmp_path, files$counts), delim = "\t")
file.copy(here("working_files/ssu/sampledata",  "all_metadata.txt"), 
          tmp_path, 
          overwrite = TRUE, recursive = FALSE, 
          copy.mode = TRUE, copy.date = FALSE)
```

```{r}
#| echo: false
#| eval: false
zip::zip(zipfile = here(share_here, paste0(workflow_name, "_curated_data.zip")), 
         files = here(share_here, paste0(workflow_name,"_curated_data")), 
         mode = "cherry-pick")
```

```{r}
#| echo: false
#| eval: false
file.copy(here(work_here,  "mothur_med_pipeline_read_changes.txt"), 
          here(share_here), 
          overwrite = TRUE, recursive = FALSE, 
          copy.mode = TRUE, copy.date = FALSE)
```

```{r}
#| echo: false
#| eval: false
fs::dir_copy(path = here(work_here,  "med_results/"), 
             new_path = here(share_here,  "med_results/"), 
             overwrite = TRUE)
zip::zip(zipfile = here(share_here, "med_results.zip"), 
         files = here(share_here, "med_results"), 
         mode = "cherry-pick")
```

<!-------------------------------------------->
<!-- These chunks are for  processing data  -->
<!-------------------------------------------->

```{r}
#| echo: false
#| eval: false
dir.create(here(share_here, paste0(workflow_name, "_processing")), 
           recursive = FALSE, showWarnings = TRUE)
copy_here <- here(share_here, paste0(workflow_name, "_processing/"))
```

```{r}
#| echo: false
#| eval: false
#| comment: from sampledata (these data are common to all 16S wf)
tmp_sampdata_path <- "working_files/ssu/sampledata"
fs::dir_copy(path = here(tmp_sampdata_path,  "fastq_rename_results/"), 
             new_path = here(copy_here,  "fastq_rename_results/"), 
             overwrite = TRUE)

fs::dir_copy(path = here(tmp_sampdata_path,  "fastq_rename_lookup/"), 
             new_path = here(copy_here,  "fastq_rename_lookup/"), 
             overwrite = TRUE)
file.copy(here(tmp_sampdata_path,  "rename.sh"), 
          here(copy_here), 
          overwrite = TRUE, recursive = FALSE, 
          copy.mode = TRUE, copy.date = FALSE)
```


```{r}
#| echo: false
#| eval: false
fs::dir_copy(path = here(work_here,  "mothur2oligo/"), 
             new_path = here(copy_here,  "mothur2oligo/"), 
             overwrite = TRUE)

fs::dir_copy(path = here(work_here,  paste0(workflow_name, "_hydra_scripts/")), 
             new_path = here(copy_here,  paste0(workflow_name, "_hydra_scripts/")), 
             overwrite = TRUE)

file.copy(here(work_here,  paste0(workflow_name, "_batchfile_processing/")), 
          copy_here, 
          overwrite = TRUE, recursive = FALSE, 
          copy.mode = TRUE, copy.date = FALSE)

file.copy(here(work_here,  "med_mapping.txt"), 
          copy_here, 
          overwrite = TRUE, recursive = FALSE, 
          copy.mode = TRUE, copy.date = FALSE)

zip::zip(zipfile = here(share_here, "med_results.zip"), 
         files = here(share_here, "med_results"), 
         mode = "cherry-pick")

file.copy(here(share_here,  "mothur_med_pipeline_read_changes.txt"), 
          here(copy_here), 
          overwrite = TRUE, recursive = FALSE, 
          copy.mode = TRUE, copy.date = FALSE)

file.copy(here(tmp_sampdata_path,  "all_metadata.txt"), 
          here(copy_here), 
          overwrite = TRUE, recursive = FALSE, 
          copy.mode = TRUE, copy.date = FALSE)
```

```{r}
#| echo: false
#| eval: false
zip::zip(zipfile = here(share_here, paste0(workflow_name, "_processing.zip")), 
         files = here(share_here, paste0(workflow_name,"_processing")), 
         mode = "cherry-pick")
dir_delete(here(share_here, paste0(workflow_name,"_processing")))
```

```{r}
#| echo: false
#| eval: true
#| message: false
#| include: false
#| comment: save R script
workflow_name <- "med"
options(knitr.duplicate.label = "allow")
# Define sources and numbered outputs
sources <- c(
  "include/_MED_Part1.qmd",
  "index.qmd"
)
# Generate corresponding temporary output files
tmp_outputs <- here("share/ssu", paste0(workflow_name, "_workflow", seq_along(sources), ".R"))
# Final combined output
final_output <- here(share_here, paste0(workflow_name, "_workflow.R"))
# Step 1: Extract code from qmd to temporary R scripts
purrr::walk2(sources, tmp_outputs, ~
  knitr::purl(.x, output = .y, documentation = 0)
)
# Step 2: Concatenate all temporary R scripts into one, with separators
sink(final_output)
purrr::walk2(tmp_outputs, seq_along(tmp_outputs), ~{
  cat(readLines(.x), sep = "\n")
  cat("\n\n",
      paste0("# -------------------- END OF SCRIPT ", .y, " --------------------\n\n"),
      sep = "")
})
sink()
# Step 3: Clean up temporary files
file.remove(tmp_outputs)
```

```{r}
#| echo: false
#| eval: false
objects()
gdata::keep(removed_samples, ds_metrics_tbl, pipe_summary, 
            me_final, me_no_low, me_raw, curate_summary,
            samdf, tax1, tax2, tax3, st,
            sure = TRUE)
save.image(here("page_build", "med_part2.rdata"))
```

#### References {.appendix}

::: {#refs}
:::

{{< include /include/_footer.qmd >}}
