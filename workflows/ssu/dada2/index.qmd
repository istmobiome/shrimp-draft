---
title: "2. DADA2 ASV Workflow"
description: |
  Workflow for processing 16S rRNA samples for ASV analysis using DADA2. Workflow uses paired end reads, beginning with raw fastq files, ending with sequence and taxonomy tables. A Microtable Object is produced to collate the data for downstream analysis. 
listing: 
    id: dada2-listing
    contents: data-dada2.yml
    type: table
    sort-ui: false
    filter-ui: false
    fields: 
      - filename
      - description
    field-links: 
      - filename
    field-display-names: 
      filename: File Name
      description: Description
---

{{< include /include/_setup.qmd >}}

```{r}
#| eval: true
#| echo: false
#| comment: set wd for here
remove(list = ls())
load(here("page_build", "dada2_part2.rdata"))
workflow_name <- "dada2" # e.g., med, dada2, etc
work_here <- paste0("working_files/ssu/", workflow_name)
share_here <- paste0("share/ssu/", workflow_name)
source(here("assets/scripts", "summarize_objs.R"))
source(here("assets/scripts", "mia_metrics.R"))
```

## Workflow Input

::: {.callout-note icon=false}
## Data & Scripts

Fastq sequence files, scripts, and other assets for running this workflow can be found on the [Dada2 Data Portal](/workflows/portal/data-dada2.qmd) page. 

The Data Portal page also contains a link to the curated output of this pipeline–-feature-sample count table, taxonomy table, sample metadata, & representative fasta sequences. Data is available as stand-alone text files, OR bundled in microtable and phyloseq objects. Archive also includes a table tracking sample read changes.
:::

#### Required Packages & Software

There are several R packages you need to run this workflow. 

```{r}
#| eval: false 
#| echo: true
#| message: false
#| results: hide
#| code-fold: true
#| code-summary: "Click here for workflow library information."
#!/usr/bin/env Rscript
set.seed(919191)
pacman::p_load(tidyverse, gridExtra, grid, phyloseq,
               formatR, gdata, ff, decontam, dada2, 
               ShortRead, Biostrings, DECIPHER, 
               install = FALSE, update = FALSE)
```

## Overview

This workflow contains the code we used to process the 16S rRNA data sets using [DADA2](https://benjjneb.github.io/dada2/) [@callahan2016dada2]. Workflow construction is based on the [DADA2 Pipeline Tutorial (1.8)](https://benjjneb.github.io/dada2/tutorial_1_8.html) and the primer identification section of the [DADA2 ITS Pipeline Workflow (1.8)](https://benjjneb.github.io/dada2/ITS_workflow.html). In the first part of the pipeline, we process the individual sequencing runs **separately**. Next we combine the sequence tables from each run  into one merged sequences table and continue with processing.

# Read Processing {#read-processing}

We processed each of the six sequencing runs separately for the first part of the DADA2 workflow. While some of the outputs are slightly different (e.g. quality scores, filtering, ASV inference, etc.) the code is the same. For posterity, code for each run is included here.

## Individual Run Workflows

The first part of the workflow consists of the following steps for each of the runs:

| Step | Command                | What we’re doing                       |
|------|------------------------|----------------------------------------|
| 1    | multiple               | prepare input file names & paths       |
| 2    | multiple               | Define primers (all orientations)      |
| 3    | `cutadapt`             | Remove primers                         |
| 4    | `plotQualityProfile()` | Plot quality scores.                   |
| 5    | `filterAndTrim()`      | Assess quality & filter reads          |
| 6    | `learnErrors()`        | Generate an error model for the data   |
| 7    | `derepFastq()`         | Dereplicate sequences                  |
| 8    | `dada()`               | Infer ASVs (forward & reverse reads).  |
| 9    | `mergePairs()`.        | Merge denoised forward & reverse reads |
| 10   | `makeSequenceTable()`  | Generate count table for each run      |
| 11   |                        | Track reads through workflow           |

{{< include include/_dada2_flowchart_1.qmd >}}

<br/>

::: panel-tabset
## BCS_26

{{< include include/_BCS_26.qmd >}}

## BCS_28

{{< include include/_BCS_28.qmd >}}

## BCS_29

{{< include include/_BCS_29.qmd >}}

## BCS_30

{{< include include/_BCS_30.qmd >}}

## BCS_34

{{< include include/_BCS_34.qmd >}}

## BCS_35

{{< include include/_BCS_35.qmd >}}
:::

## Merged Runs Workflow

Now it is time to combine the sequence tables from each run together into one merged sequences table.

{{< include include/_MERGE_RUNS.qmd >}}

# Data Set Prep

```{r}
#| eval: false
#| echo: false
#| message: false 
#| results: hide
seqtab <- readRDS(here(work_here, "rdata/3.seqtab.trim.nochim.consensus.rds"))
tax_gsrdb <- readRDS(here(work_here, "rdata/4.tax_gsrdb.consensus.rds"))
```

In this next part of the workflow our main goal is to create a *microtable object* using the R package [microeco](https://joey711.github.io/phyloseq/) [@liu2021microeco]. The microtable will be used to store the ASV by sample data as well the taxonomic, fasta, and sample data in a single object. More on that in a moment.

We will also:

-   Remove any ASVs without kingdom level classification.\
-   Revome any contaminants (chloroplast, mitochondria, etc.).\
-   Remove Negative Control (NC) samples.\
-   Remove any low-count samples.

## Read Counts Assessment

Before we begin, let's create a summary table containing some basic sample metadata and the read count data from the [Sample Data](/sampledata/index.html) section of the workflow. We want to inspect how total reads changed through the workflow. Table headers are as follows:

| Header             | Description                                                |
|------------------------|------------------------------------------------|
| `Sample ID`        | New sample ID based on Ocean, species, tissue, & unique ID |
| `input rc`         | No. of raw reads                                           |
| `final rc`         | Final read count after removing chimeras                   |
| `per reads retain` | Percent of reads remaining from `input` to `final rc`      |
| `total ASVs`       | No. of ASVs                                                |
| `Ocean`            | Sampling ocean                                             |
| `Morphospecies`    | Host shrimp species                                        |
| `Tissue`           | Shrimp tissue type                                         |
| `Habitat`          | Sampling habitat                                           |
| `Site`             | Sampling site                                              |
| `Taxon`            | Shrimp, environmental samples, Controls                    |
| `Species_Pair`     | ASK MATT                                                   |
| `Species_group`    | ASK MATT                                                   |
| `Species_complex`  | ASK MATT                                                   |
| `Run`              | Sequencing Run                                             |
| `Plate`            | Plate name                                                 |

<br/>

```{r}
#| echo: false
#| eval: false
tmp_tab1 <- readRDS(here("working_files/ssu", "sampledata/sample_data.rds"))
tmp_tab2 <- read.table(here(work_here,
    "dada2_pipeline_read_changes.txt"),
    header = TRUE, sep = "\t"
)
tmp_tab2[3:7] <- NULL
tmp_tab1 <- arrange(tmp_tab1, SampleID, .by_group = FALSE)
tmp_tab2 <- arrange(tmp_tab2, SampleID, .by_group = FALSE)
identical(tmp_tab1$SampleID, tmp_tab2$SampleID)
```


```{r}
#| echo: false
#| eval: false
tmp_tab3 <- data.frame(row.names(seqtab))
colnames(tmp_tab3) <- "SampleID"

tmp_seqtab <- data.frame(seqtab)
tmp_seqtab <- tibble::rownames_to_column(tmp_seqtab, var = "SampleID")

identical(tmp_tab3$SampleID, tmp_seqtab$SampleID)
```

```{r}
#| echo: false
#| eval: false
tmp_seqtab <- tmp_seqtab %>%
    mutate(count = rowSums(. != 0))
tmp_tab3$no_asvs <- tmp_seqtab$count
```

```{r}
#| echo: false
#| eval: false
tmp_tab4 <- dplyr::right_join(tmp_tab1, tmp_tab2, by = "SampleID")

tmp_tab2 <- tmp_tab2[order(tmp_tab2$SampleID),]
tmp_tab4 <- tmp_tab4[order(tmp_tab4$SampleID),]

identical(tmp_tab2$input, tmp_tab4$input)
identical(tmp_tab2$SampleID, tmp_tab4$SampleID)

tmp_tab3 <- tmp_tab3[order(tmp_tab3$SampleID),]

tmp_tab5 <- dplyr::left_join(tmp_tab4, tmp_tab3, by = "SampleID")
tmp_tab5$per_reads_kept <- round(tmp_tab5$nochim/tmp_tab5$input, digits = 3)
samptab <- tmp_tab5
```

```{r}
#| echo: false
#| eval: false
samptab <- samptab %>%
    dplyr::relocate(
        c(input, nochim, per_reads_kept, no_asvs),
        .after = "SampleID"
    )
samptab <- samptab %>%
    dplyr::relocate(per_reads_kept, .after = "nochim")

rm(list = ls(pattern = "tmp_"))
```

## Prep Data for `microeco`

Like any tool, the microeco package needs the data in a specific form. I formatted our data to match the mock data in the microeco tutorial, specifically [this section](https://chiliubio.github.io/microeco_tutorial/basic-class.html#prepare-the-example-data).

### A. Taxonomy Table

Here is what the taxonomy table looks like in the mock data.

```{r}
#| eval: true
#| echo: false
pandoc.table(taxonomy_table_16S[10:12, 1:3], emphasize.rownames = FALSE)
```

And here is the taxonomy table from the dada2 workflow. The name dada2 gives to each ASV is the actual sequence of the ASV. We need to change this for downstream analyses.

::: {.callout-note appearance="simple" collapse="true"}

### Expand to see the partial content of the dada2 taxonomy table

```{r}
#| eval: true
#| echo: false
pandoc.table(data.frame(tax_gsrdb)[1:3, 1:3], emphasize.rownames = FALSE)
```

:::

The first step is to rename the amplicon sequence variants so the designations are a bit more user friendly. By default, DADA2 names each ASV by its unique sequence so that data can be directly compared across studies (which is great). But this convention can get cumbersome downstream, so we rename the ASVs using a simpler convention---ASV1, ASV2, ASV3, and so on.

```{r}
#| echo: true
#| eval: false
tmp_tax <- data.frame(tax_gsrdb)
# adding unique ASV names
row.names(tmp_tax) <- paste0("ASV", seq(nrow(tmp_tax)))
```

And this is how the taxonomy table looks after assigning new names.

```{r}
#| echo: false
#| eval: false
tax.head1 <- tmp_tax[1:3, 1:3]
```

```{r}
#| echo: false
#| eval: true
pandoc.table(tax.head1, emphasize.rownames = FALSE)
```

Next we need to add rank definitions to each classification.

```{r}
#| echo: true
#| eval: false
rank_prefixes <- c(
  Kingdom = "k", 
  Phylum  = "p", 
  Class   = "c", 
  Order   = "o", 
  Family  = "f", 
  Genus   = "g"
)

tmp_tax <- tmp_tax %>%
  mutate(across(everything(), ~replace_na(., ""))) %>% 
  mutate(across(names(rank_prefixes), 
                ~ paste0(rank_prefixes[cur_column()], "__", .))) %>%
tidy_taxonomy()
```

And then this. Exactly like the mock data. 

```{r}
#| echo: false
#| eval: false
tax.head2 <- tmp_tax[1:3, 1:3]
```

```{r}
#| echo: false
#| eval: true
pandoc.table(tax.head2, emphasize.rownames = FALSE)
```

### B. Sequence Table

Here is what the sequence table looks like in the mock data.

```{r}
#| eval: true
#| echo: false
pandoc.table(otu_table_16S[1:3, 1:11], emphasize.rownames = FALSE)
```

```{r}
#| echo: true
#| eval: false
tmp_st <- data.frame(seqtab)
identical(colnames(tmp_st), row.names(tax_gsrdb))
names(tmp_st) <- row.names(tmp_tax)

tmp_st <-  tmp_st %>% tibble::rownames_to_column()

tmp_st <- tmp_st %>%
  tidyr::pivot_longer(cols = c(-1), names_to = "tmp") %>%
  tidyr::pivot_wider(names_from = c(1))
tmp_st <- tibble::column_to_rownames(tmp_st, "tmp")
```

```{r}
#| echo: false
#| eval: false
st.head <- tmp_st[1:3, 61:63]
```

And now the final, modified sequence table.

```{r}
#| eval: true
#| echo: false
pandoc.table(st.head, emphasize.rownames = FALSE)
```

### C. Sample Table

Here is what the sample table looks like in the mock data.

```{r}
#| eval: true
#| echo: true
pandoc.table(sample_info_16S[1:3,], emphasize.rownames = FALSE)
```

```{r}
#| echo: true
#| eval: false
samdf <- readRDS(here("share/ssu/sampledata/", "sample_data.rds"))
samdf <- samdf %>% tibble::column_to_rownames("SampleID")
samdf$SampleID <- rownames(samdf)
samdf <- samdf %>% relocate(SampleID)
```

And now a partial view of the final, modified sample table.

```{r}
#| echo: false
#| eval: true
pandoc.table(samdf[61:63, 1:5], emphasize.rownames = FALSE)
```

## Experiment-level Objects

In the following section we create `microtable` and `phyloseq` objects. These are single, consolidated experiment-level data objects that organize various data types, such as OTU tables (taxa abundance), sample metadata, taxonomic classifications, as well as phylogenetic trees and reference sequences, into one structured format. This unified approach facilitates reproducible, interactive analysis and visualization by bringing all related information together. 

We begin by creating the `microtable` and then use the function `meco2phyloseq` from the [file2meco](https://github.com/ChiLiubio/file2meco) package to create the `phyloseq` object. This way all of the underlying data is identical across the two objects. 

::: callout-note
These objects contain an ASV table (taxa abundances), sample metadata, and taxonomy table (mapping between ASVs and higher-level taxonomic classifications).
:::

```{r}
#| echo: true
#| eval: false
sample_info <- samdf
tax_tab <- tmp_tax
otu_tab <- tmp_st
```

```{r}
#| echo: true
#| eval: false
tmp_me <- microtable$new(sample_table = sample_info, 
                         otu_table = otu_tab, 
                         tax_table = tax_tab)
tmp_me
```

```         
microtable-class object:
sample_table have 1909 rows and 13 columns
otu_table have 72851 rows and 1909 columns
tax_table have 72851 rows and 6 columns
```

### Add Representative Sequence

We can also add representative sequences for each OTU/ASV. For this step, we can simply grab the sequences from the row names of the DADA2 taxonomy object loaded above.

```{r}
#| echo: true
#| eval: false
tmp_seq <- data.frame(row.names(data.frame(tax_gsrdb)) )
tmp_names <- data.frame(row.names(tax_tab))
tmp_fa <- cbind(tmp_names, tmp_seq)
colnames(tmp_fa) <- c("ASV_ID", "ASV_SEQ")
tmp_fa$ASV_ID <- sub("^", ">", tmp_fa$ASV_ID)

write.table(tmp_fa, here(work_here, "rep_seq.fasta"),
            sep = "\n", col.names = FALSE, row.names = FALSE,
            quote = FALSE, fileEncoding = "UTF-8")       
rep_fasta <- Biostrings::readDNAStringSet(here(work_here, "rep_seq.fasta"))
tmp_me$rep_fasta <- rep_fasta
tmp_me$tidy_dataset()
```

```{r}
#| echo: false
#| eval: false
me_raw <- microeco::clone(tmp_me)
```

## Curate the Data Set

Pretty much the last thing to do is remove unwanted taxa, negative controls, and low-count samples.

### Remove any Kingdom NAs

Here we can just use the straight up `subset` command since we do not need to worry about any ranks above Kingdom also being removed.

```{r}
#| echo: true
#| eval: false
tmp_no_na <- microeco::clone(tmp_me)
tmp_no_na$tax_table %<>% base::subset(Kingdom == "k__Archaea" | Kingdom == "k__Bacteria")
tmp_no_na$tidy_dataset()
```

```{r}
#| echo: false
#| eval: false
me_no_na <- microeco::clone(tmp_no_na)
```

```{r}
#| echo: false
#| eval: true
me_no_na
```

### Remove Contaminants

Now we can remove any potential contaminants like mitochondria or chloroplasts.

```{r}
#| echo: true
#| eval: false
tmp_no_cont <- microeco::clone(tmp_no_na)
tmp_no_cont$filter_pollution(taxa = c("mitochondria", "chloroplast"))
tmp_no_cont$tidy_dataset()
tmp_no_cont
```

```         
Total 0 features are removed from tax_table ...
```

```{r}
#| echo: false
#| eval: false
me_no_cont <- microeco::clone(tmp_no_cont)
```

```{r}
#| echo: false
#| eval: true
me_no_cont
```

### Remove Negative Controls (NC)

Now we need to remove the NC samples *and* ASVs found in those sample. We first identified all ASVs that were present in at least one NC sample represented by at least 1 read. We did this by subsetting the NC samples from the new microtable object.

```{r}
#| echo: true
#| eval: false
tmp_nc <- microeco::clone(tmp_no_cont)
tmp_nc$sample_table <- subset(tmp_nc$sample_table, TAXON == "Control")
tmp_nc$tidy_dataset()
tmp_nc
```

```
72231 taxa with 0 abundance are removed from the otu_table ...
```

```{r}
#| echo: false
#| eval: false
me_nc <- microeco::clone(tmp_nc)
```

```{r}
#| echo: false
#| eval: true
me_nc
```

Looks like there are `r nrow(me_nc$tax_table)` ASVs in the NC samples from a total of `r sum(me_nc$taxa_sums())` reads.

```{r}
#| echo: true
#| eval: false
nc_asvs <- row.names(tmp_nc$tax_table)
```

```{r}
#| echo: false
#| eval: true
head(nc_asvs, n = 20)
```

There are `r length(nc_asvs)` ASVs found in the NC sample. ASVs are numbered in order by total abundance in the data set so we know that many of the ASVs in the NC samples are not particularly abundant in the dataset. We can look at the abundance of these ASVs across all samples and compare it to the NC. This takes a bit of wrangling.

Essentially, for each ASV, the code below calculates:

-   The total number of NC samples containing at least 1 read.\
-   The total number of reads in NC samples.\
-   The total number of non-NC samples containing at least 1 read.\
-   The total number of reads in non-NC samples.\
-   The percent of reads in the NC samples and the percent of NC samples containing reads.

```{r}
#| echo: true
#| eval: false
tmp_rem_nc <- microeco::clone(tmp_no_cont)
tmp_rem_nc_df <- tmp_rem_nc$otu_table
tmp_rem_nc_df <- tmp_rem_nc_df %>% 
                 dplyr::filter(row.names(tmp_rem_nc_df) %in% nc_asvs)
tmp_rem_nc_df <- tmp_rem_nc_df %>% tibble::rownames_to_column("ASV_ID")
```

```{r}
#| echo: true
#| eval: false
#-------provide a string unique to NC samples--------------#
nc_name <- "Control_"
#----------------------------------------------------------#
tmp_rem_nc_df <- tmp_rem_nc_df  %>% 
  dplyr::mutate(total_reads_NC = rowSums(dplyr::select(., contains(nc_name))), 
         .after = "ASV_ID")
tmp_rem_nc_df <- dplyr::select(tmp_rem_nc_df, -contains(nc_name))
tmp_rem_nc_df <- tmp_rem_nc_df %>%
  dplyr::mutate(total_reads_samps = rowSums(.[3:ncol(tmp_rem_nc_df)]), 
                .after = "total_reads_NC")
tmp_rem_nc_df[, 4:ncol(tmp_rem_nc_df)] <- list(NULL)
tmp_rem_nc_df <- tmp_rem_nc_df %>%
  dplyr::mutate(perc_in_neg = 100*(
    total_reads_NC / (total_reads_NC + total_reads_samps)),
                .after = "total_reads_samps")
```

```{r}
#| echo: true
#| eval: false
tmp_rem_nc_df$perc_in_neg <- round(tmp_rem_nc_df$perc_in_neg, digits = 6)

tmp_1 <- data.frame(rowSums(tmp_rem_nc$otu_table != 0)) %>% 
                   tibble::rownames_to_column("ASV_ID") %>% 
                   dplyr::rename("total_samples" = 2) 

tmp_2 <- dplyr::select(tmp_rem_nc$otu_table, contains(nc_name))
tmp_2$num_samp_nc <- rowSums(tmp_2 != 0)
tmp_2 <- dplyr::select(tmp_2, contains("num_samp_nc")) %>% 
                      tibble::rownames_to_column("ASV_ID")

tmp_3 <- dplyr::select(tmp_rem_nc$otu_table, -contains(nc_name))
tmp_3$num_samp_no_nc <- rowSums(tmp_3 != 0)
tmp_3 <- dplyr::select(tmp_3, contains("num_samp_no_nc")) %>% 
                      tibble::rownames_to_column("ASV_ID")

tmp_rem_nc_df <- dplyr::left_join(tmp_rem_nc_df, tmp_1) %>%
                 dplyr::left_join(., tmp_2) %>%
                 dplyr::left_join(., tmp_3)

tmp_rem_nc_df <- tmp_rem_nc_df %>%
  dplyr::mutate(perc_in_neg_samp = 100*( num_samp_nc / (num_samp_nc + num_samp_no_nc)),
                .after = "num_samp_no_nc")
```

```{r}
#| echo: true
#| eval: false
nc_check <- tmp_rem_nc_df
```

Looking at these data we can see that ASVs like ASV1, ASV2, and so on, are only represented by a really small number of NC reads and samples. On the other hand, ASVs such as ASV91, ASV121, and ASV299 are relatively abundant in NC samples. We decided to remove ASVs if:

-   The number of reads found in NC samples accounted for more than 10% of total reads OR
-   The percent of NC samples containing the ASV was greater than 10% of total samples.

```{r}
#| echo: true
#| eval: false
nc_remove <- nc_check %>% 
  dplyr::filter(perc_in_neg > 10 | perc_in_neg_samp > 10)
```

```{r}
#| echo: false
#| eval: false
nc_remain <- dplyr::anti_join(nc_check, nc_remove)

rem_nc_reads <- sum(nc_remove$total_reads_NC)
rem_sam_reads <- sum(nc_remove$total_reads_samps)
per_reads_rem <- round(100*( rem_nc_reads / (rem_nc_reads + rem_sam_reads)), 
                       digits = 3)

ret_nc_reads <- sum(nc_remain$total_reads_NC)
ret_sam_reads <- sum(nc_remain$total_reads_samps)
per_reads_ret <- round(100*( ret_nc_reads / (ret_nc_reads + ret_sam_reads)), 
                       digits = 3)
```

|          | Total ASVs          | NC reads         | non NC reads      | \% NC reads       |
|----------|---------------------|------------------|-------------------|-------------------|
| Removed  | `r nrow(nc_remove)` | `r rem_nc_reads` | `r rem_sam_reads` | `r per_reads_rem` |
| Retained | `r nrow(nc_remain)` | `r ret_nc_reads` | `r ret_sam_reads` | `r per_reads_ret` |

We identified a total of `r nrow(nc_check)` ASVs that were present in at least 1 NC sample by at least 1 read. We removed any ASV where more than 10% of total reads were found in NC samples OR any ASV found in more than 10% of NC samples. Based on these criteria we removed `r nrow(nc_remove)` ASVs from the data set, which represented `r rem_nc_reads` total reads in NC samples and `r rem_sam_reads` total reads in non-NC samples. Of the total reads removed `r per_reads_rem`% came from NC samples. Of all ASVs identified in NC samples,`r nrow(nc_remain)` were retained because the fell below the threshhold criteria. These ASVs accounted for `r ret_nc_reads` reads in NC samples and `r ret_sam_reads` reads in non-NC samples. NC samples accounted for `r per_reads_ret`% of these reads.

OK, now we can remove the NC samples and any ASVs that met our criteria described above.


```{r}
#| echo: true
#| eval: false
tmp_no_nc <- microeco::clone(tmp_no_cont)

tmp_rem_asv <- as.factor(nc_remove$ASV_ID)
tmp_no_nc$otu_table <- tmp_rem_nc$otu_table %>% 
  filter(!row.names(tmp_no_nc$otu_table) %in% tmp_rem_asv)
tmp_no_nc$tidy_dataset()

tmp_no_nc$sample_table <- subset(tmp_no_nc$sample_table, 
                                 TAXON != "Control_")
tmp_no_nc$tidy_dataset()
tmp_no_nc
```

```         
9 samples with 0 abundance are removed from the otu_table ...
```

```{r}
#| echo: false
#| eval: false
me_no_nc <- microeco::clone(tmp_no_nc)
```

```{r}
#| echo: false
#| eval: true
me_no_nc
```

### Remove Low-Count Samples

Next, we can remove samples with really low read counts---here we set the threshold to `1000` reads.

```{r}
#| echo: true
#| eval: false
tmp_no_low <- microeco::clone(tmp_no_nc)
tmp_no_low$otu_table <- tmp_no_nc$otu_table %>%
          dplyr::select(where(~ is.numeric(.) && sum(.) >= 1000))
tmp_no_low$tidy_dataset()
tmp_no_low
```

```         
26 taxa with 0 abundance are removed from the otu_table ...
```

```{r}
#| echo: false
#| eval: false
me_no_low <- microeco::clone(tmp_no_low)
```

```{r}
#| echo: false
#| eval: true
me_no_low
```

Giving us the final microtable object.

```{r}
#| echo: true
#| eval: false
me_final <- microeco::clone(tmp_no_low)
```

Lastly, we can use the package [`file2meco`](https://github.com/ChiLiubio/file2meco) to generate a [phyloseq](https://joey711.github.io/phyloseq/) object. 

```{r}
#| echo: true
#| eval: false
ps_final <- file2meco::meco2phyloseq(me_final)
```

## Summary

Now time to summarize the data. For this we use the R package [miaverse](https://microbiome.github.io) [@felix2024mia].

```{r}
#| echo: false
#| eval: false
identical(rownames(me_raw$sample_table), colnames(me_raw$otu_table))
identical(rownames(me_final$sample_table), colnames(me_final$otu_table))
```

```{r}
#| echo: false
#| eval: false
objs <- c("me_raw", "me_no_na", 
          "me_no_cont", "me_no_nc", 
          "me_no_low", "me_final")
pipe_summary <- summarize_objs(objs)
pipe_summary$me_dataset <- c(
  "original", "no NA kingdoms", 
  "no contaminants", "no negative controls", 
  "no low count samps", "final")
print(pipe_summary)
```

```{r}
#| echo: false
#| eval: true
knitr::kable(pipe_summary)
```

```{r}
#| echo: false
#| eval: false
tmp_raw_ps <- file2meco::meco2phyloseq(me_raw)
tmp_final_ps <- file2meco::meco2phyloseq(me_final)
tmp_mia_raw <- mia_metrics(tmp_raw_ps)
tmp_mia_final <- mia_metrics(tmp_final_ps)

tmp_metrics_final <- rbind(tmp_mia_raw, tmp_mia_final)
rownames(tmp_metrics_final) <- c("Start", "End")

# suppose ds_metrics_final has rownames "Start" and "End"
ds_metrics_tbl <- tmp_metrics_final %>%
  t() %>%                         # transpose: metrics in rows
  as.data.frame() %>%
  tibble::rownames_to_column("Metric") %>%
  dplyr::rename(Start = Start, End = End)

ds_metrics_tbl <- ds_metrics_tbl %>%
  mutate(
    Start = ifelse(Start %% 1 == 0,
                   formatC(Start, format = "f", digits = 0),
                   formatC(Start, format = "f", digits = 3)),
    End   = ifelse(End %% 1 == 0,
                   formatC(End, format = "f", digits = 0),
                   formatC(End, format = "f", digits = 3))
  )

ds_metrics_tbl$Metric <- c(
  "Min. no. of reads",
  "Max. no. of reads",
  "Total no. of reads",
  "Avg. no. of reads",
  "Median no. of reads",
  "Total ASVs",
  "No. of singleton ASVs",
  "% of singleton ASVs",
  "Sparsity"
)
```

```{r}
#| echo: false
#| eval: true
#| tbl-cap: "Dataset metrics before and after curation."
knitr::kable(ds_metrics_tbl, format = "markdown")
```

We started off with `r nrow(me_raw$tax_table)` ASVs and `r nrow(me_raw$sample_table)` samples. After curation of the dataset, there were `r nrow(me_final$tax_table)` ASVs and `r nrow(me_final$sample_table)` samples remaining.

```{r}
#| echo: false
#| eval: false
# helper functions
make_tables <- function(obj, prefix) {
  rc <- obj$sample_sums() %>%
    as.data.frame() %>%
    tibble::rownames_to_column("SampleID") %>%
    dplyr::rename(!!paste0(prefix, "_rc") := 2)

  asv <- obj$otu_table %>%
    t() %>%
    as.data.frame() %>%
    { rowSums(. > 0) } %>%
    as.data.frame() %>%
    tibble::rownames_to_column("SampleID") %>%
    dplyr::rename(!!paste0(prefix, "_asv") := 2)

  list(rc = rc, asv = asv)
}
```

```{r}
#| echo: false
#| eval: false
# --- input rc file ---
tmp_rc <- readr::read_delim(here(work_here, "dada2_pipeline_read_changes.txt")) 

# --- generate tables ---
start_tbls <- make_tables(microeco::clone(me_raw), "start")
final_tbls <- make_tables(microeco::clone(me_final), "final")

# --- summary ---
curate_summary <- tmp_rc %>%
  dplyr::full_join(start_tbls$rc,  by = "SampleID") %>%
  dplyr::left_join(start_tbls$asv, by = "SampleID") %>%
  dplyr::left_join(final_tbls$rc,  by = "SampleID") %>%
  dplyr::left_join(final_tbls$asv, by = "SampleID")

removed_samples <- curate_summary %>%
  dplyr::filter(is.na(final_rc)) %>%
  dplyr::pull(SampleID)
```

We lost a total of `r length(removed_samples)` samples after curating the dataset. This includes `r length(removed_samples[grepl("^Control_", removed_samples)])` NC samples and `r length(removed_samples[!grepl("^Control_", removed_samples)])` non-NC samples.

Here is a list of non-NC samples that were removed. 

```{r}
#| message: false
#| echo: false
#| eval: true
removed_samples[!grepl("^Control_", removed_samples)]
```

# Download Results {#download-results}

Quick access to read changes through the pipeline and ASVs detected in negative control samples.

::: {#dada2-listing .column-body}
:::

<!------------------------------------------------------------------------>
<!-------------------- Use this area to save things ---------------------->
<!------------------------------------------------------------------------>

<!--------------------------------------->
<!-- These chunks are for curated data -->
<!--------------------------------------->

```{r}
#| echo: false
#| eval: false
workflow_name <- "dada2"
dir.create(here(share_here, paste0(workflow_name, "_curated_data")), 
           recursive = FALSE, showWarnings = TRUE)
```

```{r}
#| echo: false
#| eval: false
write_delim(nc_check, here(share_here, "asv_in_nc_samples.txt"),
    delim = "\t")
```

```{r}
#| echo: false
#| eval: false
tmp_path <- here(share_here, paste0(workflow_name, "_curated_data/"))

files <- list(
  otu = paste0(workflow_name, "_otu_table.txt"),
  tax = paste0(workflow_name, "_tax_table.txt"),
  sample = paste0(workflow_name, "_sample_table.txt"),
  rep = paste0(workflow_name, "_rep.fasta"),
  me = paste0("me_", workflow_name, ".rds"),
  ps = paste0("ps_", workflow_name, ".rds"),
  counts = paste0(workflow_name, "_track_read_counts.txt")
)
```

```{r}
#| echo: false
#| eval: false
#---------------OTU Table--------------#
tmp_otu <- me_final$otu_table %>% 
  tibble::rownames_to_column("ASV_ID")
write_delim(tmp_otu, paste0(tmp_path, files$otu), delim = "\t")
#---------------TAX Table--------------#
tmp_tax <- me_final$tax_table %>% 
  tibble::rownames_to_column("ASV_ID")
write_delim(tmp_tax, paste0(tmp_path, files$tax), delim = "\t")
#---------------SAMP Table--------------#
write_delim(me_final$sample_table, paste0(tmp_path, files$sample), delim = "\t")
#---------------REP FASTA--------------#
write.fasta(
  sequences = as.list(as.character(me_final$rep_fasta)),
  names = names(me_final$rep_fasta),
  file.out = paste0(tmp_path, files$rep)
)
#---------------        --------------#
saveRDS(me_final, paste0(tmp_path, files$me))
ps_final <- file2meco::meco2phyloseq(me_final)
saveRDS(ps_final, paste0(tmp_path, files$ps))
#---------------read count changes--------------#
write_delim(curate_summary, paste0(tmp_path, files$counts), delim = "\t")
file.copy(here("working_files/ssu/sampledata",  "all_metadata.txt"), 
          tmp_path, 
          overwrite = TRUE, recursive = FALSE, 
          copy.mode = TRUE, copy.date = FALSE)
```

```{r}
#| echo: false
#| eval: false
zip::zip(zipfile = here(share_here, paste0(workflow_name, "_curated_data.zip")), 
         files = here(share_here, paste0(workflow_name,"_curated_data")), 
         mode = "cherry-pick")
```

```{r}
#| echo: false
#| eval: false
file.copy(here(work_here,  "dada2_pipeline_read_changes.txt"), 
          here(share_here), 
          overwrite = TRUE, recursive = FALSE, 
          copy.mode = TRUE, copy.date = FALSE)
```

<!-------------------------------------------->
<!-- These chunks are for  processing data  -->
<!-------------------------------------------->

```{r}
#| echo: false
#| eval: false
dir.create(here(share_here, paste0(workflow_name, "_processing")), 
           recursive = FALSE, showWarnings = TRUE)
copy_here <- here(share_here, paste0(workflow_name, "_processing/"))
```

```{r}
#| echo: false
#| eval: false
#| comment: from sampledata (these data are common to all 16S wf)
tmp_sampdata_path <- "working_files/ssu/sampledata"
fs::dir_copy(path = here(tmp_sampdata_path,  "fastq_rename_results/"), 
             new_path = here(copy_here,  "fastq_rename_results/"), 
             overwrite = TRUE)

fs::dir_copy(path = here(tmp_sampdata_path,  "fastq_rename_lookup/"), 
             new_path = here(copy_here,  "fastq_rename_lookup/"), 
             overwrite = TRUE)

file.copy(here(tmp_sampdata_path,  "rename.sh"), 
          here(copy_here), 
          overwrite = TRUE, recursive = FALSE, 
          copy.mode = TRUE, copy.date = FALSE)
```

```{r}
#| echo: false
#| eval: false
fs::dir_copy(path = here(work_here,  paste0(workflow_name, "_hydra_scripts/")), 
             new_path = here(copy_here,  paste0(workflow_name, "_hydra_scripts/")), 
             overwrite = TRUE)

fs::dir_copy(path = here(work_here,  paste0(workflow_name, "_scripts/")), 
             new_path = here(copy_here,  paste0(workflow_name, "_scripts/")), 
             overwrite = TRUE)

fs::dir_copy(path = here(work_here,  paste0(workflow_name, "_read_changes/")), 
             new_path = here(copy_here,  paste0(workflow_name, "_read_changes/")), 
             overwrite = TRUE)

fs::dir_copy(path = here(work_here,  "dada_results/"), 
             new_path = here(copy_here,  "dada_results/"), 
             overwrite = TRUE)

file.copy(here(share_here,  "dada2_pipeline_read_changes.txt"), 
          here(copy_here), 
          overwrite = TRUE, recursive = FALSE, 
          copy.mode = TRUE, copy.date = FALSE)

file.copy(here(tmp_sampdata_path,  "all_metadata.txt"), 
          here(copy_here), 
          overwrite = TRUE, recursive = FALSE, 
          copy.mode = TRUE, copy.date = FALSE)
```

```{r}
#| echo: false
#| eval: false
zip::zip(zipfile = here(share_here, paste0(workflow_name, "_processing.zip")), 
         files = here(share_here, paste0(workflow_name,"_processing")), 
         mode = "cherry-pick")
dir_delete(here(share_here, paste0(workflow_name,"_processing")))
```

```{r}
#| echo: false
#| eval: true
#| message: false
#| include: false
#| comment: save R script
workflow_name <- "dada2"
options(knitr.duplicate.label = "allow")
# Define sources and numbered outputs
sources <- c(
  "include/_BCS_26.qmd",
  "include/_BCS_28.qmd",
  "include/_BCS_29.qmd",
  "include/_BCS_30.qmd",
  "include/_BCS_34.qmd",
  "include/_BCS_35.qmd",
  "include/_MERGE_RUNS.qmd",
  "index.qmd"
)
# Generate corresponding temporary output files
tmp_outputs <- here(share_here, paste0(workflow_name, "_workflow", seq_along(sources), ".R"))
# Final combined output
final_output <- here(share_here, paste0(workflow_name, "_workflow.R"))
# Step 1: Extract code from qmd to temporary R scripts
purrr::walk2(sources, tmp_outputs, ~
  knitr::purl(.x, output = .y, documentation = 0)
)
# Step 2: Concatenate all temporary R scripts into one, with separators
sink(final_output)
purrr::walk2(tmp_outputs, seq_along(tmp_outputs), ~{
  cat(readLines(.x), sep = "\n")
  cat("\n\n",
      paste0("# -------------------- END OF SCRIPT ", .y, " --------------------\n\n"),
      sep = "")
})
sink()
# Step 3: Clean up temporary files
file.remove(tmp_outputs)
```

```{r}
#| echo: false
#| eval: false
objects()
gdata::keep(curate_summary, ds_metrics_tbl, me_final, me_nc, 
            me_no_cont, me_no_low, me_no_na, me_no_nc, 
            me_raw, nc_asvs, nc_check, nc_name, nc_remain, nc_remove, 
            per_reads_rem, per_reads_ret, pipe_summary, rem_nc_reads, 
            rem_sam_reads, removed_samples, ret_nc_reads, ret_sam_reads, 
            samdf, sample_info, samptab, seqtab, share_here, st.head, 
            tax.head1, tax.head2, seqtab, tax_gsrdb, 
            sure = TRUE)
save.image(here("page_build", "dada2_part2.rdata"))
```

{{< include /include/_footer.qmd >}}
