{
  "hash": "40d765243e04e0276c389d1caeee5640",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"2. DADA2 ASV Workflow\"\ndescription: |\n  Workflow for processing 16S rRNA samples for ASV analysis using DADA2. Workflow uses paired end reads, beginning with raw fastq files, ending with sequence and taxonomy tables. A Microtable Object is produced to collate the data for downstream analysis. \nlisting: \n    id: dada2-listing\n    contents: data-dada2.yml\n    type: table\n    sort-ui: false\n    filter-ui: false\n    fields: \n      - filename\n      - description\n    field-links: \n      - filename\n    field-display-names: \n      filename: File Name\n      description: Description\n---\n\n\n::: {.cell}\n\n:::\n\n\n\n\n::: {.cell}\n\n:::\n\n\n## Workflow Input\n\n::: {.callout-note icon=false}\n## Data & Scripts\n\nFastq sequence files, scripts, and other assets for running this workflow can be found on the [Dada2 Data Portal](/workflows/portal/data-dada2.qmd) page. \n\nThe Data Portal page also contains a link to the curated output of this pipeline–-feature-sample count table, taxonomy table, sample metadata, & representative fasta sequences. Data is available as stand-alone text files, OR bundled in microtable and phyloseq objects. Archive also includes a table tracking sample read changes.\n:::\n\n#### Required Packages & Software\n\nThere are several R packages you need to run this workflow. \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Click here for workflow library information.\"}\n#!/usr/bin/env Rscript\nset.seed(919191)\npacman::p_load(tidyverse, gridExtra, grid, phyloseq,\n               formatR, gdata, ff, decontam, dada2, \n               ShortRead, Biostrings, DECIPHER, \n               install = FALSE, update = FALSE)\n```\n:::\n\n\n## Overview\n\nThis workflow contains the code we used to process the 16S rRNA data sets using [DADA2](https://benjjneb.github.io/dada2/) [@callahan2016dada2]. Workflow construction is based on the [DADA2 Pipeline Tutorial (1.8)](https://benjjneb.github.io/dada2/tutorial_1_8.html) and the primer identification section of the [DADA2 ITS Pipeline Workflow (1.8)](https://benjjneb.github.io/dada2/ITS_workflow.html). In the first part of the pipeline, we process the individual sequencing runs **separately**. Next we combine the sequence tables from each run  into one merged sequences table and continue with processing.\n\n# Read Processing {#read-processing}\n\nWe processed each of the six sequencing runs separately for the first part of the DADA2 workflow. While some of the outputs are slightly different (e.g. quality scores, filtering, ASV inference, etc.) the code is the same. For posterity, code for each run is included here.\n\n## Individual Run Workflows\n\nThe first part of the workflow consists of the following steps for each of the runs:\n\n| Step | Command                | What we’re doing                       |\n|------|------------------------|----------------------------------------|\n| 1    | multiple               | prepare input file names & paths       |\n| 2    | multiple               | Define primers (all orientations)      |\n| 3    | `cutadapt`             | Remove primers                         |\n| 4    | `plotQualityProfile()` | Plot quality scores.                   |\n| 5    | `filterAndTrim()`      | Assess quality & filter reads          |\n| 6    | `learnErrors()`        | Generate an error model for the data   |\n| 7    | `derepFastq()`         | Dereplicate sequences                  |\n| 8    | `dada()`               | Infer ASVs (forward & reverse reads).  |\n| 9    | `mergePairs()`.        | Merge denoised forward & reverse reads |\n| 10   | `makeSequenceTable()`  | Generate count table for each run      |\n| 11   |                        | Track reads through workflow           |\n\n```{mermaid}\n%%| eval: true\n%%| echo: false\n%%| fig-align: center\nflowchart LR\n    A(Start with raw</br>sequence data)\n    A --> B(plotQualityProfile)\n    B --> C(filterAndTrim)\n    C --> D(plotErrors)\n    C --> E(learnErrors)\n    E --> END:::hidden\n    \n```\n\n```{mermaid}\n%%| eval: true\n%%| echo: false\n%%| fig-align: center\nflowchart LR\n    BEGIN:::hidden --> F(derepFastq)\n    F --> G(dada)\n    G --> I(mergePairs) \n    I --> J(makeSequenceTable)\n    J --> K(to Merged Runs</br>Workflow)\n```\n\n\n\n<br/>\n\n::: panel-tabset\n## BCS_26\n\n\n::: {.cell}\n\n:::\n\n\n### 1. Set Working Environment\n\n:::{.callout-note}\nThe following plate were sequenced in this run: ISTHMO S5, S5, S7, S8. Access the source code for [BCS_26 processing](include/_BCS_26.qmd). \n:::\n\nFirst, we setup the working environment by defining a path for the working directory.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npath <- \"BCS_26/\"\nhead(list.files(path)) \n```\n:::\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Control_10_R1_001.trimmed.fastq\" \"Control_10_R2_001.trimmed.fastq\"\n[3] \"Control_11_R1_001.trimmed.fastq\" \"Control_11_R2_001.trimmed.fastq\"\n[5] \"Control_12_R1_001.trimmed.fastq\" \"Control_12_R2_001.trimmed.fastq\"\n```\n\n\n:::\n:::\n\n\nThen, we generate matched lists of the forward and reverse read files. We also parse out the sample name.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfnFs <- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs <- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names <- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs <- file.path(path, fnFs)\nfnRs <- file.path(path, fnRs)\n```\n:::\n\n\n### 2. Plot quality scores\n\nAnd next we plot a visual summary of the distribution of quality scores as a function of sequence position for the input fastq files. Here we set the number of records to sample from  each fastq file (`n`) to  20000 reads. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nqprofile_fwd <- plotQualityProfile(fnFs[1:x], \n                                   aggregate = TRUE, \n                                   n = 20000)\nqprofile_rev <- plotQualityProfile(fnRs[1:x], \n                                   aggregate = TRUE, \n                                   n = 20000)\nqprofile <- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Aggregated quality score plots for forward (left) & reverse (right) reads.](include/figures/BCS_26_filt_plot_qscores.png){width=1050}\n:::\n:::\n\n\n### 3. Filtering\n\nWe again make some path variables and setup a new directory of filtered reads.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfiltFs <- file.path(path, \"filtered\", \n                    paste0(sample.names, \"_F_filt.fastq\")\n                    )\nfiltRs <- file.path(path, \"filtered\", \n                    paste0(sample.names, \"_R_filt.fastq\")\n                    )\nnames(filtFs) <- sample.names\nnames(filtRs) <- sample.names\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, \n                     truncLen=c(220,180), \n                     maxN = 0, maxEE = 2, truncQ = 2, \n                     rm.phix = TRUE, compress = TRUE, \n                     multithread = 20)\n```\n:::\n\n\n:::{.callout-note}\nThese parameters should be set based on the anticipated length of the amplicon and the read quality.\n:::\n\nAnd here is a table of how the filtering step affected the number of reads in each sample. \n\n\n::: {.cell}\n\n:::\n\n\n### 4. Learn Error Rates\n\nNow it is time to assess the error rate of the data. The DADA2 algorithm uses a parametric error model. Every amplicon data set has a different set of error rates and the `learnErrors` method learns this error model *from the data*. It does this by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. The algorithm begins with an initial guess, for which the maximum possible error rates in the data are used.\n\n#### Forward Reads\n\n\n::: {.cell}\n\n```{.r .cell-code}\nerrF <- learnErrors(filtFs, multithread = TRUE)\n```\n:::\n\n\n```\n112559480 total bases in 511634 reads from 28 samples \nwill be used for learning the error rates.\n```\n\nWe can then plot the error rates for the forward reads. \n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np3 <- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_26_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_26_plot_errorF_2.png\", p3)\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Forward reads: Observed frequency of each transition (e.g., T -> G) as a function of the associated quality score.](include/figures/BCS_26_plot_errorF_2.png){width=1050}\n:::\n:::\n\n\n#### Reverse Reads\n\nAnd now we can process the reverse read error rates. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nerrR <- learnErrors(filtRs, multithread = TRUE)\n```\n:::\n\n\n```\n100501560 total bases in 558342 reads from 31 samples \nwill be used for learning the error rates.\n```\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np4 <- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_26_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_26_plot_errorR_2.png\", p4)\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Reverse reads: Observed frequency of each transition (e.g., T -> G) as a function of the associated quality score.](include/figures/BCS_26_plot_errorR_2.png){width=1050}\n:::\n:::\n\n\nThe error rates for each possible transition (A to C, A to G, etc.) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. Here the estimated error rates (black line) are a good fit to the observed rates (points), and the error rates drop with increased quality as expected.\n\n### 5. Dereplicate Reads\n\nNow we can use `derepFastq` to identify the unique sequences in the forward and reverse fastq files.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsam.names <- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs <- derepFastq(filtFs)\nnames(derepFs) <- sam.names\nderepRs <- derepFastq(filtRs)\nnames(derepRs) <- sam.names\n```\n:::\n\n\n### 6. DADA2 & ASV Inference\n\nAt this point we are ready to apply the core sample inference algorithm (`dada`) to the filtered and trimmed sequence data. DADA2 offers three options for whether and how to pool samples for ASV inference. \n\nIf `pool = TRUE`, the algorithm will pool together all samples prior to sample inference.  \nIf `pool = FALSE`, sample inference is performed on each sample individually.  \nIf `pool = \"pseudo\"`, the algorithm will perform pseudo-pooling between individually processed samples.\n\nFor our final analysis, we chose `pool = pseudo` for this data set. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndadaFs <- dada(derepFs, err = errF, pool = \"pseudo\", \n               multithread = TRUE)\ndadaRs <- dada(derepRs, err = errR, pool = \"pseudo\",\n               multithread = TRUE)\n```\n:::\n\n\nAs an example, we can inspect the returned `dada-class` object for the forward reads from the sample #1:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndadaFs[[1]]\n```\n:::\n\n\n```\ndada-class: object describing DADA2 denoising results\n15 sequence variants were inferred from 813 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\n```\n\nAnd the corresponding reverse reads.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndadaRs[[1]]\n```\n:::\n\n\n```\ndada-class: object describing DADA2 denoising results\n13 sequence variants were inferred from 683 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\n```\n\nThese outputs tell us how many true sequence variants the DADA2 algorithm inferred from the unique sequences, in this case the sample 1.\n\n### 7. Merge Paired Reads\n\nWe now merge the forward and reverse reads together to obtain the full denoised sequence dataset. Merging is performed by aligning the denoised forward reads with the reverse-complement of the corresponding denoised reverse reads, and then constructing the merged “contig” sequences. By default, merged sequences are only output if the forward and reverse reads overlap by at least 12 bases, and are identical to each other in the overlap region (but these conditions can be changed via function arguments).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\n```\n:::\n\n\nThe `mergers` objects are lists of data.frames from each sample. Each data.frame contains the merged sequence, its abundance, and the indices of the forward and reverse sequence variants that were merged. Paired reads that did not exactly overlap were removed by `mergePairs`, further reducing spurious output.\n\n### 8. Construct Sequence Table\n\nNow we construct an amplicon sequence variant (ASV) table.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBCS_26 <- makeSequenceTable(mergers)\ndim(BCS_26)\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\nLooks like we have **29430** sequence variants from **384** samples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(nchar(getSequences(BCS_26)))\n```\n:::\n\n\n```\n  220   221   222   223   224   225   226   227   228   229   230   231   234 \n   32    30     6    13     3     2     1     9     5     2     1     3     2 \n  235   236   237   238   239   240   241   242   243   244   245   246   247 \n    2   479    83     3     4    45    31   218    72    20     3     3     7 \n  248   249   250   251   252   253   254   255   256   257   258   259   260 \n    8    10    14    47   985 25810  1025    80    27    15     6     2     3 \n  261   262   263   268   270   272   273   275   276   278   279   280   286 \n   10     3     2     1     1     2     1     1     4     1     2     1     1 \n  289   293   294   295   296   298   300   311   316   318   319   321   323 \n    1     3     3     1     2     1     1     1     2     1     1     1     1 \n  324   333   334   335   337   338   339   340   341   342   343   344   345 \n    1     1     7     5     9     1    10     5     3     7     1     3     2 \n  346   347   348   349   350   351   352   355   356   357   359   360   361 \n    3     2     5     8     5     1     7     7     2     2     4     2    22 \n  362   363   364   365   366   367   368   369   370   372   373   374   377 \n   28     4    30    50     5     3     3     2     1     3    10     1     3 \n  378 \n    2 \n```\n\nThe sequence table is a matrix with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants. We have **29430** sequence variants but also a range of sequence lengths. Since many of these sequence variants are singletons or doubletons, we can just select a range that corresponds to the expected amplicon length and eliminate the spurious reads. But we will do this step after merging sequence tables from all 6 runs. \n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Distribution of read length by total ASVs before removing extreme length variants.](include/figures/read_length_before_pseudo_BCS_26.png){width=1050}\n:::\n:::\n\n\n### 9. Tracking Reads \n\nIf you are interested how reads changed up to this point in the pipeline please see the [Dada2 Data Portal](/workflows/portal/data-dada2.qmd) page for more details. \n\n\n::: {.cell}\n\n:::\n\n\n\nAnd save the sequence table to an `RDS` file.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsaveRDS(BCS_26, \"BCS_26.rds\")\n```\n:::\n\n\n\n## BCS_28\n\n\n::: {.cell}\n\n:::\n\n\n### 1. Set Working Environment\n\n:::{.callout-note}\nThe following plate were sequenced in this run: ISTHMO S3, S4. Access the source code for [BCS_28 processing](include/_BCS_28.qmd). \n:::\n\nFirst, we setup the working environment by defining a path for the working directory.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npath <- \"BCS_28/\"\nhead(list.files(path)) \n```\n:::\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Control_31_R1_001.trimmed.fastq\" \"Control_31_R2_001.trimmed.fastq\"\n[3] \"Control_32_R1_001.trimmed.fastq\" \"Control_32_R2_001.trimmed.fastq\"\n[5] \"Control_33_R1_001.trimmed.fastq\" \"Control_33_R2_001.trimmed.fastq\"\n```\n\n\n:::\n:::\n\n\nThen, we generate matched lists of the forward and reverse read files. We also parse out the sample name.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfnFs <- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs <- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names <- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs <- file.path(path, fnFs)\nfnRs <- file.path(path, fnRs)\n```\n:::\n\n\n### 2. Plot quality scores\n\nAnd next we plot a visual summary of the distribution of quality scores as a function of sequence position for the input fastq files. Here we set the number of records to sample from  each fastq file (`n`) to  20000 reads. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nqprofile_fwd <- plotQualityProfile(fnFs[1:x], \n                                   aggregate = TRUE, \n                                   n = 20000)\nqprofile_rev <- plotQualityProfile(fnRs[1:x], \n                                   aggregate = TRUE, \n                                   n = 20000)\nqprofile <- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Aggregated quality score plots for forward (left) & reverse (right) reads.](include/figures/BCS_28_filt_plot_qscores.png){width=1050}\n:::\n:::\n\n\n### 3. Filtering\n\nWe again make some path variables and setup a new directory of filtered reads.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfiltFs <- file.path(path, \"filtered\", \n                    paste0(sample.names, \"_F_filt.fastq\")\n                    )\nfiltRs <- file.path(path, \"filtered\", \n                    paste0(sample.names, \"_R_filt.fastq\")\n                    )\nnames(filtFs) <- sample.names\nnames(filtRs) <- sample.names\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, \n                     truncLen=c(220,180), \n                     maxN = 0, maxEE = 2, truncQ = 2, \n                     rm.phix = TRUE, compress = TRUE, \n                     multithread = 20)\n```\n:::\n\n\n:::{.callout-note}\nThese parameters should be set based on the anticipated length of the amplicon and the read quality.\n:::\n\nAnd here is a table of how the filtering step affected the number of reads in each sample. \n\n\n::: {.cell}\n\n:::\n\n\n### 4. Learn Error Rates\n\nNow it is time to assess the error rate of the data. The DADA2 algorithm uses a parametric error model. Every amplicon data set has a different set of error rates and the `learnErrors` method learns this error model *from the data*. It does this by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. The algorithm begins with an initial guess, for which the maximum possible error rates in the data are used.\n\n#### Forward Reads\n\n\n::: {.cell}\n\n```{.r .cell-code}\nerrF <- learnErrors(filtFs, multithread = TRUE)\n```\n:::\n\n\n```\n100307240 total bases in 455942 reads from 31 samples \nwill be used for learning the error rates.\n```\n\nWe can then plot the error rates for the forward reads. \n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np3 <- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_28_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_28_plot_errorF_2.png\", p3)\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Forward reads: Observed frequency of each transition (e.g., T -> G) as a function of the associated quality score.](include/figures/BCS_28_plot_errorF_2.png){width=1050}\n:::\n:::\n\n\n#### Reverse Reads\n\nAnd now we can process the reverse read error rates. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nerrR <- learnErrors(filtRs, multithread = TRUE)\n```\n:::\n\n\n```\n103203360 total bases in 573352 reads from 36 samples \nwill be used for learning the error rates.\n```\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np4 <- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_28_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_28_plot_errorR_2.png\", p4)\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Reverse reads: Observed frequency of each transition (e.g., T -> G) as a function of the associated quality score.](include/figures/BCS_28_plot_errorR_2.png){width=1050}\n:::\n:::\n\n\nThe error rates for each possible transition (A to C, A to G, etc.) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. Here the estimated error rates (black line) are a good fit to the observed rates (points), and the error rates drop with increased quality as expected.\n\n### 5. Dereplicate Reads\n\nNow we can use `derepFastq` to identify the unique sequences in the forward and reverse fastq files.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsam.names <- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs <- derepFastq(filtFs)\nnames(derepFs) <- sam.names\nderepRs <- derepFastq(filtRs)\nnames(derepRs) <- sam.names\n```\n:::\n\n\n### 6. DADA2 & ASV Inference\n\nAt this point we are ready to apply the core sample inference algorithm (`dada`) to the filtered and trimmed sequence data. DADA2 offers three options for whether and how to pool samples for ASV inference. \n\nIf `pool = TRUE`, the algorithm will pool together all samples prior to sample inference.  \nIf `pool = FALSE`, sample inference is performed on each sample individually.  \nIf `pool = \"pseudo\"`, the algorithm will perform pseudo-pooling between individually processed samples.\n\nFor our final analysis, we chose `pool = pseudo` for this data set. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndadaFs <- dada(derepFs, err = errF, pool = \"pseudo\", \n               multithread = TRUE)\ndadaRs <- dada(derepRs, err = errR, pool = \"pseudo\",\n               multithread = TRUE)\n```\n:::\n\n\nAs an example, we can inspect the returned `dada-class` object for the forward reads from the sample #1:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndadaFs[[1]]\n```\n:::\n\n\n```\ndada-class: object describing DADA2 denoising results\n76 sequence variants were inferred from 8771 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\n```\n\nAnd the corresponding reverse reads.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndadaRs[[1]]\n```\n:::\n\n\n```\ndada-class: object describing DADA2 denoising results\n74 sequence variants were inferred from 6979 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\n```\n\nThese outputs tell us how many true sequence variants the DADA2 algorithm inferred from the unique sequences, in this case the sample 1.\n\n### 7. Merge Paired Reads\n\nWe now merge the forward and reverse reads together to obtain the full denoised sequence dataset. Merging is performed by aligning the denoised forward reads with the reverse-complement of the corresponding denoised reverse reads, and then constructing the merged “contig” sequences. By default, merged sequences are only output if the forward and reverse reads overlap by at least 12 bases, and are identical to each other in the overlap region (but these conditions can be changed via function arguments).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\n```\n:::\n\n\nThe `mergers` objects are lists of data.frames from each sample. Each data.frame contains the merged sequence, its abundance, and the indices of the forward and reverse sequence variants that were merged. Paired reads that did not exactly overlap were removed by `mergePairs`, further reducing spurious output.\n\n### 8. Construct Sequence Table\n\nNow we construct an amplicon sequence variant (ASV) table.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBCS_28 <- makeSequenceTable(mergers)\ndim(BCS_28)\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\nLooks like we have **9461** sequence variants from **192** samples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(nchar(getSequences(BCS_28)))\n```\n:::\n\n\n```\n 220  221  222  223  224  227  230  231  236  237  238  239  241  242  243  244 \n   3    8    2    1    1    4    1    1    1    2    1    1    2    5    1    1 \n 245  246  247  248  249  250  251  252  253  254  255  256  257  258  259  268 \n   1    1    2    1    1    4    8  328 8699  252   21    9    6    1    1    1 \n 270  292  318  335  336  337  338  339  341  342  344  345  348  357  359  361 \n   1    1    1    1    2    2    1    1    2    3    1    1    3    1    1    5 \n 362  363  364  365  366  367  368  372  373  374  376  378  385  388 \n   8    3   22   13    1    1    8    1    1    1    1    1    1    1 \n```\n\nThe sequence table is a matrix with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants. We have **9461** sequence variants but also a range of sequence lengths. Since many of these sequence variants are singletons or doubletons, we can just select a range that corresponds to the expected amplicon length and eliminate the spurious reads. But we will do this step after merging sequence tables from all 6 runs. \n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Distribution of read length by total ASVs before removing extreme length variants.](include/figures/read_length_before_pseudo_BCS_28.png){width=1050}\n:::\n:::\n\n\n### 9. Tracking Reads \n\nIf you are interested how reads changed up to this point in the pipeline please see the [Dada2 Data Portal](/workflows/portal/data-dada2.qmd) page for more details. \n\n\n::: {.cell}\n\n:::\n\n\nAnd save the sequence table to an `RDS` file.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsaveRDS(BCS_28, \"BCS_28.rds\")\n```\n:::\n\n\n\n## BCS_29\n\n\n::: {.cell}\n\n:::\n\n\n### 1. Set Working Environment\n\n:::{.callout-note}\nThe following plate were sequenced in this run: ISTHMO S13, S14, S15, S16. Access the source code for [BCS_29 processing](include/_BCS_29.qmd).  \n:::\n\nFirst, we setup the working environment by defining a path for the working directory.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npath <- \"BCS_29/\"\nhead(list.files(path)) \n```\n:::\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Control_37_R1_001.trimmed.fastq\" \"Control_37_R2_001.trimmed.fastq\"\n[3] \"Control_38_R1_001.trimmed.fastq\" \"Control_38_R2_001.trimmed.fastq\"\n[5] \"Control_39_R1_001.trimmed.fastq\" \"Control_39_R2_001.trimmed.fastq\"\n```\n\n\n:::\n:::\n\n\nThen, we generate matched lists of the forward and reverse read files. We also parse out the sample name.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfnFs <- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs <- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names <- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs <- file.path(path, fnFs)\nfnRs <- file.path(path, fnRs)\n```\n:::\n\n\n### 2. Plot quality scores\n\nAnd next we plot a visual summary of the distribution of quality scores as a function of sequence position for the input fastq files. Here we set the number of records to sample from  each fastq file (`n`) to  20000 reads. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nqprofile_fwd <- plotQualityProfile(fnFs[1:x], \n                                   aggregate = TRUE, \n                                   n = 20000)\nqprofile_rev <- plotQualityProfile(fnRs[1:x], \n                                   aggregate = TRUE, \n                                   n = 20000)\nqprofile <- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Aggregated quality score plots for forward (left) & reverse (right) reads.](include/figures/BCS_29_filt_plot_qscores.png){width=1050}\n:::\n:::\n\n\n### 3. Filtering\n\nWe again make some path variables and setup a new directory of filtered reads.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfiltFs <- file.path(path, \"filtered\", \n                    paste0(sample.names, \"_F_filt.fastq\")\n                    )\nfiltRs <- file.path(path, \"filtered\", \n                    paste0(sample.names, \"_R_filt.fastq\")\n                    )\nnames(filtFs) <- sample.names\nnames(filtRs) <- sample.names\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, \n                     truncLen=c(220,180), \n                     maxN = 0, maxEE = 2, truncQ = 2, \n                     rm.phix = TRUE, compress = TRUE, \n                     multithread = 20)\n```\n:::\n\n\n:::{.callout-note}\nThese parameters should be set based on the anticipated length of the amplicon and the read quality.\n:::\n\nAnd here is a table of how the filtering step affected the number of reads in each sample. \n\n\n::: {.cell}\n\n:::\n\n\n### 4. Learn Error Rates\n\nNow it is time to assess the error rate of the data. The DADA2 algorithm uses a parametric error model. Every amplicon data set has a different set of error rates and the `learnErrors` method learns this error model *from the data*. It does this by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. The algorithm begins with an initial guess, for which the maximum possible error rates in the data are used.\n\n#### Forward Reads\n\n\n::: {.cell}\n\n```{.r .cell-code}\nerrF <- learnErrors(filtFs, multithread = TRUE)\n```\n:::\n\n\n```\n104777640 total bases in 476262 reads from 36 samples \nwill be used for learning the error rates.\n```\n\nWe can then plot the error rates for the forward reads. \n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np3 <- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_29_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_29_plot_errorF_2.png\", p3)\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Forward reads: Observed frequency of each transition (e.g., T -> G) as a function of the associated quality score.](include/figures/BCS_29_plot_errorF_2.png){width=1050}\n:::\n:::\n\n\n#### Reverse Reads\n\nAnd now we can process the reverse read error rates. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nerrR <- learnErrors(filtRs, multithread = TRUE)\n```\n:::\n\n\n```\n103229460 total bases in 573497 reads from 43 samples \nwill be used for learning the error rates.\n```\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np4 <- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_29_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_29_plot_errorR_2.png\", p4)\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Reverse reads: Observed frequency of each transition (e.g., T -> G) as a function of the associated quality score.](include/figures/BCS_29_plot_errorR_2.png){width=1050}\n:::\n:::\n\n\nThe error rates for each possible transition (A to C, A to G, etc.) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. Here the estimated error rates (black line) are a good fit to the observed rates (points), and the error rates drop with increased quality as expected.\n\n### 5. Dereplicate Reads\n\nNow we can use `derepFastq` to identify the unique sequences in the forward and reverse fastq files.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsam.names <- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs <- derepFastq(filtFs)\nnames(derepFs) <- sam.names\nderepRs <- derepFastq(filtRs)\nnames(derepRs) <- sam.names\n```\n:::\n\n\n### 6. DADA2 & ASV Inference\n\nAt this point we are ready to apply the core sample inference algorithm (`dada`) to the filtered and trimmed sequence data. DADA2 offers three options for whether and how to pool samples for ASV inference. \n\nIf `pool = TRUE`, the algorithm will pool together all samples prior to sample inference.  \nIf `pool = FALSE`, sample inference is performed on each sample individually.  \nIf `pool = \"pseudo\"`, the algorithm will perform pseudo-pooling between individually processed samples.\n\nFor our final analysis, we chose `pool = pseudo` for this data set. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndadaFs <- dada(derepFs, err = errF, pool = \"pseudo\", \n               multithread = TRUE)\ndadaRs <- dada(derepRs, err = errR, pool = \"pseudo\",\n               multithread = TRUE)\n```\n:::\n\n\nAs an example, we can inspect the returned `dada-class` object for the forward reads from the sample #1:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndadaFs[[1]]\n```\n:::\n\n\n```\ndada-class: object describing DADA2 denoising results\n46 sequence variants were inferred from 3108 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\n```\n\nAnd the corresponding reverse reads.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndadaRs[[1]]\n```\n:::\n\n\n```\ndada-class: object describing DADA2 denoising results\n44 sequence variants were inferred from 2744 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\n```\n\nThese outputs tell us how many true sequence variants the DADA2 algorithm inferred from the unique sequences, in this case the sample 1.\n\n### 7. Merge Paired Reads\n\nWe now merge the forward and reverse reads together to obtain the full denoised sequence dataset. Merging is performed by aligning the denoised forward reads with the reverse-complement of the corresponding denoised reverse reads, and then constructing the merged “contig” sequences. By default, merged sequences are only output if the forward and reverse reads overlap by at least 12 bases, and are identical to each other in the overlap region (but these conditions can be changed via function arguments).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\n```\n:::\n\n\nThe `mergers` objects are lists of data.frames from each sample. Each data.frame contains the merged sequence, its abundance, and the indices of the forward and reverse sequence variants that were merged. Paired reads that did not exactly overlap were removed by `mergePairs`, further reducing spurious output.\n\n### 8. Construct Sequence Table\n\nNow we construct an amplicon sequence variant (ASV) table.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBCS_29 <- makeSequenceTable(mergers)\ndim(BCS_29)\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\nLooks like we have **21105** sequence variants from **384** samples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(nchar(getSequences(BCS_29)))\n```\n:::\n\n\n```\n   220   221   222   223   224   225   226   227   228   229   231   234   236 \n   16    22     1    10     5     5     2    10     1     2    11     3   518 \n  237   238   240   241   242   243   244   245   246   247   248   249   250 \n    3     7     8   155    11    81     1     2     4     4     1     2     8 \n  251   252   253   254   255   256   257   258   260   262   269   270   272 \n   28   690 18300   857    42    17    18     1     1     2     1     1     1 \n  273   274   275   277   278   281   286   288   292   293   294   297   300 \n    1     1     1     1     3     1     1     1     1     2     2     1     1 \n  303   304   305   308   313   315   318   319   329   330   334   335   336 \n    2     1     3     1     2     1     1     1     3     2     3     1     1 \n  337   339   340   341   342   343   344   347   348   349   350   352   356 \n    2     4     5    21     7     1     1     1     2     3     1     1     2 \n  357   358   359   360   361   362   363   364   365   366   367   368   370 \n   14     1     2     6    14    65     4    32    13     4     2     2     2 \n  379   384 \n    1     1  \n```\n\nThe sequence table is a matrix with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants. We have **21105** sequence variants but also a range of sequence lengths. Since many of these sequence variants are singletons or doubletons, we can just select a range that corresponds to the expected amplicon length and eliminate the spurious reads. But we will do this step after merging sequence tables from all 6 runs. \n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Distribution of read length by total ASVs before removing extreme length variants.](include/figures/read_length_before_pseudo_BCS_29.png){width=1050}\n:::\n:::\n\n\n### 9. Tracking Reads \n\nIf you are interested how reads changed up to this point in the pipeline please see the [Dada2 Data Portal](/workflows/portal/data-dada2.qmd) page for more details. \n\n\n::: {.cell}\n\n:::\n\n\nAnd save the sequence table to an `RDS` file.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsaveRDS(BCS_29, \"BCS_29.rds\")\n```\n:::\n\n\n\n## BCS_30\n\n\n::: {.cell}\n\n:::\n\n\n### 1. Set Working Environment\n\n:::{.callout-note}\nThe following plate were sequenced in this run: ISTHMO S17, S18, S19, S20.  Access the source code for [BCS_30 processing](include/_BCS_30.qmd). \n:::\n\nFirst, we setup the working environment by defining a path for the working directory.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npath <- \"BCS_30/\"\nhead(list.files(path)) \n```\n:::\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Control_49_R1_001.trimmed.fastq\" \"Control_49_R2_001.trimmed.fastq\"\n[3] \"Control_50_R1_001.trimmed.fastq\" \"Control_50_R2_001.trimmed.fastq\"\n[5] \"Control_51_R1_001.trimmed.fastq\" \"Control_51_R2_001.trimmed.fastq\"\n```\n\n\n:::\n:::\n\n\nThen, we generate matched lists of the forward and reverse read files. We also parse out the sample name.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfnFs <- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs <- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names <- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs <- file.path(path, fnFs)\nfnRs <- file.path(path, fnRs)\n```\n:::\n\n\n### 2. Plot quality scores\n\nAnd next we plot a visual summary of the distribution of quality scores as a function of sequence position for the input fastq files. Here we set the number of records to sample from  each fastq file (`n`) to  20000 reads. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nqprofile_fwd <- plotQualityProfile(fnFs[1:x], \n                                   aggregate = TRUE, \n                                   n = 20000)\nqprofile_rev <- plotQualityProfile(fnRs[1:x], \n                                   aggregate = TRUE, \n                                   n = 20000)\nqprofile <- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Aggregated quality score plots for forward (left) & reverse (right) reads.](include/figures/BCS_30_filt_plot_qscores.png){width=1050}\n:::\n:::\n\n\n### 3. Filtering\n\nWe again make some path variables and setup a new directory of filtered reads.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfiltFs <- file.path(path, \"filtered\", \n                    paste0(sample.names, \"_F_filt.fastq\")\n                    )\nfiltRs <- file.path(path, \"filtered\", \n                    paste0(sample.names, \"_R_filt.fastq\")\n                    )\nnames(filtFs) <- sample.names\nnames(filtRs) <- sample.names\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, \n                     truncLen=c(220,180), \n                     maxN = 0, maxEE = 2, truncQ = 2, \n                     rm.phix = TRUE, compress = TRUE, \n                     multithread = 20)\n```\n:::\n\n\n:::{.callout-note}\nThese parameters should be set based on the anticipated length of the amplicon and the read quality.\n:::\n\nAnd here is a table of how the filtering step affected the number of reads in each sample. \n\n\n::: {.cell}\n\n:::\n\n\n### 4. Learn Error Rates\n\nNow it is time to assess the error rate of the data. The DADA2 algorithm uses a parametric error model. Every amplicon data set has a different set of error rates and the `learnErrors` method learns this error model *from the data*. It does this by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. The algorithm begins with an initial guess, for which the maximum possible error rates in the data are used.\n\n#### Forward Reads\n\n\n::: {.cell}\n\n```{.r .cell-code}\nerrF <- learnErrors(filtFs, multithread = TRUE)\n```\n:::\n\n\n```\n101887500 total bases in 463125 reads from 26 samples \nwill be used for learning the error rates.\n```\n\nWe can then plot the error rates for the forward reads. \n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np3 <- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_30_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_30_plot_errorF_2.png\", p3)\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Forward reads: Observed frequency of each transition (e.g., T -> G) as a function of the associated quality score.](include/figures/BCS_30_plot_errorF_2.png){width=1050}\n:::\n:::\n\n\n#### Reverse Reads\n\nAnd now we can process the reverse read error rates. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nerrR <- learnErrors(filtRs, multithread = TRUE)\n```\n:::\n\n\n```\n105320340 total bases in 585113 reads from 30 samples \nwill be used for learning the error rates.\n```\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np4 <- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_30_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_30_plot_errorR_2.png\", p4)\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Reverse reads: Observed frequency of each transition (e.g., T -> G) as a function of the associated quality score.](include/figures/BCS_30_plot_errorR_2.png){width=1050}\n:::\n:::\n\n\nThe error rates for each possible transition (A to C, A to G, etc.) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. Here the estimated error rates (black line) are a good fit to the observed rates (points), and the error rates drop with increased quality as expected.\n\n### 5. Dereplicate Reads\n\nNow we can use `derepFastq` to identify the unique sequences in the forward and reverse fastq files.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsam.names <- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs <- derepFastq(filtFs)\nnames(derepFs) <- sam.names\nderepRs <- derepFastq(filtRs)\nnames(derepRs) <- sam.names\n```\n:::\n\n\n### 6. DADA2 & ASV Inference\n\nAt this point we are ready to apply the core sample inference algorithm (`dada`) to the filtered and trimmed sequence data. DADA2 offers three options for whether and how to pool samples for ASV inference. \n\nIf `pool = TRUE`, the algorithm will pool together all samples prior to sample inference.  \nIf `pool = FALSE`, sample inference is performed on each sample individually.  \nIf `pool = \"pseudo\"`, the algorithm will perform pseudo-pooling between individually processed samples.\n\nFor our final analysis, we chose `pool = pseudo` for this data set. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndadaFs <- dada(derepFs, err = errF, pool = \"pseudo\", \n               multithread = TRUE)\ndadaRs <- dada(derepRs, err = errR, pool = \"pseudo\",\n               multithread = TRUE)\n```\n:::\n\n\nAs an example, we can inspect the returned `dada-class` object for the forward reads from the sample #1:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndadaFs[[1]]\n```\n:::\n\n\n```\ndada-class: object describing DADA2 denoising results\n9 sequence variants were inferred from 25 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\n```\n\nAnd the corresponding reverse reads.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndadaRs[[1]]\n```\n:::\n\n\n```\ndada-class: object describing DADA2 denoising results\n9 sequence variants were inferred from 21 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\n```\n\nThese outputs tell us how many true sequence variants the DADA2 algorithm inferred from the unique sequences, in this case the sample 1.\n\n### 7. Merge Paired Reads\n\nWe now merge the forward and reverse reads together to obtain the full denoised sequence dataset. Merging is performed by aligning the denoised forward reads with the reverse-complement of the corresponding denoised reverse reads, and then constructing the merged “contig” sequences. By default, merged sequences are only output if the forward and reverse reads overlap by at least 12 bases, and are identical to each other in the overlap region (but these conditions can be changed via function arguments).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\n```\n:::\n\n\nThe `mergers` objects are lists of data.frames from each sample. Each data.frame contains the merged sequence, its abundance, and the indices of the forward and reverse sequence variants that were merged. Paired reads that did not exactly overlap were removed by `mergePairs`, further reducing spurious output.\n\n### 8. Construct Sequence Table\n\nNow we construct an amplicon sequence variant (ASV) table.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBCS_30 <- makeSequenceTable(mergers)\ndim(BCS_30)\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\nLooks like we have **36401** sequence variants from **380** samples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(nchar(getSequences(BCS_30)))\n```\n:::\n\n\n```\n  220   221   222   223   224   225   226   227   228   229   231   232   234 \n   45    24     5    12    10     3     2    13     2     1    15     2     3 \n  235   236   237   238   240   241   242   243   244   245   246   247   248 \n    5   142     2    29    33   128    26   266     5     3     4    10     8 \n  249   250   251   252   253   254   255   256   257   258   259   260   261 \n    7    16    62  1119 31980  1708   149    56    47     7     6     2     4 \n  262   263   264   267   268   269   270   271   272   273   274   276   278 \n    3     1     2     1     1     1     4     7     1     2     2     1     5 \n  279   282   284   285   286   288   289   291   292   293   294   295   296 \n    1     1     1     3     1     1     2     1     1     5     1     1     1 \n  297   298   303   305   307   308   311   313   316   317   319   320   321 \n    1     1     1     2     1     1     2     2     2     1     2     1     1 \n  322   323   325   326   328   332   333   334   335   336   337   338   339 \n    2     1     2     1     2     1     1     2     1     6     3     3     5 \n  340   341   342   343   344   345   347   348   349   350   351   352   353 \n    2    21    23     4     2     4     8     6     6     3     1     2     1 \n  354   355   356   357   358   359   360   361   362   363   364   365   366 \n    1     1     3     6     2     8     9    76    58    18    33    22     5 \n  368   371   372   373   379   380   387 \n    8     1     1     1     2     1     2 \n```\n\nThe sequence table is a matrix with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants. We have **36401** sequence variants but also a range of sequence lengths. Since many of these sequence variants are singletons or doubletons, we can just select a range that corresponds to the expected amplicon length and eliminate the spurious reads. But we will do this step after merging sequence tables from all 6 runs. \n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Distribution of read length by total ASVs before removing extreme length variants.](include/figures/read_length_before_pseudo_BCS_30.png){width=1050}\n:::\n:::\n\n\n### 9. Tracking Reads \n\nIf you are interested how reads changed up to this point in the pipeline please see the [Dada2 Data Portal](/workflows/portal/data-dada2.qmd) page for more details. \n\n\n::: {.cell}\n\n:::\n\n\nAnd save the sequence table to an `RDS` file.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsaveRDS(BCS_30, \"BCS_30.rds\")\n```\n:::\n\n\n\n## BCS_34\n\n\n::: {.cell}\n\n:::\n\n\n### 1. Set Working Environment\n\n:::{.callout-note}\nThe following plate were sequenced in this run: ISTHMO S01, S02.  Access the source code for [BCS_34 processing](include/_BCS_34.qmd).  \n:::\n\nFirst, we setup the working environment by defining a path for the working directory.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npath <- \"BCS_34/\"\nhead(list.files(path)) \n```\n:::\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Control_25_R1_001.trimmed.fastq\" \"Control_25_R2_001.trimmed.fastq\"\n[3] \"Control_26_R1_001.trimmed.fastq\" \"Control_26_R2_001.trimmed.fastq\"\n[5] \"Control_27_R1_001.trimmed.fastq\" \"Control_27_R2_001.trimmed.fastq\"\n```\n\n\n:::\n:::\n\n\nThen, we generate matched lists of the forward and reverse read files. We also parse out the sample name.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfnFs <- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs <- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names <- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs <- file.path(path, fnFs)\nfnRs <- file.path(path, fnRs)\n```\n:::\n\n\n### 2. Plot quality scores\n\nAnd next we plot a visual summary of the distribution of quality scores as a function of sequence position for the input fastq files. Here we set the number of records to sample from  each fastq file (`n`) to  20000 reads. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nqprofile_fwd <- plotQualityProfile(fnFs[1:x], \n                                   aggregate = TRUE, \n                                   n = 20000)\nqprofile_rev <- plotQualityProfile(fnRs[1:x], \n                                   aggregate = TRUE, \n                                   n = 20000)\nqprofile <- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Aggregated quality score plots for forward (left) & reverse (right) reads.](include/figures/BCS_34_filt_plot_qscores.png){width=1050}\n:::\n:::\n\n\n### 3. Filtering\n\nWe again make some path variables and setup a new directory of filtered reads.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfiltFs <- file.path(path, \"filtered\", \n                    paste0(sample.names, \"_F_filt.fastq\")\n                    )\nfiltRs <- file.path(path, \"filtered\", \n                    paste0(sample.names, \"_R_filt.fastq\")\n                    )\nnames(filtFs) <- sample.names\nnames(filtRs) <- sample.names\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, \n                     truncLen=c(220,180), \n                     maxN = 0, maxEE = 2, truncQ = 2, \n                     rm.phix = TRUE, compress = TRUE, \n                     multithread = 20)\n```\n:::\n\n\n:::{.callout-note}\nThese parameters should be set based on the anticipated length of the amplicon and the read quality.\n:::\n\nAnd here is a table of how the filtering step affected the number of reads in each sample. \n\n\n::: {.cell}\n\n:::\n\n\n### 4. Learn Error Rates\n\nNow it is time to assess the error rate of the data. The DADA2 algorithm uses a parametric error model. Every amplicon data set has a different set of error rates and the `learnErrors` method learns this error model *from the data*. It does this by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. The algorithm begins with an initial guess, for which the maximum possible error rates in the data are used.\n\n#### Forward Reads\n\n\n::: {.cell}\n\n```{.r .cell-code}\nerrF <- learnErrors(filtFs, multithread = TRUE)\n```\n:::\n\n\n```\n100938640 total bases in 458812 reads from 22 samples \nwill be used for learning the error rates.\n```\n\nWe can then plot the error rates for the forward reads. \n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np3 <- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_34_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_34_plot_errorF_2.png\", p3)\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Forward reads: Observed frequency of each transition (e.g., T -> G) as a function of the associated quality score.](include/figures/BCS_34_plot_errorF_2.png){width=1050}\n:::\n:::\n\n\n#### Reverse Reads\n\nAnd now we can process the reverse read error rates. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nerrR <- learnErrors(filtRs, multithread = TRUE)\n```\n:::\n\n\n```\n104387760 total bases in 579932 reads from 25 samples \nwill be used for learning the error rates.\n```\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np4 <- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_34_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_34_plot_errorR_2.png\", p4)\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Reverse reads: Observed frequency of each transition (e.g., T -> G) as a function of the associated quality score.](include/figures/BCS_34_plot_errorR_2.png){width=1050}\n:::\n:::\n\n\nThe error rates for each possible transition (A to C, A to G, etc.) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. Here the estimated error rates (black line) are a good fit to the observed rates (points), and the error rates drop with increased quality as expected.\n\n### 5. Dereplicate Reads\n\nNow we can use `derepFastq` to identify the unique sequences in the forward and reverse fastq files.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsam.names <- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs <- derepFastq(filtFs)\nnames(derepFs) <- sam.names\nderepRs <- derepFastq(filtRs)\nnames(derepRs) <- sam.names\n```\n:::\n\n\n### 6. DADA2 & ASV Inference\n\nAt this point we are ready to apply the core sample inference algorithm (`dada`) to the filtered and trimmed sequence data. DADA2 offers three options for whether and how to pool samples for ASV inference. \n\nIf `pool = TRUE`, the algorithm will pool together all samples prior to sample inference.  \nIf `pool = FALSE`, sample inference is performed on each sample individually.  \nIf `pool = \"pseudo\"`, the algorithm will perform pseudo-pooling between individually processed samples.\n\nFor our final analysis, we chose `pool = pseudo` for this data set. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndadaFs <- dada(derepFs, err = errF, pool = \"pseudo\", \n               multithread = TRUE)\ndadaRs <- dada(derepRs, err = errR, pool = \"pseudo\",\n               multithread = TRUE)\n```\n:::\n\n\nAs an example, we can inspect the returned `dada-class` object for the forward reads from the sample #1:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndadaFs[[1]]\n```\n:::\n\n\n```\ndada-class: object describing DADA2 denoising results\n9 sequence variants were inferred from 327 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\n```\n\nAnd the corresponding reverse reads.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndadaRs[[1]]\n```\n:::\n\n\n```\ndada-class: object describing DADA2 denoising results\n8 sequence variants were inferred from 255 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\n```\n\nThese outputs tell us how many true sequence variants the DADA2 algorithm inferred from the unique sequences, in this case the sample 1.\n\n### 7. Merge Paired Reads\n\nWe now merge the forward and reverse reads together to obtain the full denoised sequence dataset. Merging is performed by aligning the denoised forward reads with the reverse-complement of the corresponding denoised reverse reads, and then constructing the merged “contig” sequences. By default, merged sequences are only output if the forward and reverse reads overlap by at least 12 bases, and are identical to each other in the overlap region (but these conditions can be changed via function arguments).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\n```\n:::\n\n\nThe `mergers` objects are lists of data.frames from each sample. Each data.frame contains the merged sequence, its abundance, and the indices of the forward and reverse sequence variants that were merged. Paired reads that did not exactly overlap were removed by `mergePairs`, further reducing spurious output.\n\n### 8. Construct Sequence Table\n\nNow we construct an amplicon sequence variant (ASV) table.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBCS_34 <- makeSequenceTable(mergers)\ndim(BCS_34)\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\nLooks like we have **18373** sequence variants from **190** samples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(nchar(getSequences(BCS_34)))\n```\n:::\n\n\n```\n  220   221   222   223   224   225   226   227   229   230   231   234   236 \n   14    21     2     6     1     2     2     9     1     1     2     2   101 \n  237   238   239   241   242   243   244   245   246   247   248   249   250 \n   62     1     1    11    16     6     4     4     4     2     4     5     6 \n  251   252   253   254   255   256   257   258   259   261   265   270   271 \n   20   697 16533   584    52    15    11     5     1     2     1     2     1 \n  274   278   279   285   286   290   291   295   308   309   322   325   328 \n    1     1     1     1     1     1     1     1     1     1     1     1     1 \n  335   336   337   338   339   340   341   342   343   344   345   347   348 \n    2     1     4     1     5     1     4     8     2     3     2     1     5 \n  349   356   357   358   359   360   361   362   363   364   365   366   367 \n    1     1     1     3     2     1    14    15     8    25    16     2     1 \n  368   371   372   373   376   377   378   385   386 \n    3     2     2     4     2     1     1     1     1  \n```\n\nThe sequence table is a matrix with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants. We have **18373** sequence variants but also a range of sequence lengths. Since many of these sequence variants are singletons or doubletons, we can just select a range that corresponds to the expected amplicon length and eliminate the spurious reads. But we will do this step after merging sequence tables from all 6 runs. \n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Distribution of read length by total ASVs before removing extreme length variants.](include/figures/read_length_before_pseudo_BCS_34.png){width=1050}\n:::\n:::\n\n\n### 9. Tracking Reads \n\nIf you are interested how reads changed up to this point in the pipeline please see the [Dada2 Data Portal](/workflows/portal/data-dada2.qmd) page for more details. \n\n\n::: {.cell}\n\n:::\n\n\nAnd save the sequence table to an `RDS` file.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsaveRDS(BCS_34, \"BCS_34.rds\")\n```\n:::\n\n\n\n## BCS_35\n\n\n::: {.cell}\n\n:::\n\n\n### 1. Set Working Environment\n\n:::{.callout-note}\nThe following plate were sequenced in this run: ISTHMO S9, S10, S11, S12.  Access the source code for [BCS_35 processing](include/_BCS_35.qmd).  \n:::\n\nFirst, we setup the working environment by defining a path for the working directory.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npath <- \"BCS_35/\"\nhead(list.files(path)) \n```\n:::\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Control_13_R1_001.trimmed.fastq\" \"Control_13_R2_001.trimmed.fastq\"\n[3] \"Control_14_R1_001.trimmed.fastq\" \"Control_14_R2_001.trimmed.fastq\"\n[5] \"Control_15_R1_001.trimmed.fastq\" \"Control_15_R2_001.trimmed.fastq\"\n```\n\n\n:::\n:::\n\n\nThen, we generate matched lists of the forward and reverse read files. We also parse out the sample name.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfnFs <- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs <- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names <- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs <- file.path(path, fnFs)\nfnRs <- file.path(path, fnRs)\n```\n:::\n\n\n### 2. Plot quality scores\n\nAnd next we plot a visual summary of the distribution of quality scores as a function of sequence position for the input fastq files. Here we set the number of records to sample from  each fastq file (`n`) to  20000 reads. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nqprofile_fwd <- plotQualityProfile(fnFs[1:x], \n                                   aggregate = TRUE, \n                                   n = 20000)\nqprofile_rev <- plotQualityProfile(fnRs[1:x], \n                                   aggregate = TRUE, \n                                   n = 20000)\nqprofile <- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Aggregated quality score plots for forward (left) & reverse (right) reads.](include/figures/BCS_35_filt_plot_qscores.png){width=1050}\n:::\n:::\n\n\n### 3. Filtering\n\nWe again make some path variables and setup a new directory of filtered reads.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfiltFs <- file.path(path, \"filtered\", \n                    paste0(sample.names, \"_F_filt.fastq\")\n                    )\nfiltRs <- file.path(path, \"filtered\", \n                    paste0(sample.names, \"_R_filt.fastq\")\n                    )\nnames(filtFs) <- sample.names\nnames(filtRs) <- sample.names\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, \n                     truncLen=c(220,180), \n                     maxN = 0, maxEE = 2, truncQ = 2, \n                     rm.phix = TRUE, compress = TRUE, \n                     multithread = 20)\n```\n:::\n\n\n:::{.callout-note}\nThese parameters should be set based on the anticipated length of the amplicon and the read quality.\n:::\n\nAnd here is a table of how the filtering step affected the number of reads in each sample. \n\n\n::: {.cell}\n\n:::\n\n\n### 4. Learn Error Rates\n\nNow it is time to assess the error rate of the data. The DADA2 algorithm uses a parametric error model. Every amplicon data set has a different set of error rates and the `learnErrors` method learns this error model *from the data*. It does this by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. The algorithm begins with an initial guess, for which the maximum possible error rates in the data are used.\n\n#### Forward Reads\n\n\n::: {.cell}\n\n```{.r .cell-code}\nerrF <- learnErrors(filtFs, multithread = TRUE)\n```\n:::\n\n\n```\n101017840 total bases in 459172 reads from 48 samples \nwill be used for learning the error rates.\n```\n\nWe can then plot the error rates for the forward reads. \n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np3 <- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_35_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_35_plot_errorF_2.png\", p3)\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Forward reads: Observed frequency of each transition (e.g., T -> G) as a function of the associated quality score.](include/figures/BCS_35_plot_errorF_2.png){width=1050}\n:::\n:::\n\n\n#### Reverse Reads\n\nAnd now we can process the reverse read error rates. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nerrR <- learnErrors(filtRs, multithread = TRUE)\n```\n:::\n\n\n```\n100695240 total bases in 559418 reads from 59 samples \nwill be used for learning the error rates.\n```\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np4 <- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_35_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_35_plot_errorR_2.png\", p4)\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Reverse reads: Observed frequency of each transition (e.g., T -> G) as a function of the associated quality score.](include/figures/BCS_35_plot_errorR_2.png){width=1050}\n:::\n:::\n\n\nThe error rates for each possible transition (A to C, A to G, etc.) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. Here the estimated error rates (black line) are a good fit to the observed rates (points), and the error rates drop with increased quality as expected.\n\n### 5. Dereplicate Reads\n\nNow we can use `derepFastq` to identify the unique sequences in the forward and reverse fastq files.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsam.names <- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs <- derepFastq(filtFs)\nnames(derepFs) <- sam.names\nderepRs <- derepFastq(filtRs)\nnames(derepRs) <- sam.names\n```\n:::\n\n\n### 6. DADA2 & ASV Inference\n\nAt this point we are ready to apply the core sample inference algorithm (`dada`) to the filtered and trimmed sequence data. DADA2 offers three options for whether and how to pool samples for ASV inference. \n\nIf `pool = TRUE`, the algorithm will pool together all samples prior to sample inference.  \nIf `pool = FALSE`, sample inference is performed on each sample individually.  \nIf `pool = \"pseudo\"`, the algorithm will perform pseudo-pooling between individually processed samples.\n\nFor our final analysis, we chose `pool = pseudo` for this data set. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndadaFs <- dada(derepFs, err = errF, pool = \"pseudo\", \n               multithread = TRUE)\ndadaRs <- dada(derepRs, err = errR, pool = \"pseudo\",\n               multithread = TRUE)\n```\n:::\n\n\nAs an example, we can inspect the returned `dada-class` object for the forward reads from the sample #1:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndadaFs[[1]]\n```\n:::\n\n\n```\ndada-class: object describing DADA2 denoising results\n15 sequence variants were inferred from 432 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\n```\n\nAnd the corresponding reverse reads.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndadaRs[[1]]\n```\n:::\n\n\n```\ndada-class: object describing DADA2 denoising results\n19 sequence variants were inferred from 465 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\n```\n\nThese outputs tell us how many true sequence variants the DADA2 algorithm inferred from the unique sequences, in this case the sample 1.\n\n### 7. Merge Paired Reads\n\nWe now merge the forward and reverse reads together to obtain the full denoised sequence dataset. Merging is performed by aligning the denoised forward reads with the reverse-complement of the corresponding denoised reverse reads, and then constructing the merged “contig” sequences. By default, merged sequences are only output if the forward and reverse reads overlap by at least 12 bases, and are identical to each other in the overlap region (but these conditions can be changed via function arguments).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\n```\n:::\n\n\nThe `mergers` objects are lists of data.frames from each sample. Each data.frame contains the merged sequence, its abundance, and the indices of the forward and reverse sequence variants that were merged. Paired reads that did not exactly overlap were removed by `mergePairs`, further reducing spurious output.\n\n### 8. Construct Sequence Table\n\nNow we construct an amplicon sequence variant (ASV) table.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBCS_35 <- makeSequenceTable(mergers)\ndim(BCS_35)\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\nLooks like we have **23367** sequence variants from **379** samples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(nchar(getSequences(BCS_35)))\n```\n:::\n\n\n```\n  220   221   222   223   224   225   226   227   228   229   230   234   235 \n   16    28     5     9     2     2     4     4     1     1     1     1     2 \n  236   237   240   241   242   243   245   246   247   248   249   250   251 \n  147     4    10    76    16    27     5     1     2     3     2     8    31 \n  252   253   254   255   256   257   258   260   262   263   266   268   273 \n 1004 20727   767    44     9    22     1     3     3     1     1     1     3 \n  282   292   293   294   295   303   307   309   310   312   333   334   335 \n    1     1     3     1     1     1     1     1     1     1     1     4     1 \n  336   337   339   340   341   342   344   346   347   348   349   350   352 \n    1     6     6     3    39    23     4     1    19     5     5     3     3 \n  357   359   360   361   362   363   364   365   366   367   368   369   372 \n   13     1     8    64    46    18    33    26     4     4    14     3     1 \n  376   377 \n    1     1  \n```\n\nThe sequence table is a matrix with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants. We have **23367** sequence variants but also a range of sequence lengths. Since many of these sequence variants are singletons or doubletons, we can just select a range that corresponds to the expected amplicon length and eliminate the spurious reads. But we will do this step after merging sequence tables from all 6 runs. \n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Distribution of read length by total ASVs before removing extreme length variants.](include/figures/read_length_before_pseudo_BCS_35.png){width=1050}\n:::\n:::\n\n\n### 9. Tracking Reads \n\nIf you are interested how reads changed up to this point in the pipeline please see the [Dada2 Data Portal](/workflows/portal/data-dada2.qmd) page for more details. \n\n\n::: {.cell}\n\n:::\n\n\nAnd save the sequence table to an `RDS` file.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsaveRDS(BCS_35, \"BCS_35.rds\")\n```\n:::\n\n\n:::\n\n## Merged Runs Workflow\n\nNow it is time to combine the sequence tables from each run together into one merged sequences table.\n\n\n::: {.cell}\n\n:::\n\n\nWe start by reading in each sequence table.\n\n::: callout-note\nClick here to access the source code for the [Merge Runs](include/_MERGE_RUNS.qmd) section of the workflow.\n:::\n\n| Step | Command                 | What we’re doing                  |\n|------|-------------------------|-----------------------------------|\n| 10   | `mergeSequenceTables()` | merge seqtabs from all runs.      |\n| 11   | `removeBimeraDenovo()`  | screen for & remove chimeras      |\n| 12   |                         | track reads through workflow      |\n| 13   | `assignTaxonomy()`      | assign taxonomy & finish workflow |\n\n```{mermaid}\n%%| eval: true\n%%| echo: false\n%%| fig-align: center\nflowchart LR\n    D(BCS_26<br/>BCS_28<br/>BCS_29<br/>BCS_30<br/>BCS_34<br/>BCS_35<br/>) --> M\n    D --> M\n    D --> M\n    D --> M\n    D --> M\n    D --> M\n    M(mergeSequenceTables) --> N(removeBimeraDenovo)\n    N --> O(assignTaxonomy)\n```\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBCS_26 <- readRDS(\"`BCS_26.rds\")\nBCS_28 <- readRDS(\"`BCS_28.rds\")\nBCS_29 <- readRDS(\"`BCS_29.rds\")\nBCS_30 <- readRDS(\"`BCS_30.rds\")\nBCS_34 <- readRDS(\"`BCS_34.rds\")\nBCS_35 <- readRDS(\"`BCS_35.rds\")\n```\n:::\n\n\n### 1. Merge Sequencing Tables\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseqtab.merge <- mergeSequenceTables(BCS_26, BCS_28, BCS_29, \n                                    BCS_30, BCS_34, BCS_35)\ndim(seqtab.merge)\n```\n:::\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  1909 96680\n```\n\n\n:::\n:::\n\n\nSo our count is 96680 ASVs across 1909 samples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(nchar(getSequences(seqtab.merge)))\n```\n:::\n\n\n```         \n  220   221   222   223   224   225   226   227   228   229   230   231   232 \n  124    67    14    36    20    13    10    25     8     6     4    30     2 \n  234   235   236   237   238   239   240   241   242   243   244   245   246 \n    9     8  1371   151    41     6    96   401   291   443    31    14    13 \n  247   248   249   250   251   252   253   254   255   256   257   258   259 \n   26    23    19    48   159  3606 83887  3756   315   121    95    20    10 \n  260   261   262   263   264   265   266   267   268   269   270   271   272 \n    8    16     9     4     2     1     1     1     4     2     9     8     4 \n  273   274   275   276   277   278   279   280   281   282   284   285   286 \n    7     3     2     5     1     7     4     1     1     2     1     4     4 \n  288   289   290   291   292   293   294   295   296   297   298   300   303 \n    1     3     1     2     4     8     7     2     3     2     2     2     3 \n  304   305   307   308   309   310   311   312   313   315   316   317   318 \n    1     5     2     3     2     1     3     1     3     1     4     1     3 \n  319   320   321   322   323   324   325   326   328   329   330   332   333 \n    3     1     2     3     2     1     3     1     3     3     2     1     3 \n  334   335   336   337   338   339   340   341   342   343   344   345   346 \n   13     6     7    17     5    25    16    70    51     8     7     8     4 \n  347   348   349   350   351   352   353   354   355   356   357   358   359 \n   28    17    21    10     2    11     1     1     7     6    31     6    15 \n  360   361   362   363   364   365   366   367   368   369   370   371   372 \n   21   161   186    43   135   107    19     9    26     5     3     3     8 \n  373   374   376   377   378   379   380   384   385   386   387   388 \n   11     2     3     5     2     3     1     1     1     1     2     1 \n```\n\n\n::: {.cell}\n\n```{.r .cell-code}\nread_length_all <-  data.frame(nchar(getSequences(seqtab.merge)))\ncolnames(read_length_all) <- \"length\"\nplot_all <- qplot(length, data = read_length_all, geom = \"histogram\", \n                  binwidth = 1, xlab = \"read length\", \n                  ylab = \"total variants\", xlim = c(200,400)) \n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Distribution of read length by total ASVs after merging & before removing extreme length variants.](include/figures/read_length_before_collapse.png){width=1050}\n:::\n:::\n\n\nThen we remove length variants.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseqtab.trim <- seqtab.merge[,nchar(colnames(seqtab.merge)) %in% \n                              seq(252, 254)]\ndim(seqtab.trim)\n```\n:::\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  1909 91249\n```\n\n\n:::\n:::\n\n\nAnd now our count is 91249 ASVs across 1909 samples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(nchar(getSequences(seqtab.trim)))\n```\n:::\n\n\n```         \n 252   253   254 \n 3606 83887  3756 \n```\n\n### 2. Remove Chimeras\n\nEven though the `dada` method corrects substitution and indel errors, chimeric sequences remain. According to the DADA2 documentation, the accuracy of sequence variants after denoising makes identifying chimeric ASVs simpler than when dealing with fuzzy OTUs. Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant parent sequences.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseqtab.trim.nochim.consensus <- \n  removeBimeraDenovo(seqtab.trim, \n                     method = \"consensus\", \n                     multithread = 20,  \n                     verbose = TRUE)\ndim(seqtab.trim.nochim.consensus)\n```\n:::\n\n\n```         \nIdentified 18398 bimeras out of 91249 input sequences.\n```\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  1909 72851\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(seqtab.nochim)/sum(seqtab.2)\n```\n:::\n\n\n```         \n[1] 0.9669996\n```\n\n\n::: {.cell}\n\n:::\n\n\nChimera checking removed an additional 18398 sequence variants however, when we account for the abundances of each variant, we see chimeras accounts for about 3.30004% of the merged sequence reads. Not bad.\n\n### 3. Track Reads through Workflow\n\nAt this point we can look at the number of reads that made it through each step of the workflow for every sample.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngetN <- function(x) sum(getUniques(x))\ntrack <- cbind(rowSums(seqtab), \n               rowSums(seqtab.trim), \n               rowSums(seqtab.trim.nochim.pool), \n               rowSums(seqtab.trim.nochim.consensus))\n\ncolnames(track) <- c(\"merged\", \"trim\", \n                     \"chimera_pool\", \n                     \"chimera_concensus\")\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n### 4. Assign Taxonomy\n\nThe `assignTaxonomy` command implements the naive Bayesian classifier, so for reproducible results you need to set a random number seed (see issue [#538](https://github.com/benjjneb/dada2/issues/538)). We did this at the beginning of the workflow. For taxonomic assignment, we are using the GSR database [@molano2024gsr]. The developers of DADA2 maintain [formatted versions of popular databases](https://benjjneb.github.io/dada2/training.html), however the GSR-DB has not been formatted by the developers yet.\n\n::: callout-note\nClick the link to can download an appropriate version of the  [GSR database](https://manichanh.vhir.org/gsrdb/download_db_links2.php).\n:::\n\nTo create a DADA2 formatted version GSR-DB[^_merge_runs-1], we perform the following steps.\n\n[^_merge_runs-1]: From the developers: GSR database (Greengenes, SILVA, and RDP database) is an integrated and manually curated database for bacterial and archaeal 16S amplicon taxonomy analysis. Unlike previous integration approaches, this database creation pipeline includes a taxonomy unification step to ensure consistency in taxonomical annotations. The database was validated with three mock communities and two real datasets and compared with existing 16S databases such as Greengenes, GTDB, ITGDB, SILVA, RDP, and MetaSquare. Results showed that the GSR database enhances taxonomical annotations of 16S sequences, outperforming current 16S databases at the species level. The GSR database is available for full-length 16S sequences and the most commonly used hypervariable regions: V4, V1-V3, V3-V4, and V3-V5.\n\n#### Download a data base\n\nHere we are using the [GSR V4 database](https://manichanh.vhir.org/gsrdb/GSR-DB_V4_cluster-1.tar.gz). \n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nwget https://manichanh.vhir.org/gsrdb/GSR-DB_V4_cluster-1.tar.gz\ntar -xvzf GSR-DB_V4_cluster-1.tar.gz\n```\n:::\n\n\nOnce you uncompress the `tar` file you should see four files, two `.qza` files (which you can ignore), a `_taxa.txt` file and a `_seqs.fasta` file. We are interested in the latter two files. These are the files we need to format for DADA2. How about we have a look at each file?\n\nFirst the taxonomy file. \n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nhead GSR-DB_V4_cluster-1_taxa.txt\n```\n:::\n\n\n```         \nFeature ID  Taxon\nAY999846    k__Bacteria; p__Actinobacteria; c__Actinomycetia; o__Actinomycetales-Streptomycetales-Unknown; f__Streptomycetaceae-Unknown; g__Streptomyces-Unknown; s__Unknown\nJN885187.1.1362 k__Bacteria; p__Actinobacteria; c__Actinomycetia; o__Actinomycetales-Streptomycetales-Unknown; f__Streptomycetaceae-Unknown; g__Kitasatospora-Streptomyces-Unknown; s__Unknown\n```\n\nAnd next the fasta file. \n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nhead GSR-DB_V4_cluster-1_seqs.fasta\n```\n:::\n\n\n```         \n>AY999846\nTACGTAGGGCGCAAGCGTTGTCCGGAATTATTGGGCGTAAAGAGCTCGTAGGCGGCTTGT\nCACGTCGGTTGTGAAAGCCCGGGGCTTAACCCCGGGTCTGCAGTCGATACGGGCAGGCTA\nGAGTTCGGTAGGGGAGATCGGAATTCCTGGTGTAGCGGTGAAATGCGCAGATATCAGGAG\nGAACACCGGTGGCGAAGGCGGATCTCTGGGCCGATACTGACGCTGAGGAGCGAAAGCGTG\nGGGAGCGAACAGG\n>JN885187.1.1362\nTACGTAGGGCGCGAGCGTTGTCCGGAATTATTGGGCGTAAAGAGCTCGTAGGCGGCTTGT\nCACGTCGGTTGTGAAAGCCCGGGGCTTAACCCCGGGTCTGCAGTCGATACGGGCAGGCTA\nGAGTTCGGTAGGGGAGATCGGAATTCCTGGTGTAGCGGTGAAATGCGCAGATATCAGGAG\n```\n\nDADA2 requires a very specific format for classification. For the next few step we use a tool called SeqKit [@shen2024seqkit2] for fasta defline manipulation.  \n\nThis first step replaces the original fasta defline with the correspoding lineage information. \n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nconda activate seqkit\nseqkit replace -w 0  -p \"(.+)\" -r '{kv}' -k GSR-DB_V4_cluster-1_taxa.txt GSR-DB_V4_cluster-1_seqs.fasta > tmp_1.fa\n```\n:::\n\n\n```         \n[INFO] read key-value file: GSR-DB_V4_cluster-1_taxa.txt\n[INFO] 38802 pairs of key-value loaded\n```\n\nHere is what the first few entries look like. \n\n```\n>k__Bacteria; p__Actinobacteria; c__Actinomycetia; o__Actinomycetales-Streptomycetales-Unknown; f__Streptomycetaceae-Unknown; g__Streptomyces-Unknown; s__Unknown\nTACGTAGGGCGCAAGCGTTGTCCGGAATTATTGGGCGTAAAGAGCTCGTAGGCGGCTTGTCACGTCGGTTGTGAAAGCCCGGGGCTTAACCCCGGGTCTGCAGTCGATACGGGCAGGCTAGAGTTCGGTAGGGGAGATCGGAATTCCTGGTGTAGCGGTGAAATGCGCAGATATCAGGAGGAACACCGGTGGCGAAGGCGGATCTCTGGGCCGATACTGACGCTGAGGAGCGAAAGCGTGGGGAGCGAACAGG\n>k__Bacteria; p__Actinobacteria; c__Actinomycetia; o__Actinomycetales-Streptomycetales-Unknown; f__Streptomycetaceae-Unknown; g__Kitasatospora-Streptomyces-Unknown; s__Unknown\nTACGTAGGGCGCGAGCGTTGTCCGGAATTATTGGGCGTAAAGAGCTCGTAGGCGGCTTGTCACGTCGGTTGTGAAAGCCCGGGGCTTAACCCCGGGTCTGCAGTCGATACGGGCAGGCTAGAGTTCGGTAGGGGAGATCGGAATTCCTGGTGTAGCGGTGAAATGCGCAGATATCAGGAGGAACACCGGTGGCGAAGGCGGATCTCTGGGCCGATACTGACGCTGAGGAGCGAAAGCGTGGGGAGCGAACAGGATTAGATACCCTGGTAGTCCACGCCGTAAACGGTGGGCACTAGGTGTAGG\n```\n\nNext we tidy up the deflines to remove spaces and leading taxonomic rank designations. \n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nseqkit replace -w 0  -p \" s__.*\" -r ''  tmp_1.fa > tmp_2.fa\nseqkit replace -w 0  -p \"\\s\" -r ''  tmp_2.fa > tmp_3.fa\nseqkit replace -w 0  -p \"\\w__\" -r ''  tmp_3.fa > gsrdb_dada2.fa\nrm tmp_*\n```\n:::\n\n\nAnd here are the first few lines of the final formatted GSR-DB fasta file. \n\n```         \n>Bacteria;Actinobacteria;Actinomycetia;Actinomycetales-Streptomycetales-Unknown;Streptomycetaceae-Unknown;Streptomyces-Unknown;\nTACGTAGGGCGCAAGCGTTGTCCGGAATTATTGGGCGTAAAGAGCTCGTAGGCGGCTTGTCACGTCGGTTGTGAAAGCCCGGGGCTTAACCCCGGGTCTGCAGTCGATACGGGCAGGCTAGAGTTCGGTAGGGGAGATCGGAATTCCTGGTGTAGCGGTGAAATGCGCAGATATCAGGAGGAACACCGGTGGCGAAGGCGGATCTCTGGGCCGATACTGACGCTGAGGAGCGAAAGCGTGGGGAGCGAACAGG\n>Bacteria;Actinobacteria;Actinomycetia;Actinomycetales-Streptomycetales-Unknown;Streptomycetaceae-Unknown;Kitasatospora-Streptomyces-Unknown;\nTACGTAGGGCGCGAGCGTTGTCCGGAATTATTGGGCGTAAAGAGCTCGTAGGCGGCTTGTCACGTCGGTTGTGAAAGCCCGGGGCTTAACCCCGGGTCTGCAGTCGATACGGGCAGGCTAGAGTTCGGTAGGGGAGATCGGAATTCCTGGTGTAGCGGTGAAATGCGCAGATATCAGGAGGAACACCGGTGGCGAAGGCGGATCTCTGGGCCGATACTGACGCTGAGGAGCGAAAGCGTGGGGAGCGAACAGGATTAGATACCCTGGTAGTCCACGCCGTAAACGGTGGGCACTAGGTGTAGG\n```\n\nNow we can run the classification step. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nseqtab.consensus <- seqtab.trim.nochim.consensus\ntax_gsrdb.consensus <- \n  assignTaxonomy(seqtab.consensus, \n                 \"TAXONOMY_FILES/gsrdb_dada2.fa\",\n                 multithread = TRUE, \n                 verbose = TRUE)\nsaveRDS(tax_gsrdb.consensus, \"4.tax_gsrdb.consensus.rds\")\n```\n:::\n\n\n## R Session Information & Code\n\nThis workflow was run on the [Smithsonian High Performance Cluster (SI/HPC)](https://doi.org/10.25572/SIHPC), Smithsonian Institution. Below are the specific packages and versions used in this workflow using both `sessionInfo()` and `devtools::session_info()`. Click the arrow to see the details.\n\n<details>\n\n<summary>Expand to see R Session Info</summary>\n\n```         \nsessionInfo()\nR version 4.3.2 (2023-10-31)\nPlatform: x86_64-conda-linux-gnu (64-bit)\nRunning under: Rocky Linux 8.9 (Green Obsidian)\n\nMatrix products: default\nBLAS/LAPACK: /home/scottjj/miniconda3/envs/R/lib/libopenblasp-r0.3.25.so;  LAPACK version 3.11.0\n\nlocale:\n[1] C\n\ntime zone: America/New_York\ntzcode source: system (glibc)\n\nattached base packages:\n [1] parallel  stats4    grid      stats     graphics  grDevices utils    \n [8] datasets  methods   base     \n\nother attached packages:\n [1] DECIPHER_2.30.0             RSQLite_2.3.4              \n [3] ShortRead_1.60.0            GenomicAlignments_1.38.0   \n [5] SummarizedExperiment_1.32.0 Biobase_2.62.0             \n [7] MatrixGenerics_1.14.0       matrixStats_1.2.0          \n [9] Rsamtools_2.18.0            GenomicRanges_1.54.1       \n[11] Biostrings_2.70.1           GenomeInfoDb_1.38.2        \n[13] XVector_0.42.0              IRanges_2.36.0             \n[15] S4Vectors_0.40.2            BiocParallel_1.36.0        \n[17] BiocGenerics_0.48.1         decontam_1.22.0            \n[19] dplyr_1.1.4                 gridExtra_2.3              \n[21] phyloseq_1.46.0             ff_4.0.9                   \n[23] bit_4.0.5                   ggplot2_3.4.4              \n[25] dada2_1.30.0                Rcpp_1.0.11                \n\nloaded via a namespace (and not attached):\n [1] DBI_1.2.0               bitops_1.0-7            deldir_2.0-2           \n [4] permute_0.9-7           rlang_1.1.2             magrittr_2.0.3         \n [7] ade4_1.7-22             compiler_4.3.2          mgcv_1.9-1             \n[10] systemfonts_1.0.5       png_0.1-8               vctrs_0.6.5            \n[13] reshape2_1.4.4          stringr_1.5.1           pkgconfig_2.0.3        \n[16] crayon_1.5.2            fastmap_1.1.1           labeling_0.4.3         \n[19] utf8_1.2.4              ragg_1.2.7              zlibbioc_1.48.0        \n[22] cachem_1.0.8            jsonlite_1.8.8          biomformat_1.30.0      \n[25] blob_1.2.4              rhdf5filters_1.14.1     DelayedArray_0.28.0    \n[28] Rhdf5lib_1.24.1         jpeg_0.1-10             cluster_2.1.6          \n[31] R6_2.5.1                stringi_1.8.3           RColorBrewer_1.1-3     \n[34] iterators_1.0.14        Matrix_1.6-4            splines_4.3.2          \n[37] igraph_1.6.0            tidyselect_1.2.0        abind_1.4-5            \n[40] vegan_2.6-4             codetools_0.2-19        hwriter_1.3.2.1        \n[43] lattice_0.22-5          tibble_3.2.1            plyr_1.8.9             \n[46] withr_2.5.2             survival_3.5-7          RcppParallel_5.1.7     \n[49] pillar_1.9.0            foreach_1.5.2           generics_0.1.3         \n[52] RCurl_1.98-1.13         munsell_0.5.0           scales_1.3.0           \n[55] glue_1.6.2              tools_4.3.2             interp_1.1-5           \n[58] data.table_1.14.10      rhdf5_2.46.1            ape_5.7-1              \n[61] latticeExtra_0.6-30     colorspace_2.1-0        nlme_3.1-164           \n[64] GenomeInfoDbData_1.2.11 cli_3.6.2               textshaping_0.3.7      \n[67] fansi_1.0.6             S4Arrays_1.2.0          gtable_0.3.4           \n[70] digest_0.6.33           SparseArray_1.2.2       farver_2.1.1           \n[73] memoise_2.0.1           multtest_2.58.0         lifecycle_1.0.4        \n[76] bit64_4.0.5             MASS_7.3-60            \n\ndevtools::session_info()\n- Session info ---------------------------------------------------------------\n setting  value\n version  R version 4.3.2 (2023-10-31)\n os       Rocky Linux 8.9 (Green Obsidian)\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  C\n ctype    C\n tz       America/New_York\n date     2024-08-15\n pandoc   3.1.3 @ /home/scottjj/miniconda3/envs/R/bin/pandoc\n\n- Packages -------------------------------------------------------------------\n package              * version   date (UTC) lib source\n abind                  1.4-5     2016-07-21 [1] CRAN (R 4.3.2)\n ade4                   1.7-22    2023-02-06 [1] CRAN (R 4.3.2)\n ape                    5.7-1     2023-03-13 [1] CRAN (R 4.3.2)\n Biobase              * 2.62.0    2023-10-24 [1] Bioconductor\n BiocGenerics         * 0.48.1    2023-11-01 [1] Bioconductor\n BiocParallel         * 1.36.0    2023-10-24 [1] Bioconductor\n biomformat             1.30.0    2023-10-24 [1] Bioconductor\n Biostrings           * 2.70.1    2023-10-25 [1] Bioconductor\n bit                  * 4.0.5     2022-11-15 [1] CRAN (R 4.3.0)\n bit64                  4.0.5     2020-08-30 [1] CRAN (R 4.3.0)\n bitops                 1.0-7     2021-04-24 [1] CRAN (R 4.3.2)\n blob                   1.2.4     2023-03-17 [1] CRAN (R 4.3.0)\n cachem                 1.0.8     2023-05-01 [1] CRAN (R 4.3.0)\n cli                    3.6.2     2023-12-11 [1] CRAN (R 4.3.2)\n cluster                2.1.6     2023-12-01 [1] CRAN (R 4.3.2)\n codetools              0.2-19    2023-02-01 [1] CRAN (R 4.3.0)\n colorspace             2.1-0     2023-01-23 [1] CRAN (R 4.3.0)\n crayon                 1.5.2     2022-09-29 [1] CRAN (R 4.3.0)\n dada2                * 1.30.0    2023-10-24 [1] Bioconductor\n data.table             1.14.10   2023-12-08 [1] CRAN (R 4.3.2)\n DBI                    1.2.0     2023-12-21 [1] CRAN (R 4.3.2)\n DECIPHER             * 2.30.0    2023-10-24 [1] Bioconductor\n decontam             * 1.22.0    2023-10-24 [1] Bioconductor\n DelayedArray           0.28.0    2023-10-24 [1] Bioconductor\n deldir                 2.0-2     2023-11-23 [1] CRAN (R 4.3.2)\n devtools               2.4.5     2022-10-11 [1] CRAN (R 4.3.2)\n digest                 0.6.33    2023-07-07 [1] CRAN (R 4.3.0)\n dplyr                * 1.1.4     2023-11-17 [1] CRAN (R 4.3.2)\n ellipsis               0.3.2     2021-04-29 [1] CRAN (R 4.3.0)\n fansi                  1.0.6     2023-12-08 [1] CRAN (R 4.3.2)\n farver                 2.1.1     2022-07-06 [1] CRAN (R 4.3.0)\n fastmap                1.1.1     2023-02-24 [1] CRAN (R 4.3.0)\n ff                   * 4.0.9     2023-01-25 [1] CRAN (R 4.3.2)\n foreach                1.5.2     2022-02-02 [1] CRAN (R 4.3.0)\n fs                     1.6.3     2023-07-20 [1] CRAN (R 4.3.1)\n generics               0.1.3     2022-07-05 [1] CRAN (R 4.3.0)\n GenomeInfoDb         * 1.38.2    2023-12-13 [1] Bioconductor 3.18 (R 4.3.2)\n GenomeInfoDbData       1.2.11    2023-12-26 [1] Bioconductor\n GenomicAlignments    * 1.38.0    2023-10-24 [1] Bioconductor\n GenomicRanges        * 1.54.1    2023-10-29 [1] Bioconductor\n ggplot2              * 3.4.4     2023-10-12 [1] CRAN (R 4.3.1)\n glue                   1.6.2     2022-02-24 [1] CRAN (R 4.3.0)\n gridExtra            * 2.3       2017-09-09 [1] CRAN (R 4.3.2)\n gtable                 0.3.4     2023-08-21 [1] CRAN (R 4.3.1)\n htmltools              0.5.7     2023-11-03 [1] CRAN (R 4.3.2)\n htmlwidgets            1.6.4     2023-12-06 [1] CRAN (R 4.3.2)\n httpuv                 1.6.13    2023-12-06 [1] CRAN (R 4.3.2)\n hwriter                1.3.2.1   2022-04-08 [1] CRAN (R 4.3.2)\n igraph                 1.6.0     2023-12-11 [1] CRAN (R 4.3.2)\n interp                 1.1-5     2023-11-27 [1] CRAN (R 4.3.2)\n IRanges              * 2.36.0    2023-10-24 [1] Bioconductor\n iterators              1.0.14    2022-02-05 [1] CRAN (R 4.3.0)\n jpeg                   0.1-10    2022-11-29 [1] CRAN (R 4.3.2)\n jsonlite               1.8.8     2023-12-04 [1] CRAN (R 4.3.2)\n labeling               0.4.3     2023-08-29 [1] CRAN (R 4.3.1)\n later                  1.3.2     2023-12-06 [1] CRAN (R 4.3.2)\n lattice                0.22-5    2023-10-24 [1] CRAN (R 4.3.1)\n latticeExtra           0.6-30    2022-07-04 [1] CRAN (R 4.3.2)\n lifecycle              1.0.4     2023-11-07 [1] CRAN (R 4.3.2)\n magrittr               2.0.3     2022-03-30 [1] CRAN (R 4.3.0)\n MASS                   7.3-60    2023-05-04 [1] CRAN (R 4.3.0)\n Matrix                 1.6-4     2023-11-30 [1] CRAN (R 4.3.2)\n MatrixGenerics       * 1.14.0    2023-10-24 [1] Bioconductor\n matrixStats          * 1.2.0     2023-12-11 [1] CRAN (R 4.3.2)\n memoise                2.0.1     2021-11-26 [1] CRAN (R 4.3.0)\n mgcv                   1.9-1     2023-12-21 [1] CRAN (R 4.3.2)\n mime                   0.12      2021-09-28 [1] CRAN (R 4.3.0)\n miniUI                 0.1.1.1   2018-05-18 [1] CRAN (R 4.3.2)\n multtest               2.58.0    2023-10-24 [1] Bioconductor\n munsell                0.5.0     2018-06-12 [1] CRAN (R 4.3.0)\n nlme                   3.1-164   2023-11-27 [1] CRAN (R 4.3.2)\n permute                0.9-7     2022-01-27 [1] CRAN (R 4.3.2)\n phyloseq             * 1.46.0    2023-10-24 [1] Bioconductor\n pillar                 1.9.0     2023-03-22 [1] CRAN (R 4.3.0)\n pkgbuild               1.4.3     2023-12-10 [1] CRAN (R 4.3.2)\n pkgconfig              2.0.3     2019-09-22 [1] CRAN (R 4.3.0)\n pkgload                1.3.3     2023-09-22 [1] CRAN (R 4.3.2)\n plyr                   1.8.9     2023-10-02 [1] CRAN (R 4.3.1)\n png                    0.1-8     2022-11-29 [1] CRAN (R 4.3.2)\n profvis                0.3.8     2023-05-02 [1] CRAN (R 4.3.2)\n promises               1.2.1     2023-08-10 [1] CRAN (R 4.3.1)\n purrr                  1.0.2     2023-08-10 [1] CRAN (R 4.3.1)\n R6                     2.5.1     2021-08-19 [1] CRAN (R 4.3.0)\n ragg                   1.2.7     2023-12-11 [1] CRAN (R 4.3.2)\n RColorBrewer           1.1-3     2022-04-03 [1] CRAN (R 4.3.0)\n Rcpp                 * 1.0.11    2023-07-06 [1] CRAN (R 4.3.0)\n RcppParallel           5.1.7     2023-02-27 [1] CRAN (R 4.3.2)\n RCurl                  1.98-1.13 2023-11-02 [1] CRAN (R 4.3.2)\n remotes                2.4.2.1   2023-07-18 [1] CRAN (R 4.3.2)\n reshape2               1.4.4     2020-04-09 [1] CRAN (R 4.3.0)\n rhdf5                  2.46.1    2023-11-29 [1] Bioconductor 3.18 (R 4.3.2)\n rhdf5filters           1.14.1    2023-11-06 [1] Bioconductor\n Rhdf5lib               1.24.1    2023-12-11 [1] Bioconductor 3.18 (R 4.3.2)\n rlang                  1.1.2     2023-11-04 [1] CRAN (R 4.3.2)\n Rsamtools            * 2.18.0    2023-10-24 [1] Bioconductor\n RSQLite              * 2.3.4     2023-12-08 [1] CRAN (R 4.3.2)\n S4Arrays               1.2.0     2023-10-24 [1] Bioconductor\n S4Vectors            * 0.40.2    2023-11-23 [1] Bioconductor 3.18 (R 4.3.2)\n scales                 1.3.0     2023-11-28 [1] CRAN (R 4.3.2)\n sessioninfo            1.2.2     2021-12-06 [1] CRAN (R 4.3.2)\n shiny                  1.8.0     2023-11-17 [1] CRAN (R 4.3.2)\n ShortRead            * 1.60.0    2023-10-24 [1] Bioconductor\n SparseArray            1.2.2     2023-11-07 [1] Bioconductor\n stringi                1.8.3     2023-12-11 [1] CRAN (R 4.3.2)\n stringr                1.5.1     2023-11-14 [1] CRAN (R 4.3.2)\n SummarizedExperiment * 1.32.0    2023-10-24 [1] Bioconductor\n survival               3.5-7     2023-08-14 [1] CRAN (R 4.3.1)\n systemfonts            1.0.5     2023-10-09 [1] CRAN (R 4.3.1)\n textshaping            0.3.7     2023-10-09 [1] CRAN (R 4.3.1)\n tibble                 3.2.1     2023-03-20 [1] CRAN (R 4.3.0)\n tidyselect             1.2.0     2022-10-10 [1] CRAN (R 4.3.0)\n urlchecker             1.0.1     2021-11-30 [1] CRAN (R 4.3.2)\n usethis                2.2.2     2023-07-06 [1] CRAN (R 4.3.0)\n utf8                   1.2.4     2023-10-22 [1] CRAN (R 4.3.1)\n vctrs                  0.6.5     2023-12-01 [1] CRAN (R 4.3.2)\n vegan                  2.6-4     2022-10-11 [1] CRAN (R 4.3.2)\n withr                  2.5.2     2023-10-30 [1] CRAN (R 4.3.1)\n xtable                 1.8-4     2019-04-21 [1] CRAN (R 4.3.0)\n XVector              * 0.42.0    2023-10-24 [1] Bioconductor\n zlibbioc               1.48.0    2023-10-24 [1] Bioconductor\n```\n\n\n\n</details>\n\n<br/>\n\n<!------------------------------------------------------------------------>\n<!-------------------- Use this area to save things ---------------------->\n<!------------------------------------------------------------------------>\n\n\n# Data Set Prep\n\n\n::: {.cell}\n\n:::\n\n\nIn this next part of the workflow our main goal is to create a *microtable object* using the R package [microeco](https://joey711.github.io/phyloseq/) [@liu2021microeco]. The microtable will be used to store the ASV by sample data as well the taxonomic, fasta, and sample data in a single object. More on that in a moment.\n\nWe will also:\n\n-   Remove any ASVs without kingdom level classification.\\\n-   Revome any contaminants (chloroplast, mitochondria, etc.).\\\n-   Remove Negative Control (NC) samples.\\\n-   Remove any low-count samples.\n\n## Read Counts Assessment\n\nBefore we begin, let's create a summary table containing some basic sample metadata and the read count data from the [Sample Data](/sampledata/index.html) section of the workflow. We want to inspect how total reads changed through the workflow. Table headers are as follows:\n\n| Header             | Description                                                |\n|------------------------|------------------------------------------------|\n| `Sample ID`        | New sample ID based on Ocean, species, tissue, & unique ID |\n| `input rc`         | No. of raw reads                                           |\n| `final rc`         | Final read count after removing chimeras                   |\n| `per reads retain` | Percent of reads remaining from `input` to `final rc`      |\n| `total ASVs`       | No. of ASVs                                                |\n| `Ocean`            | Sampling ocean                                             |\n| `Morphospecies`    | Host shrimp species                                        |\n| `Tissue`           | Shrimp tissue type                                         |\n| `Habitat`          | Sampling habitat                                           |\n| `Site`             | Sampling site                                              |\n| `Taxon`            | Shrimp, environmental samples, Controls                    |\n| `Species_Pair`     | ASK MATT                                                   |\n| `Species_group`    | ASK MATT                                                   |\n| `Species_complex`  | ASK MATT                                                   |\n| `Run`              | Sequencing Run                                             |\n| `Plate`            | Plate name                                                 |\n\n<br/>\n\n\n::: {.cell}\n\n:::\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n## Prep Data for `microeco`\n\nLike any tool, the microeco package needs the data in a specific form. I formatted our data to match the mock data in the microeco tutorial, specifically [this section](https://chiliubio.github.io/microeco_tutorial/basic-class.html#prepare-the-example-data).\n\n### A. Taxonomy Table\n\nHere is what the taxonomy table looks like in the mock data.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n---------------------------------------------------------------------\n  &nbsp;      Kingdom           Phylum                 Class         \n---------- ------------- -------------------- -----------------------\n  OTU_50    k__Bacteria   p__Proteobacteria    c__Betaproteobacteria \n\n OTU_8058   k__Bacteria   p__Actinobacteria      c__Actinobacteria   \n\n OTU_7152   k__Bacteria   p__Verrucomicrobia    c__OPB35 soil group  \n---------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\nAnd here is the taxonomy table from the dada2 workflow. The name dada2 gives to each ASV is the actual sequence of the ASV. We need to change this for downstream analyses.\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n\n### Expand to see the partial content of the dada2 taxonomy table\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n                                                                                                                            &nbsp;                                                                                                                               Kingdom  \n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ----------\n TACGGAGGGTGCAAGCGTTAATCGGAATTACTGGGCGTAAAGCGCTCGTAGGCGGCACGCTAAGTCGGATGTGAAATCCCAGGGCTTAACCTTGGAACTGCATTCGATACTGGCGAGCTAGAGTGTGTGAGAGGGTAGTGGAATTCCAGGTGTAGCGGTGAAATGCGTAGATATCTGGAGGAACATCAGTGGCGAAGGCGACTACCTGGCACAACACTGACGCTGAGGAGCGAAAGCGTGGGGAGCAAACAGG   Bacteria \n\n TACGGAGGGTGCGAGCGTTAATCGGAATTACTGGGCGTAAAGCGCATGCAGGTGGTTTGTTAAGTCAGATGTGAAAGCCCGGGGCTCAACCTCGGAATAGCATTTGAAACTGGCAGACTAGAGTACTGTAGAGGGGGGTAGAATTTCAGGTGTAGCGGTGAAATGCGTAGAGATCTGAAGGAATACCGGTGGCGAAGGCGGCCCCCTGGACAGATACTGACACTCAGATGCGAAAGCGTGGGGAGCAAACAGG   Bacteria \n\n TACATAGGTGTCAAGCGTTATCCGGATTTATTGGGCGTAAAGAGTGCGCAGATGGTTTAATAAGTTTGGGGTTAAATGCTAAAGCTTAACTTTAGTATGCCTTGAAAACTGTTTTACTAGAGTGTGGTAGAAGTTGATGGAATTTCATGTGTAGCGGTGGAATGCGTAGATATATGAAGGAACACCAATGGCGAAGGCAATCAACTATGCCATTACTGACATTCATGCACGAAAGCGTGGGGAGCGAATAGG    Bacteria \n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nTable: Table continues below\n\n \n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n                                                                                                                            &nbsp;                                                                                                                                   Phylum     \n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ----------------\n TACGGAGGGTGCAAGCGTTAATCGGAATTACTGGGCGTAAAGCGCTCGTAGGCGGCACGCTAAGTCGGATGTGAAATCCCAGGGCTTAACCTTGGAACTGCATTCGATACTGGCGAGCTAGAGTGTGTGAGAGGGTAGTGGAATTCCAGGTGTAGCGGTGAAATGCGTAGATATCTGGAGGAACATCAGTGGCGAAGGCGACTACCTGGCACAACACTGACGCTGAGGAGCGAAAGCGTGGGGAGCAAACAGG   Proteobacteria \n\n TACGGAGGGTGCGAGCGTTAATCGGAATTACTGGGCGTAAAGCGCATGCAGGTGGTTTGTTAAGTCAGATGTGAAAGCCCGGGGCTCAACCTCGGAATAGCATTTGAAACTGGCAGACTAGAGTACTGTAGAGGGGGGTAGAATTTCAGGTGTAGCGGTGAAATGCGTAGAGATCTGAAGGAATACCGGTGGCGAAGGCGGCCCCCTGGACAGATACTGACACTCAGATGCGAAAGCGTGGGGAGCAAACAGG   Proteobacteria \n\n TACATAGGTGTCAAGCGTTATCCGGATTTATTGGGCGTAAAGAGTGCGCAGATGGTTTAATAAGTTTGGGGTTAAATGCTAAAGCTTAACTTTAGTATGCCTTGAAAACTGTTTTACTAGAGTGTGGTAGAAGTTGATGGAATTTCATGTGTAGCGGTGGAATGCGTAGATATATGAAGGAACACCAATGGCGAAGGCAATCAACTATGCCATTACTGACATTCATGCACGAAAGCGTGGGGAGCGAATAGG          NA       \n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nTable: Table continues below\n\n \n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n                                                                                                                            &nbsp;                                                                                                                                      Class        \n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ---------------------\n TACGGAGGGTGCAAGCGTTAATCGGAATTACTGGGCGTAAAGCGCTCGTAGGCGGCACGCTAAGTCGGATGTGAAATCCCAGGGCTTAACCTTGGAACTGCATTCGATACTGGCGAGCTAGAGTGTGTGAGAGGGTAGTGGAATTCCAGGTGTAGCGGTGAAATGCGTAGATATCTGGAGGAACATCAGTGGCGAAGGCGACTACCTGGCACAACACTGACGCTGAGGAGCGAAAGCGTGGGGAGCAAACAGG   Gammaproteobacteria \n\n TACGGAGGGTGCGAGCGTTAATCGGAATTACTGGGCGTAAAGCGCATGCAGGTGGTTTGTTAAGTCAGATGTGAAAGCCCGGGGCTCAACCTCGGAATAGCATTTGAAACTGGCAGACTAGAGTACTGTAGAGGGGGGTAGAATTTCAGGTGTAGCGGTGAAATGCGTAGAGATCTGAAGGAATACCGGTGGCGAAGGCGGCCCCCTGGACAGATACTGACACTCAGATGCGAAAGCGTGGGGAGCAAACAGG   Gammaproteobacteria \n\n TACATAGGTGTCAAGCGTTATCCGGATTTATTGGGCGTAAAGAGTGCGCAGATGGTTTAATAAGTTTGGGGTTAAATGCTAAAGCTTAACTTTAGTATGCCTTGAAAACTGTTTTACTAGAGTGTGGTAGAAGTTGATGGAATTTCATGTGTAGCGGTGGAATGCGTAGATATATGAAGGAACACCAATGGCGAAGGCAATCAACTATGCCATTACTGACATTCATGCACGAAAGCGTGGGGAGCGAATAGG            NA          \n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\n:::\n\nThe first step is to rename the amplicon sequence variants so the designations are a bit more user friendly. By default, DADA2 names each ASV by its unique sequence so that data can be directly compared across studies (which is great). But this convention can get cumbersome downstream, so we rename the ASVs using a simpler convention---ASV1, ASV2, ASV3, and so on.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp_tax <- data.frame(tax_gsrdb)\n# adding unique ASV names\nrow.names(tmp_tax) <- paste0(\"ASV\", seq(nrow(tmp_tax)))\n```\n:::\n\n\nAnd this is how the taxonomy table looks after assigning new names.\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n----------------------------------------------------------\n &nbsp;   Kingdom        Phylum              Class        \n-------- ---------- ---------------- ---------------------\n  ASV1    Bacteria   Proteobacteria   Gammaproteobacteria \n\n  ASV2    Bacteria   Proteobacteria   Gammaproteobacteria \n\n  ASV3    Bacteria         NA                 NA          \n----------------------------------------------------------\n```\n\n\n:::\n:::\n\n\nNext we need to add rank definitions to each classification.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrank_prefixes <- c(\n  Kingdom = \"k\", \n  Phylum  = \"p\", \n  Class   = \"c\", \n  Order   = \"o\", \n  Family  = \"f\", \n  Genus   = \"g\"\n)\n\ntmp_tax <- tmp_tax %>%\n  mutate(across(everything(), ~replace_na(., \"\"))) %>% \n  mutate(across(names(rank_prefixes), \n                ~ paste0(rank_prefixes[cur_column()], \"__\", .))) %>%\ntidy_taxonomy()\n```\n:::\n\n\nAnd then this. Exactly like the mock data. \n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n-------------------------------------------------------------------\n &nbsp;     Kingdom          Phylum                 Class          \n-------- ------------- ------------------- ------------------------\n  ASV1    k__Bacteria   p__Proteobacteria   c__Gammaproteobacteria \n\n  ASV2    k__Bacteria   p__Proteobacteria   c__Gammaproteobacteria \n\n  ASV3    k__Bacteria          p__                   c__           \n-------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\n### B. Sequence Table\n\nHere is what the sequence table looks like in the mock data.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n--------------------------------------------------------------------\n  &nbsp;    S1   S2   S3   S4   S5   S6   S7   S9   S10   S11   S12 \n---------- ---- ---- ---- ---- ---- ---- ---- ---- ----- ----- -----\n OTU_4272   1    0    1    1    0    0    1    1     0     1     1  \n\n OTU_236    1    4    0    2    35   5    94   0    177   14    27  \n\n OTU_399    9    2    2    4    4    0    3    6     0     1     2  \n--------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp_st <- data.frame(seqtab)\nidentical(colnames(tmp_st), row.names(tax_gsrdb))\nnames(tmp_st) <- row.names(tmp_tax)\n\ntmp_st <-  tmp_st %>% tibble::rownames_to_column()\n\ntmp_st <- tmp_st %>%\n  tidyr::pivot_longer(cols = c(-1), names_to = \"tmp\") %>%\n  tidyr::pivot_wider(names_from = c(1))\ntmp_st <- tibble::column_to_rownames(tmp_st, \"tmp\")\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\nAnd now the final, modified sequence table.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n--------------------------------------------------------------------\n &nbsp;   EP_A_AREN_MG_8652   EP_A_AREN_MG_8654   EP_A_AREN_MG_8697 \n-------- ------------------- ------------------- -------------------\n  ASV1            7                   0                   0         \n\n  ASV2          1113                1803                 31         \n\n  ASV3            0                   0                   0         \n--------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\n### C. Sample Table\n\nHere is what the sample table looks like in the mock data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npandoc.table(sample_info_16S[1:3,], emphasize.rownames = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n----------------------------------------------------\n &nbsp;   SampleID   Group   Type       Saline      \n-------- ---------- ------- ------ -----------------\n   S1        S1       IW      NE    Non-saline soil \n\n   S2        S2       IW      NE    Non-saline soil \n\n   S3        S3       IW      NE    Non-saline soil \n----------------------------------------------------\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamdf <- readRDS(here(\"share/ssu/sampledata/\", \"sample_data.rds\"))\nsamdf <- samdf %>% tibble::column_to_rownames(\"SampleID\")\nsamdf$SampleID <- rownames(samdf)\nsamdf <- samdf %>% relocate(SampleID)\n```\n:::\n\n\nAnd now a partial view of the final, modified sample table.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n-------------------------------------------------------------------------\n      &nbsp;             SampleID        OCEAN   SPECIES   TISSUE    ID  \n------------------- ------------------- ------- --------- -------- ------\n EP_A_AREN_EG_8651   EP_A_AREN_EG_8651    EP     A_AREN      EG     8651 \n\n EP_A_AREN_EG_8654   EP_A_AREN_EG_8654    EP     A_AREN      EG     8654 \n\n EP_A_AREN_EG_8698   EP_A_AREN_EG_8698    EP     A_AREN      EG     8698 \n-------------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\n## Experiment-level Objects\n\nIn the following section we create `microtable` and `phyloseq` objects. These are single, consolidated experiment-level data objects that organize various data types, such as OTU tables (taxa abundance), sample metadata, taxonomic classifications, as well as phylogenetic trees and reference sequences, into one structured format. This unified approach facilitates reproducible, interactive analysis and visualization by bringing all related information together. \n\nWe begin by creating the `microtable` and then use the function `meco2phyloseq` from the [file2meco](https://github.com/ChiLiubio/file2meco) package to create the `phyloseq` object. This way all of the underlying data is identical across the two objects. \n\n::: callout-note\nThese objects contain an ASV table (taxa abundances), sample metadata, and taxonomy table (mapping between ASVs and higher-level taxonomic classifications).\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_info <- samdf\ntax_tab <- tmp_tax\notu_tab <- tmp_st\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp_me <- microtable$new(sample_table = sample_info, \n                         otu_table = otu_tab, \n                         tax_table = tax_tab)\ntmp_me\n```\n:::\n\n\n```         \nmicrotable-class object:\nsample_table have 1909 rows and 13 columns\notu_table have 72851 rows and 1909 columns\ntax_table have 72851 rows and 6 columns\n```\n\n### Add Representative Sequence\n\nWe can also add representative sequences for each OTU/ASV. For this step, we can simply grab the sequences from the row names of the DADA2 taxonomy object loaded above.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp_seq <- data.frame(row.names(data.frame(tax_gsrdb)) )\ntmp_names <- data.frame(row.names(tax_tab))\ntmp_fa <- cbind(tmp_names, tmp_seq)\ncolnames(tmp_fa) <- c(\"ASV_ID\", \"ASV_SEQ\")\ntmp_fa$ASV_ID <- sub(\"^\", \">\", tmp_fa$ASV_ID)\n\nwrite.table(tmp_fa, here(work_here, \"rep_seq.fasta\"),\n            sep = \"\\n\", col.names = FALSE, row.names = FALSE,\n            quote = FALSE, fileEncoding = \"UTF-8\")       \nrep_fasta <- Biostrings::readDNAStringSet(here(work_here, \"rep_seq.fasta\"))\ntmp_me$rep_fasta <- rep_fasta\ntmp_me$tidy_dataset()\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n## Curate the Data Set\n\nPretty much the last thing to do is remove unwanted taxa, negative controls, and low-count samples.\n\n### Remove any Kingdom NAs\n\nHere we can just use the straight up `subset` command since we do not need to worry about any ranks above Kingdom also being removed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp_no_na <- microeco::clone(tmp_me)\ntmp_no_na$tax_table %<>% base::subset(Kingdom == \"k__Archaea\" | Kingdom == \"k__Bacteria\")\ntmp_no_na$tidy_dataset()\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nmicrotable-class object:\nsample_table have 1909 rows and 14 columns\notu_table have 72827 rows and 1909 columns\ntax_table have 72827 rows and 6 columns\nrep_fasta have 72827 sequences\n```\n\n\n:::\n:::\n\n\n### Remove Contaminants\n\nNow we can remove any potential contaminants like mitochondria or chloroplasts.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp_no_cont <- microeco::clone(tmp_no_na)\ntmp_no_cont$filter_pollution(taxa = c(\"mitochondria\", \"chloroplast\"))\ntmp_no_cont$tidy_dataset()\ntmp_no_cont\n```\n:::\n\n\n```         \nTotal 0 features are removed from tax_table ...\n```\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nmicrotable-class object:\nsample_table have 1909 rows and 14 columns\notu_table have 72827 rows and 1909 columns\ntax_table have 72827 rows and 6 columns\nrep_fasta have 72827 sequences\n```\n\n\n:::\n:::\n\n\n### Remove Negative Controls (NC)\n\nNow we need to remove the NC samples *and* ASVs found in those sample. We first identified all ASVs that were present in at least one NC sample represented by at least 1 read. We did this by subsetting the NC samples from the new microtable object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp_nc <- microeco::clone(tmp_no_cont)\ntmp_nc$sample_table <- subset(tmp_nc$sample_table, TAXON == \"Control\")\ntmp_nc$tidy_dataset()\ntmp_nc\n```\n:::\n\n\n```\n72231 taxa with 0 abundance are removed from the otu_table ...\n```\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nmicrotable-class object:\nsample_table have 60 rows and 14 columns\notu_table have 596 rows and 60 columns\ntax_table have 596 rows and 6 columns\nrep_fasta have 596 sequences\n```\n\n\n:::\n:::\n\n\nLooks like there are 596 ASVs in the NC samples from a total of 205834 reads.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnc_asvs <- row.names(tmp_nc$tax_table)\n```\n:::\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"ASV1\"  \"ASV2\"  \"ASV4\"  \"ASV5\"  \"ASV6\"  \"ASV7\"  \"ASV10\" \"ASV11\" \"ASV13\"\n[10] \"ASV14\" \"ASV15\" \"ASV16\" \"ASV17\" \"ASV18\" \"ASV20\" \"ASV22\" \"ASV23\" \"ASV26\"\n[19] \"ASV28\" \"ASV31\"\n```\n\n\n:::\n:::\n\n\nThere are 596 ASVs found in the NC sample. ASVs are numbered in order by total abundance in the data set so we know that many of the ASVs in the NC samples are not particularly abundant in the dataset. We can look at the abundance of these ASVs across all samples and compare it to the NC. This takes a bit of wrangling.\n\nEssentially, for each ASV, the code below calculates:\n\n-   The total number of NC samples containing at least 1 read.\\\n-   The total number of reads in NC samples.\\\n-   The total number of non-NC samples containing at least 1 read.\\\n-   The total number of reads in non-NC samples.\\\n-   The percent of reads in the NC samples and the percent of NC samples containing reads.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp_rem_nc <- microeco::clone(tmp_no_cont)\ntmp_rem_nc_df <- tmp_rem_nc$otu_table\ntmp_rem_nc_df <- tmp_rem_nc_df %>% \n                 dplyr::filter(row.names(tmp_rem_nc_df) %in% nc_asvs)\ntmp_rem_nc_df <- tmp_rem_nc_df %>% tibble::rownames_to_column(\"ASV_ID\")\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#-------provide a string unique to NC samples--------------#\nnc_name <- \"Control_\"\n#----------------------------------------------------------#\ntmp_rem_nc_df <- tmp_rem_nc_df  %>% \n  dplyr::mutate(total_reads_NC = rowSums(dplyr::select(., contains(nc_name))), \n         .after = \"ASV_ID\")\ntmp_rem_nc_df <- dplyr::select(tmp_rem_nc_df, -contains(nc_name))\ntmp_rem_nc_df <- tmp_rem_nc_df %>%\n  dplyr::mutate(total_reads_samps = rowSums(.[3:ncol(tmp_rem_nc_df)]), \n                .after = \"total_reads_NC\")\ntmp_rem_nc_df[, 4:ncol(tmp_rem_nc_df)] <- list(NULL)\ntmp_rem_nc_df <- tmp_rem_nc_df %>%\n  dplyr::mutate(perc_in_neg = 100*(\n    total_reads_NC / (total_reads_NC + total_reads_samps)),\n                .after = \"total_reads_samps\")\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp_rem_nc_df$perc_in_neg <- round(tmp_rem_nc_df$perc_in_neg, digits = 6)\n\ntmp_1 <- data.frame(rowSums(tmp_rem_nc$otu_table != 0)) %>% \n                   tibble::rownames_to_column(\"ASV_ID\") %>% \n                   dplyr::rename(\"total_samples\" = 2) \n\ntmp_2 <- dplyr::select(tmp_rem_nc$otu_table, contains(nc_name))\ntmp_2$num_samp_nc <- rowSums(tmp_2 != 0)\ntmp_2 <- dplyr::select(tmp_2, contains(\"num_samp_nc\")) %>% \n                      tibble::rownames_to_column(\"ASV_ID\")\n\ntmp_3 <- dplyr::select(tmp_rem_nc$otu_table, -contains(nc_name))\ntmp_3$num_samp_no_nc <- rowSums(tmp_3 != 0)\ntmp_3 <- dplyr::select(tmp_3, contains(\"num_samp_no_nc\")) %>% \n                      tibble::rownames_to_column(\"ASV_ID\")\n\ntmp_rem_nc_df <- dplyr::left_join(tmp_rem_nc_df, tmp_1) %>%\n                 dplyr::left_join(., tmp_2) %>%\n                 dplyr::left_join(., tmp_3)\n\ntmp_rem_nc_df <- tmp_rem_nc_df %>%\n  dplyr::mutate(perc_in_neg_samp = 100*( num_samp_nc / (num_samp_nc + num_samp_no_nc)),\n                .after = \"num_samp_no_nc\")\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnc_check <- tmp_rem_nc_df\n```\n:::\n\n\nLooking at these data we can see that ASVs like ASV1, ASV2, and so on, are only represented by a really small number of NC reads and samples. On the other hand, ASVs such as ASV91, ASV121, and ASV299 are relatively abundant in NC samples. We decided to remove ASVs if:\n\n-   The number of reads found in NC samples accounted for more than 10% of total reads OR\n-   The percent of NC samples containing the ASV was greater than 10% of total samples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnc_remove <- nc_check %>% \n  dplyr::filter(perc_in_neg > 10 | perc_in_neg_samp > 10)\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n|          | Total ASVs          | NC reads         | non NC reads      | \\% NC reads       |\n|----------|---------------------|------------------|-------------------|-------------------|\n| Removed  | 371 | 170482 | 233876 | 42.161 |\n| Retained | 225 | 35352 | 12394240 | 0.284 |\n\nWe identified a total of 596 ASVs that were present in at least 1 NC sample by at least 1 read. We removed any ASV where more than 10% of total reads were found in NC samples OR any ASV found in more than 10% of NC samples. Based on these criteria we removed 371 ASVs from the data set, which represented 170482 total reads in NC samples and 233876 total reads in non-NC samples. Of the total reads removed 42.161% came from NC samples. Of all ASVs identified in NC samples,225 were retained because the fell below the threshhold criteria. These ASVs accounted for 35352 reads in NC samples and 12394240 reads in non-NC samples. NC samples accounted for 0.284% of these reads.\n\nOK, now we can remove the NC samples and any ASVs that met our criteria described above.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp_no_nc <- microeco::clone(tmp_no_cont)\n\ntmp_rem_asv <- as.factor(nc_remove$ASV_ID)\ntmp_no_nc$otu_table <- tmp_rem_nc$otu_table %>% \n  filter(!row.names(tmp_no_nc$otu_table) %in% tmp_rem_asv)\ntmp_no_nc$tidy_dataset()\n\ntmp_no_nc$sample_table <- subset(tmp_no_nc$sample_table, \n                                 TAXON != \"Control_\")\ntmp_no_nc$tidy_dataset()\ntmp_no_nc\n```\n:::\n\n\n```         \n9 samples with 0 abundance are removed from the otu_table ...\n```\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nmicrotable-class object:\nsample_table have 1900 rows and 14 columns\notu_table have 72456 rows and 1900 columns\ntax_table have 72456 rows and 6 columns\nrep_fasta have 72456 sequences\n```\n\n\n:::\n:::\n\n\n### Remove Low-Count Samples\n\nNext, we can remove samples with really low read counts---here we set the threshold to `1000` reads.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp_no_low <- microeco::clone(tmp_no_nc)\ntmp_no_low$otu_table <- tmp_no_nc$otu_table %>%\n          dplyr::select(where(~ is.numeric(.) && sum(.) >= 1000))\ntmp_no_low$tidy_dataset()\ntmp_no_low\n```\n:::\n\n\n```         \n26 taxa with 0 abundance are removed from the otu_table ...\n```\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nmicrotable-class object:\nsample_table have 1848 rows and 14 columns\notu_table have 72430 rows and 1848 columns\ntax_table have 72430 rows and 6 columns\nrep_fasta have 72430 sequences\n```\n\n\n:::\n:::\n\n\nGiving us the final microtable object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nme_final <- microeco::clone(tmp_no_low)\n```\n:::\n\n\nLastly, we can use the package [`file2meco`](https://github.com/ChiLiubio/file2meco) to generate a [phyloseq](https://joey711.github.io/phyloseq/) object. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nps_final <- file2meco::meco2phyloseq(me_final)\n```\n:::\n\n\n## Summary\n\nNow time to summarize the data. For this we use the R package [miaverse](https://microbiome.github.io) [@felix2024mia].\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|me_dataset           | total_asvs| total_reads| total_samples|\n|:--------------------|----------:|-----------:|-------------:|\n|original             |      72851|    35764704|          1909|\n|no NA kingdoms       |      72827|    35760968|          1909|\n|no contaminants      |      72827|    35760968|          1909|\n|no negative controls |      72456|    35356610|          1900|\n|no low count samps   |      72430|    35345679|          1848|\n|final                |      72430|    35345679|          1848|\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell tbl-cap='Dataset metrics before and after curation.'}\n::: {.cell-output-display}\n\n\n|Metric                |Start    |End       |\n|:---------------------|:--------|:---------|\n|Min. no. of reads     |49       |1202      |\n|Max. no. of reads     |251022   |249532    |\n|Total no. of reads    |35764704 |35345679  |\n|Avg. no. of reads     |18735    |19126     |\n|Median no. of reads   |14827    |15149.500 |\n|Total ASVs            |72851    |72430     |\n|No. of singleton ASVs |106      |106       |\n|% of singleton ASVs   |0.146    |0.146     |\n|Sparsity              |0.996    |0.996     |\n\n\n:::\n:::\n\n\nWe started off with 72851 ASVs and 1909 samples. After curation of the dataset, there were 72430 ASVs and 1848 samples remaining.\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\nWe lost a total of 61 samples after curating the dataset. This includes 50 NC samples and 11 non-NC samples.\n\nHere is a list of non-NC samples that were removed. \n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"WA_A_BAHA_GL_7474\" \"WA_A_NUTT_EG_9410\" \"WA_A_WEBS_HP_9411\"\n [4] \"WA_A_CRIS_GL_9624\" \"WA_A_CRIS_HP_7536\" \"WA_A_FLOR_GL_9659\"\n [7] \"WA_A_PARA_GL_9474\" \"WA_A_PARA_MG_9364\" \"WA_A_PARA_ST_9477\"\n[10] \"WA_A_PCNS_MG_9513\" \"WA_A_THOM_ST_9389\"\n```\n\n\n:::\n:::\n\n\n# Download Results {#download-results}\n\nQuick access to read changes through the pipeline and ASVs detected in negative control samples.\n\n::: {#dada2-listing .column-body}\n:::\n\n<!------------------------------------------------------------------------>\n<!-------------------- Use this area to save things ---------------------->\n<!------------------------------------------------------------------------>\n\n<!--------------------------------------->\n<!-- These chunks are for curated data -->\n<!--------------------------------------->\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n<!-------------------------------------------->\n<!-- These chunks are for  processing data  -->\n<!-------------------------------------------->\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n#### Detailed Session Info {.appendix}\n\n{{< dstart summary=\"Expand to see Session Info\" >}}\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.5.1 (2025-06-13)\n os       macOS Ventura 13.7.8\n system   x86_64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/Los_Angeles\n date     2025-10-02\n pandoc   3.6.3 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/x86_64/ (via rmarkdown)\n quarto   1.8.25 @ /Applications/quarto/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package                  * version   date (UTC) lib source\n Biobase                  * 2.68.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n BiocGenerics             * 0.54.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n Biostrings               * 2.76.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n downloadthis             * 0.5.0     2025-09-26 [1] Github (fmmattioni/downloadthis@18e3e5a)\n dplyr                    * 1.1.4     2023-11-17 [1] CRAN (R 4.5.0)\n fontawesome              * 0.5.3     2024-11-16 [1] CRAN (R 4.5.0)\n forcats                  * 1.0.0     2023-01-29 [1] CRAN (R 4.5.0)\n fs                       * 1.6.6     2025-04-12 [1] CRAN (R 4.5.0)\n generics                 * 0.1.4     2025-05-09 [1] CRAN (R 4.5.0)\n GenomeInfoDb             * 1.44.2    2025-08-18 [1] Bioconductor 3.21 (R 4.5.1)\n GenomicRanges            * 1.60.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n ggplot2                  * 4.0.0     2025-09-11 [1] CRAN (R 4.5.1)\n here                     * 1.0.2     2025-09-15 [1] CRAN (R 4.5.1)\n htmltools                * 0.5.8.1   2024-04-04 [1] CRAN (R 4.5.0)\n IRanges                  * 2.42.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n lubridate                * 1.9.4     2024-12-08 [1] CRAN (R 4.5.0)\n magrittr                 * 2.0.4     2025-09-12 [1] CRAN (R 4.5.1)\n MatrixGenerics           * 1.20.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n matrixStats              * 1.5.0     2025-01-07 [1] CRAN (R 4.5.0)\n mia                      * 1.15.6    2024-11-22 [1] Bioconductor 3.21 (R 4.5.0)\n microbiome               * 1.30.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n microeco                 * 1.15.0    2025-05-18 [1] CRAN (R 4.5.0)\n microViz                 * 0.12.7    2025-08-01 [1] https://david-barnett.r-universe.dev (R 4.5.1)\n MultiAssayExperiment     * 1.34.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n pander                   * 0.6.6     2025-03-01 [1] CRAN (R 4.5.0)\n phyloseq                 * 1.52.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n purrr                    * 1.1.0     2025-07-10 [1] CRAN (R 4.5.1)\n R.methodsS3              * 1.8.2     2022-06-13 [1] CRAN (R 4.5.0)\n R.oo                     * 1.27.1    2025-05-02 [1] CRAN (R 4.5.0)\n R.utils                  * 2.13.0    2025-02-24 [1] CRAN (R 4.5.0)\n reactable                * 0.4.4     2023-03-12 [1] CRAN (R 4.5.0)\n reactablefmtr            * 2.0.0     2022-03-16 [1] CRAN (R 4.5.0)\n readr                    * 2.1.5     2024-01-10 [1] CRAN (R 4.5.0)\n S4Vectors                * 0.46.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n seqinr                   * 4.2-36    2023-12-08 [1] CRAN (R 4.5.0)\n sessioninfo              * 1.2.3     2025-02-05 [1] CRAN (R 4.5.0)\n SingleCellExperiment     * 1.30.1    2025-05-05 [1] Bioconductor 3.21 (R 4.5.0)\n stringr                  * 1.5.2     2025-09-08 [1] CRAN (R 4.5.1)\n SummarizedExperiment     * 1.38.1    2025-04-28 [1] Bioconductor 3.21 (R 4.5.0)\n tibble                   * 3.3.0     2025-06-08 [1] CRAN (R 4.5.0)\n tidyr                    * 1.3.1     2024-01-24 [1] CRAN (R 4.5.0)\n tidyverse                * 2.0.0     2023-02-22 [1] CRAN (R 4.5.0)\n tinytable                * 0.13.0.10 2025-09-07 [1] https://vincentarelbundock.r-universe.dev (R 4.5.1)\n TreeSummarizedExperiment * 2.16.1    2025-05-08 [1] Bioconductor 3.21 (R 4.5.0)\n XVector                  * 0.48.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n zip                      * 2.3.3     2025-05-13 [1] CRAN (R 4.5.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.5-x86_64/Resources/library\n * ── Packages attached to the search path.\n\n──────────────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n======   Devtools Session info   ===================================\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n package                  * version   date (UTC) lib source\n Biobase                  * 2.68.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n BiocGenerics             * 0.54.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n Biostrings               * 2.76.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n downloadthis             * 0.5.0     2025-09-26 [1] Github (fmmattioni/downloadthis@18e3e5a)\n dplyr                    * 1.1.4     2023-11-17 [1] CRAN (R 4.5.0)\n fontawesome              * 0.5.3     2024-11-16 [1] CRAN (R 4.5.0)\n forcats                  * 1.0.0     2023-01-29 [1] CRAN (R 4.5.0)\n fs                       * 1.6.6     2025-04-12 [1] CRAN (R 4.5.0)\n generics                 * 0.1.4     2025-05-09 [1] CRAN (R 4.5.0)\n GenomeInfoDb             * 1.44.2    2025-08-18 [1] Bioconductor 3.21 (R 4.5.1)\n GenomicRanges            * 1.60.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n ggplot2                  * 4.0.0     2025-09-11 [1] CRAN (R 4.5.1)\n here                     * 1.0.2     2025-09-15 [1] CRAN (R 4.5.1)\n htmltools                * 0.5.8.1   2024-04-04 [1] CRAN (R 4.5.0)\n IRanges                  * 2.42.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n lubridate                * 1.9.4     2024-12-08 [1] CRAN (R 4.5.0)\n magrittr                 * 2.0.4     2025-09-12 [1] CRAN (R 4.5.1)\n MatrixGenerics           * 1.20.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n matrixStats              * 1.5.0     2025-01-07 [1] CRAN (R 4.5.0)\n mia                      * 1.15.6    2024-11-22 [1] Bioconductor 3.21 (R 4.5.0)\n microbiome               * 1.30.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n microeco                 * 1.15.0    2025-05-18 [1] CRAN (R 4.5.0)\n microViz                 * 0.12.7    2025-08-01 [1] https://david-barnett.r-universe.dev (R 4.5.1)\n MultiAssayExperiment     * 1.34.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n pander                   * 0.6.6     2025-03-01 [1] CRAN (R 4.5.0)\n phyloseq                 * 1.52.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n purrr                    * 1.1.0     2025-07-10 [1] CRAN (R 4.5.1)\n R.methodsS3              * 1.8.2     2022-06-13 [1] CRAN (R 4.5.0)\n R.oo                     * 1.27.1    2025-05-02 [1] CRAN (R 4.5.0)\n R.utils                  * 2.13.0    2025-02-24 [1] CRAN (R 4.5.0)\n reactable                * 0.4.4     2023-03-12 [1] CRAN (R 4.5.0)\n reactablefmtr            * 2.0.0     2022-03-16 [1] CRAN (R 4.5.0)\n readr                    * 2.1.5     2024-01-10 [1] CRAN (R 4.5.0)\n S4Vectors                * 0.46.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n seqinr                   * 4.2-36    2023-12-08 [1] CRAN (R 4.5.0)\n sessioninfo              * 1.2.3     2025-02-05 [1] CRAN (R 4.5.0)\n SingleCellExperiment     * 1.30.1    2025-05-05 [1] Bioconductor 3.21 (R 4.5.0)\n stringr                  * 1.5.2     2025-09-08 [1] CRAN (R 4.5.1)\n SummarizedExperiment     * 1.38.1    2025-04-28 [1] Bioconductor 3.21 (R 4.5.0)\n tibble                   * 3.3.0     2025-06-08 [1] CRAN (R 4.5.0)\n tidyr                    * 1.3.1     2024-01-24 [1] CRAN (R 4.5.0)\n tidyverse                * 2.0.0     2023-02-22 [1] CRAN (R 4.5.0)\n tinytable                * 0.13.0.10 2025-09-07 [1] https://vincentarelbundock.r-universe.dev (R 4.5.1)\n TreeSummarizedExperiment * 2.16.1    2025-05-08 [1] Bioconductor 3.21 (R 4.5.0)\n XVector                  * 0.48.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n zip                      * 2.3.3     2025-05-13 [1] CRAN (R 4.5.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.5-x86_64/Resources/library\n * ── Packages attached to the search path.\n```\n\n\n:::\n:::\n\n\n{{< dstop >}}\n\n#### Last updated on {.appendix}\n\n2025-10-02\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}