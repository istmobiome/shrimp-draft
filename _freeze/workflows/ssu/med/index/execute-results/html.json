{
  "hash": "cfd64f6511e6060fd5e2c2f7194a6af2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"5. MED Workflow\"\ndescription: |\n  Workflow for running MED analysis. Workflow begins with redundant, aligned fasta file from mothur and ends with the MED analysis. A Microtable Object is produced to collate the data for downstream analysis.\nlisting: \n    id: med-listing\n    contents: data-med.yml\n    type: table\n    sort-ui: false\n    filter-ui: false\n    fields: \n      - filename\n      - description\n    field-links: \n      - filename\n    field-display-names: \n      filename: File Name\n      description: Description\n---\n\n\n::: {.cell}\n\n:::\n\n\n\n\n::: {.cell}\n\n:::\n\n\n## Workflow Input\n\n::: {.callout-note icon=false}\n## Data & Scripts\n\nFastq sequence files, scripts, and other assets for running this workflow can be found on the [MED Data Portal](/workflows/portal/data-med.qmd) page. \n\nThe Data Portal page also contains a link to the curated output of this pipelineâ€“-feature-sample count table, taxonomy table, sample metadata, & representative fasta sequences. Data is available as stand-alone text files, OR bundled in microtable and phyloseq objects. Archive also includes a table tracking sample read changes.\n:::\n\n#### Required Packages & Software\n\nTo run this workflow you need to  [install mothur](https://mothur.org/wiki/installation/) and the [Oligotyping/MED](https://merenlab.org/2014/08/16/installing-the-oligotyping-pipeline/). You will need the [tidyverse](https://www.tidyverse.org) package. \n\n## Overview\n\nWith the mothur pipeline finished, we can turn our attention to [Minimum Entropy Decomposition (MED)](https://merenlab.org/2014/11/04/med/) [@eren2015minimum]. MED is a novel, information theory-based clustering algorithm for sensitive partitioning of high-throughput marker gene sequences. \n\n(From the Meren Lab website) MED:\n\n- Does not perform pairwise sequence comparison,  \n- Does not rely on arbitrary sequence similarity thresholds,  \n- Does not require user supervision,  \n- Does not require preliminary classification or clustering results,  \n- Is agnostic to sampling strategy or how long your sequences are,  \n- Gives you 1 nucleotide resolution over any sequencing length with computational efficiency and minimal computational heuristics.\n\nMED needs a redundant alignment file of read data. This means **all** identical sequences need to be included.  We again use mothur but this pipeline starts with the currated output of the [`align.seqs`](/workflows/ssu/otu/#aligning-reads) portion of our mothur OTU pipeline. \n\nWe set up our run in the same way as the mothur pipeline. \n\n```{mermaid}\n%%| eval: true\n%%| echo: false\n%%| fig-align: center\nflowchart LR\n  A(Start with curated</br>mothur alignment)\n  B(\"<b><a href='https://mothur.org/wiki/get.groups/'>get.groups</a></b><br/>(NC Samples)\")\n  A --> C(Remove</br>NC Samples<br/>files needed by mothur </br>were generated in R)\n  B --> C\n  C --> D(<b><a href='https://mothur.org/wiki/remove.seqs/'>remove.seqs</a></b>)\n  C --> E(<b><a href='https://mothur.org/wiki/remove.groups/'>remove.groups</a></b>)\n  E --> H\n  D --> H(<b><a href='https://mothur.org/wiki/chimera.vsearch/'>chimera.vsearch</a></b>)\n  H --> END:::hidden\n```\n\n```{mermaid}\n%%| eval: true\n%%| echo: false\n%%| fig-align: center\nflowchart LR\n  BEGIN:::hidden --> I\n  I(<b><a href='https://mothur.org/wiki/classify.seqs/'>classify.seqs</a></b>)\n  I --> J(<b><a href='https://mothur.org/wiki/remove.lineage/'>remove.lineage</a></b>) \n  J --> K(to <b><a href='https://merenlab.org/2014/11/04/med/'>MED pipeline</a></b>)\n\n  K --> END:::hidden \n```\n\n\n\n# Read Processing\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n\n### Expand to see environment variables\n\n```         \n$ export DATA=01_TRIMMED_DATA/\n$ export TYPE=fastq\n$ export PROC=30\n\n$ export REF_LOC=reference_dbs\n$ export TAXREF_FASTA=gsrdb.fasta\n$ export TAXREF_TAX=gsrdb.tax\n$ export ALIGNREF=silva.v4.fasta\n\n$ export CONTAMINENTS=Chloroplast-Mitochondria-unknown-Eukaryota\n```\n:::\n\n\n::: {.cell}\n\n:::\n\n\n## Getting Started\n\nThe first thing to do is copy the output of the `align.seqs` portion of the mothur workflow to a new working directory.  \n\n\n```{.default}\nset.dir(output=pipelineFiles_med/)\n```\n\n```         \nMothur's directories:\noutputDir=pipelineFiles_med/\n```\n\n\n```{.default}\nsystem(cp pipelineFiles/shrimp.trim.contigs.good.unique.good.filter.unique.fasta pipelineFiles_med/)\n\nsystem(cp pipelineFiles/shrimp.trim.contigs.good.unique.good.filter.count_table pipelineFiles_med/)\n```\n\n## Remove Negative Controls\n\nAs with the OTU workflow, we remove NC samples, but in this case we skip the `pre.cluster` step. Our goal here to remove all NC samples from the dataset, as well as any reads that are predominantly found in NC samples. More on what that means in a minute. Before we do anything we need to identify all reads present in NC samples. \n\nHere is what we are going to do:\n\n1. Subset the NC samples (and associated reads) from the `fasta` and `count.table`. To do this in mothur we need all of the NC sample names collected in an `.accnos` file, which is a text file used in  mothur  that contains a single column of names--these can be sequences, OTU labels, or sample names. This list is used to select or remove specific items from other mothur files. That way we can process subsets of the data without having to retype long lists of names. \n\nTo generate the  `.accnos` file of NC samples we can use the `shrimp.files` file generated at the beginning of the mothur pipeline.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp_accnos <- readr::read_delim(here(work_here, \"nc_screen/shrimp.files\"), \n                                delim = \"\\t\", col_names = FALSE)\ntmp_accnos[, 2:3] <- NULL\ntmp_accnos <- tmp_accnos[grepl(\"Control_\", tmp_accnos$X1), ]\nreadr::write_delim(tmp_accnos, file = here(work_here, \"nc_screen/nc_samples.accnos\"), \n                   col_names = FALSE)\n```\n:::\n\n\n2. Now we have a list of all NC sample names. The mothur command `get.groups` in conjunction with `accnos` file allows us to subset the full `fasta` and `count_table`\n\n\n```{.default}\nget.groups(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.fasta, count=shrimp.trim.contigs.good.unique.good.filter.count_table, accnos=nc_samples.accnos)\n```\n\n```         \nSelected 192842 sequences from your count file.\nSelected 34262 sequences from your fasta file.\n\nOutput File names:\nshrimp.trim.contigs.good.unique.good.filter.pick.count_table\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.fasta\n```\n\n3. Next we rename the new files to something more informative (and shorter).\n\n\n```{.default}\nrename.file(input=shrimp.trim.contigs.good.unique.good.filter.unique.pick.fasta, new=nc.fasta)\nrename.file(input=shrimp.trim.contigs.good.unique.good.filter.pick.count_table, new=nc.count_table)\n```\n\n4. And a quick summary of the NC subset. \n\n\n```{.default}\nsummary.seqs(fasta=nc.fasta, count=nc.count_table, processors=$PROC)\n```\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n\n### Expand to see negative control summary\n\n|             | Start | End | NBases | Ambigs | Polymer | NumSeqs |\n|:------------|:-----:|:---:|:------:|:------:|:-------:|:-------:|\n| Minimum     |   1   | 554 |  248   |   0    |    3    |    1    |\n| 2.5%-tile:  |   1   | 554 |  252   |   0    |    4    |  4822   |\n| 25%-tile:   |   1   | 554 |  253   |   0    |    4    |  48211  |\n| Median:     |   1   | 554 |  253   |   0    |    5    |  96422  |\n| 75%-tile:   |   1   | 554 |  253   |   0    |    5    | 144632  |\n| 97.5%-tile: |   1   | 554 |  254   |   0    |    6    | 188021  |\n| Maximum:    |   1   | 554 |  254   |   0    |    6    | 192842  |\n| Mean:       |   1   | 554 |  253   |   0    |    4    |         |\n\n```         \n# of unique seqs:   34262\ntotal # of seqs:    192842\n\nIt took 0 secs to summarize 192842 sequences.\n\nOutput File Names:\nnc.summary\n```\n:::\n\n5. Sweet. We use the command `list.seqs` to get a complete list of all repseq names in the NC subset.\n\n\n```{.default}\nlist.seqs(count=nc.count_table)\n```\n\n```         \nOutput File Names: \nnc.accnos\n```\n\nThis gives us all repseq IDs in the NC samples. \n\n6.  We could simply use the `nc.accnos` file from the `list.seqs` command to remove repseqs found in negative control (NC) samples from the main dataset. This seems reasonable enough. Except mothur will remove **all** repseqs found in a NC sample, in other words, any repseq in the accnos file. For example, let's consider the following scenario where we have two repseqs:\n\n`repseq01` is abundant in many NC samples but not found in any other samples.   \n`repseq02` on the other hand is represented by say one read in a single NC sample but very abundant in other samples.   \n\nIt makes sense to remove `repseq01` but not necessarily `repseq02`. Essentially, for each `repseq` in the `nc.accnos` file we want to calculate:\n\n-   The total number of reads in NC samples.\\\n-   The total number of reads in non-NC samples.\\\n-   The percent of reads in the NC samples.\n-   The total number of NC samples containing at least 1 read.\\\n-   The total number of non-NC samples containing at least 1 read.\\\n-   The percent of NC samples containing reads.\n\nWhere a final data table might look something like this\n\n| repseq     | rc_nc | rc_samps | %in_nc  | nc_samp | no_nc_samp | %_in_nc_samp   |\n|:-----------|:-----:|:--------:|:-------:|:-------:|:----------:|:--------------:|\n| repseq001  |   3   | 5        |  37.5   |   1     |    2       |    33.31       |\n| repseq002  |   196 | 308      |  38.9   |   17    |    38      |    30.7        |\n| repseq003  |   3   | 23       |  11.1   |   3     |    18      |    14.5        |\n\nTo accomplish this we will parse out relevant data from the `.count_table` files. We got the idea on how best to do this from a [discussion on the mothur forum](https://forum.mothur.org/t/negative-control/2754).\n\nTo save space and minimize file size, mothur formats the [`.count_table`](https://mothur.org/wiki/count_file/) using a sparse format by storing only non zero sample counts. However, we need the full format which lists each repseq and its abundance counts for each sample. Using the command `count.seqs` in conjunction with the `.count_table` will return a full format table. \n\n\n```{.default}\ncount.seqs(count=nc.count_table, compress=f)\n```\n\n```\nOutput File Names:\nnc.full.count_table\n```\n\nThen we use the accnos file (`nc.accnos`)--containing repseqs found in NC samples, to generate a subset of the complete dataset containing only the reseqs found in NC samples. \n\n\n```{.default}\nget.seqs(accnos=nc.accnos, fasta=shrimp.trim.contigs.good.unique.good.filter.unique.fasta, count=shrimp.trim.contigs.good.unique.good.filter.count_table)\n```\n\n\n```{.default}\nrename.file(input=shrimp.trim.contigs.good.unique.good.filter.unique.pick.fasta, new=subset.fasta)\nrename.file(input=shrimp.trim.contigs.good.unique.good.filter.pick.count_table, new=subset.count_table)\n```\n\nAnd again run `count.seqs` to get a full format table of the complete dataset, which we subsetted earlier to contain only repseqs found in NC samples. \n\n\n```{.default}\ncount.seqs(count=subset.count_table, compress=f)\n```\n\n```\nOutput File Names:\nsubset.full.count_table\n```\n\nFinally we can parse out read count data from the  two `subset.full.count_table` files. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nfull_count_tab <- readr::read_delim(here(work_here, \"nc_screen/subset.full.count_table\"), \n                                    delim = \"\\t\", col_names = TRUE)\n# figure out which columns to use\ncontrol_cols     <- grep(\"^Control_\", names(full_count_tab), value = TRUE)\nnoncontrol_cols  <- setdiff(names(full_count_tab)[-(1:2)], control_cols)\n\n# now do the rowwise sums\nread_totals <- full_count_tab %>%\n  rowwise() %>%\n  mutate(\n    total_reads_nc   = sum(c_across(all_of(control_cols)), na.rm = TRUE),\n    total_reads_non_nc = sum(c_across(all_of(noncontrol_cols)), na.rm = TRUE)\n  ) %>%\n  ungroup() %>%\n  select(1, 2, total_reads_nc, total_reads_non_nc)\n\nread_totals <- read_totals %>% dplyr::rename(\"total_reads\" = 2)\n```\n:::\n\n\nAnd here is what the new dataframe looks like. Three columns where the first is the repseq name, the second the total number of reads in NC samples, and the third the total number of reads in the entire dataset (this includes NC samples).\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n-------------------------------------------------------------------------------------------------\n           Representative_Sequence             total_reads   total_reads_nc   total_reads_non_nc \n--------------------------------------------- ------------- ---------------- --------------------\n M06508_12_000000000-CJG44_1_1101_16846_2451      33614          19913              13701        \n\n M06508_12_000000000-CJG44_1_1101_9357_2876        571            342                229         \n\n M06508_18_000000000-CNPPR_1_1101_15534_2103     201974            2                201972       \n\n M06508_12_000000000-CJG44_1_1101_15015_3135      6446            703                5743        \n\n M06508_12_000000000-CJG44_1_1101_15072_3643      26928           2907              24021        \n\n M06508_9_000000000-JTBW3_1_1106_8860_16108      368528            2                368526       \n-------------------------------------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\nIn total there are 34262 repseqs that were potential contaminants.\n\nNow we add in a column that calculates the percent of reads in the NC samples. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp_read_totals <- read_totals %>%\n  dplyr::mutate(perc_reads_in_nc = 100*(\n    total_reads_nc / (total_reads_nc + total_reads_non_nc)),\n                .after = \"total_reads_non_nc\")\ntmp_read_totals$perc_reads_in_nc <- \n  round(tmp_read_totals$perc_reads_in_nc, digits = 6)\n```\n:::\n\n\nAnd then we calculate row sums to get the number of NC and non-NC samples containing these reads. For this we can use the `subset.full.count_table` we read in above.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncontrol_cols     <- grep(\"^Control_\", names(full_count_tab), value = TRUE)\nnoncontrol_cols  <- setdiff(names(full_count_tab)[-(1:2)], control_cols)\n# rowwise tally of non-zero columns\nsamp_totals <- full_count_tab %>%\n  rowwise() %>%\n  mutate(\n    num_nc_samp     = sum(c_across(all_of(control_cols)) != 0, na.rm = TRUE),\n    num_non_nc_samp = sum(c_across(all_of(noncontrol_cols)) != 0, na.rm = TRUE)\n  ) %>%\n  ungroup() %>%\n  select(1, num_nc_samp, num_non_nc_samp)\n```\n:::\n\n\nFinally add a column with the total number of samples and calculate the percent of NC samples containing these reads. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamp_totals$total_samp <- samp_totals$num_nc_samp + samp_totals$num_non_nc_samp\nsamp_totals <- samp_totals %>%  \n  dplyr::relocate(\"total_samp\", .after = \"Representative_Sequence\")\nsamp_totals <- samp_totals %>%\n  dplyr::mutate(perc_nc_samp = \n                  100*( num_nc_samp / (num_nc_samp + num_non_nc_samp)),\n                  .after = \"num_non_nc_samp\")\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\nAfter all of this wrangling here is a snippet of the table showing the distribution of NC reads across samples. \n\n\n::: {.cell}\n\n:::\n\n\nNow we remove any repseqs where:\n\n-   The number of reads found in NC samples accounted for more than 10% of total reads OR\n-   The percent of NC samples containing the repseq was greater than 10% of total samples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnc_remove <- nc_check %>% \n  filter(perc_reads_in_nc > 10 | perc_nc_samp > 10)\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n|          | Total rep seqs      | NC reads         | non NC reads      | \\% NC reads       |\n|----------|---------------------|------------------|-------------------|-------------------|\n| Removed  | 32438 | 166614 | 212292 | 43.972 |\n| Retained | 1824 | 26228 | 10280853 | 0.254 |\n\nWe identified a total of **34262** representative sequences (`repseqs`) that were present in at least 1 NC sample by at least 1 read. We removed any repseq where more than 10% of total reads were found in NC samples OR any repseq found in more than 10% of NC samples. Based on these criteria we removed **32438 **repseqs from the data set, which accounted for **166614** total reads in NC samples and **212292** total reads in non-NC samples. Of the total reads removed **43.972%** came from NC samples. Of all repseqs identified in NC samples, **1824** were retained because they fell below the threshold criteria. These repseqs accounted for **26228** reads in NC samples and **10280853** reads in non-NC samples. NC samples accounted for **0.254%** of these reads.\n\nOK, now we can create a new `neg_control.accnos` containing only repseqs abundant in NC samples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_delim(\n  data.frame(nc_remove$Representative_Sequence), \n  here(work_here, \"nc_screen/nc_repseq_remove.accnos\"), \n  col_names = FALSE)\n```\n:::\n\n\nAnd then use this file in conjunction with the mothur command `remove.seqs`. \n\n\n```{.default}\nremove.seqs(accnos=nc_repseq_remove.accnos, fasta=shrimp.trim.contigs.good.unique.good.filter.unique.fasta, count=shrimp.trim.contigs.good.unique.good.filter.count_table)\n```\n\n```         \nRemoved 32438 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.fasta.\nRemoved 378906 sequences from shrimp.trim.contigs.good.unique.good.filter.count_table.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.fasta\nshrimp.trim.contigs.good.unique.good.filter.pick.count_table\n```\n\n\n```{.default}\ncount.groups(count=shrimp.trim.contigs.good.unique.good.filter.pick.count_table)\n```\n\n```         \nSize of smallest group: 1.\n\nTotal seqs: 35944156.\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.good.filter.pick.count.summary\n```\n\nBefore we remove the NC samples we need to check whether some NC samples were already removed. When mothur runs the `remove.seqs` command it will automatically remove any samples where the read count has fallen to zero. If mothur did remove samples and we try to remove all NC samples, we will get an error. To check we can compare the `count.summary` files before and after the previous `remove.seqs` command.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp_before <- read_tsv(\n  here(work_here, \"nc_screen/shrimp.trim.contigs.good.unique.good.filter.count.summary\"),\n  col_names = FALSE,\n  col_select = 1\n)\n\ntmp_after <- read_tsv(\n  here(work_here, \"nc_screen/shrimp.trim.contigs.good.unique.good.filter.pick.count.summary\"),\n  col_names = FALSE,\n  col_select = 1\n)\ntmp_nc_lost <- anti_join(tmp_before, tmp_after)\ntmp_nc_lost$X1\n```\n:::\n\n\nThese are the samples that were removed when we ran `remove.seqs`. We need to eliminate these sample IDs from our list of NC samples to remove or mothur will throw an error. \n\n```         \n[1] \"Control_15\" \"Control_18\" \"Control_21\" \"Control_5\"  \n```\n\nAs before, we can generate a list of NC samples to use in conjunction with the  `remove.groups` command to eliminate all NC samples. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nnc_to_remove <- semi_join(tmp_before, tmp_after)\nnc_to_remove <- nc_to_remove %>%\n  dplyr::filter(\n    stringr::str_starts(X1, \"Control\")\n    )\nreadr::write_delim(nc_to_remove, \n                   file = here(work_here, \"nc_screen/nc_samples_remove.accnos\"), \n                   col_names = FALSE)\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\nIn total the following mothur command should remove 56 negative control samples.\n\n\n```{.default}\nremove.groups(count=shrimp.trim.contigs.good.unique.good.filter.pick.count_table, fasta=shrimp.trim.contigs.good.unique.good.filter.unique.pick.fasta, accnos=nc_samples_remove.accnos)\n```\n\n```         \nRemoved 26228 sequences from your count file.\nRemoved 0 sequences from your fasta file.\n\nOutput File names: \nshrimp.trim.contigs.good.unique.good.filter.pick.pick.count_table\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.fasta\n```\n\n\n```{.default}\nsummary.seqs(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.fasta, count=shrimp.trim.contigs.good.unique.good.filter.pick.pick.count_table, processors=30)\n```\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n\n### Expand to see data set summary\n\n|             | Start | End | NBases | Ambigs | Polymer | NumSeqs  |\n|:------------|:-----:|:---:|:------:|:------:|:-------:|:--------:|\n| Minimum     |   1   | 554 |  208   |   0    |    3    |    1     |\n| 2.5%-tile:  |   1   | 554 |  252   |   0    |    3    |  897949  |\n| 25%-tile:   |   1   | 554 |  253   |   0    |    4    | 8979483  |\n| Median:     |   1   | 554 |  253   |   0    |    4    | 17958965 |\n| 75%-tile:   |   1   | 554 |  253   |   0    |    5    | 26938447 |\n| 97.5%-tile: |   1   | 554 |  253   |   0    |    6    | 35019980 |\n| Maximum:    |   1   | 554 |  254   |   0    |    6    | 35917928 |\n| Mean:       |   1   | 554 |  252   |   0    |    4    |          |\n\n```         \n# of unique seqs:   4146230\ntotal # of seqs:    35917928\n\nIt took 74 secs to summarize 35917928 sequences.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.summary\n```\n\n:::\n\n\n```{.default}\ncount.groups(count=shrimp.trim.contigs.good.unique.good.filter.pick.pick.count_table)\n```\n\n```         \nSize of smallest group: 49.\n\nTotal seqs: 35917928.\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.good.filter.pick.pick.count.summary\n```\n\n## Remove Chimeras\n\n\n```{.default}\nchimera.vsearch(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.fasta, count=shrimp.trim.contigs.good.unique.good.filter.pick.pick.count_table, dereplicate=t, processors=30)\n```\n\n```         \nUsing vsearch version v2.30.0.\nChecking sequences from shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.fasta ...\n...\n\n/******************************************/\nSplitting by sample: \n\n...\n\nRemoving chimeras from your input files:\n/******************************************/\nRunning command: remove.seqs(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.fasta,\naccnos=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.accnos)\nRemoved 710630 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.fasta.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.pick.fasta\n\n/******************************************/\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.count_table\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.chimeras\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.accnos\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.fasta\n\n/******************************************/\n```\n\n\n```{.default}\nsummary.seqs(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.fasta, count=shrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.count_table, processors=30)\n```\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n\n### Expand to see data set summary\n\n|             | Start | End | NBases | Ambigs | Polymer | NumSeqs  |\n|:------------|:-----:|:---:|:------:|:------:|:-------:|:--------:|\n| Minimum     |   1   | 554 |  208   |   0    |    3    |    1     |\n| 2.5%-tile:  |   1   | 554 |  252   |   0    |    3    |  865884  |\n| 25%-tile:   |   1   | 554 |  253   |   0    |    4    |  8658836 |\n| Median:     |   1   | 554 |  253   |   0    |    4    | 17317672 |\n| 75%-tile:   |   1   | 554 |  253   |   0    |    5    | 25976508 |\n| 97.5%-tile: |   1   | 554 |  253   |   0    |    6    | 33769460 |\n| Maximum:    |   1   | 554 |  254   |   0    |    6    | 34635343 |\n| Mean:       |   1   | 554 |  252   |   0    |    4    |          |\n\n```         \n# of unique seqs:   3435600\ntotal # of seqs:    34635343\n\nIt took 64 secs to summarize 34635343 sequences.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.summary\n\n/******************************************/\n```\n:::\n\n\n```{.default}\ncount.groups(count=shrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.count_table)\n```\n\n```         \nSize of smallest group: 49.\n\nTotal seqs: 34635343.\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.count.summary\n\n/******************************************/\n```\n\n## Repseq Taxonomy\n\nThe `classify.seqs` command requires properly formatted reference and taxonomy databases. For taxonomic assignment, we are using the GSR database [@molano2024gsr]. The developers of mothur maintain [formatted versions of popular databases](https://mothur.org/wiki/taxonomy_outline/), however the GSR-DB has not been formatted by the developers yet.\n\n::: callout-note\nYou can visit the [GSR database download](https://manichanh.vhir.org/gsrdb/download_db_links2.php) page to find a database suitable to your data.\n:::\n\nTo create a mothur formatted version GSR-DB[^_merge_runs-1], we perform the following steps.\n\n[^_merge_runs-1]: From the developers: GSR database (Greengenes, SILVA, and RDP database) is an integrated and manually curated database for bacterial and archaeal 16S amplicon taxonomy analysis. Unlike previous integration approaches, this database creation pipeline includes a taxonomy unification step to ensure consistency in taxonomical annotations. The database was validated with three mock communities and two real datasets and compared with existing 16S databases such as Greengenes, GTDB, ITGDB, SILVA, RDP, and MetaSquare. Results showed that the GSR database enhances taxonomical annotations of 16S sequences, outperforming current 16S databases at the species level. The GSR database is available for full-length 16S sequences and the most commonly used hypervariable regions: V4, V1-V3, V3-V4, and V3-V5.\n\n#### Download a data base\n\nHere we are using the [GSR V4 database](https://manichanh.vhir.org/gsrdb/GSR-DB_V4_cluster-1.tar.gz). \n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nwget https://manichanh.vhir.org/gsrdb/GSR-DB_V4_cluster-1.tar.gz\ntar -xvzf GSR-DB_V4_cluster-1.tar.gz\n```\n:::\n\n\nFirst (in the command line) we remove first line of the taxonomy file. \n\n\n::: {.cell}\n\n```{.zsh .cell-code}\ncp GSR-DB_V4_cluster-1_taxa.txt tmp0.txt\nsed '1d' tmp0.txt > tmp1.txt\n```\n:::\n\n\nNext, delete species and remove leading \\[a-z\\]\\_\\_ from taxa names\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nsed -E 's/s__.*//g' tmp1.txt > tmp2.txt\nsed -E 's/[a-zA-Z]__//g' tmp2.txt > gsrdb.tax\ncp GSR-DB_V4_cluster-1_seqs.fasta gsrdb.fasta\nrm tmp*\n```\n:::\n\n\n\n```{.default}\nclassify.seqs(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.fasta, count=shrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.count_table, reference=reference_dbs/gsrdb.fasta, taxonomy=reference_dbs/gsrdb.tax, processors=30)\n```\n\n```         \nReading template taxonomy...     DONE.\nReading template probabilities...     DONE.\nIt took 4 seconds get probabilities.\nClassifying sequences from \nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.fasta ...\n[WARNING]: M06508_12_000000000-CJG44_1_2103_6654_25682 could not be classified. \nYou can use the remove.lineage command with taxon=unknown; to remove such sequences.\n\n...\n\nIt took 839 secs to classify 3435600 sequences.\n\nIt took 1697 secs to create the summary file for 3435600 sequences.\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.gsrdb.wang.taxonomy\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.gsrdb.wang.tax.summary\n\n/******************************************/\n```\n\n## Remove Contaminants\n\n\n```{.default}\nremove.lineage(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.fasta, count=shrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.count_table, taxonomy=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.gsrdb.wang.taxonomy, taxon=Chloroplast-Mitochondria-unknown-Eukaryota)\n```\n\n```         \nRunning command: \nremove.seqs(accnos=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.gsrdb.wang.accnos, count=shrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.count_table, fasta=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.fasta)\nRemoved 2160 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.fasta.\nRemoved 7262 sequences from shrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.count_table.\n\n/******************************************/\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.gsrdb.wang.pick.taxonomy\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.gsrdb.wang.accnos\nshrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.pick.count_table\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.pick.fasta\n\n/******************************************/\n```\n\n\n```{.default}\nsummary.seqs(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.pick.fasta, count=shrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.pick.count_table, processors=30)\n```\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n\n### Expand to see data set summary\n\n|             | Start | End | NBases | Ambigs | Polymer | NumSeqs  |\n|:------------|:-----:|:---:|:------:|:------:|:-------:|:--------:|\n| Minimum     |   1   | 554 |  208   |   0    |    3    |    1     |\n| 2.5%-tile:  |   1   | 554 |  252   |   0    |    3    |  865703  |\n| 25%-tile:   |   1   | 554 |  253   |   0    |    4    | 8657021  |\n| Median:     |   1   | 554 |  253   |   0    |    4    | 17314041 |\n| 75%-tile:   |   1   | 554 |  253   |   0    |    5    | 25971061 |\n| 97.5%-tile: |   1   | 554 |  253   |   0    |    6    | 33762379 |\n| Maximum:    |   1   | 554 |  254   |   0    |    6    | 34628081 |\n| Mean:       |   1   | 554 |  252   |   0    |    4    |          |\n\n```         \n# of unique seqs:\t3433440\ntotal # of seqs:\t34628081\n\nIt took 62 secs to summarize 34628081 sequences.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.pick.summary\n\n/******************************************/\n```\n:::\n\n\n```{.default}\nsummary.tax(taxonomy=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.gsrdb.wang.pick.taxonomy, count=shrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.pick.count_table)\n```\n\n```         \nUsing shrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.pick.count_table as input file for the count parameter.\nUsing shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.gsrdb.wang.pick.taxonomy as input file for the taxonomy parameter.\nIt took 1580 secs to create the summary file for 34628081 sequences.\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.gsrdb.wang.pick.tax.summary\n\n/******************************************/\n```\n\n\n```{.default}\ncount.groups(count=shrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.pick.count_table)\n```\n\n```         \nSize of smallest group: 49.\n\nTotal seqs: 34628081.\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.pick.count.summary\n\n/******************************************/\n```\n\n## Track Reads through Workflow\n\nAt this point we can look at the number of reads that made it through each step of the workflow for every sample.\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nread_change <- read_tsv(\n  here(work_here, \"mothur_med_pipeline_read_changes.txt\"),\n  col_names = TRUE\n)\n```\n:::\n\n\n## Preparing for analysis\n\n\n```{.default}\nrename.file(fasta=current, count=current, taxonomy=current, prefix=final_med)\n```\n\n\n<!------------------------------------------------------------------------>\n<!-------------------- Use this area to save things ---------------------->\n<!------------------------------------------------------------------------>\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n# MED Analysis\n\n\n::: {.cell}\n\n:::\n\n\n::: {style=\"text-align: center;\"}\n```{mermaid}\n%%| eval: true\n%%| echo: false\nflowchart LR\n    style Z fill:transparent\n    style Z font-size:1.1em\n    Z(<a href='https://github.com/michberr/MicrobeMiseq'>mothur2oligo</a></br> script)\n\n    I(<b><a href='https://mothur.org/wiki/get.lineage/'>get.lineage</a></b>) --> J(<b><a href='https://mothur.org/wiki/list.seqs/'>list.seqs</a></b>)\n    J --> K(<b><a href='https://mothur.org/wiki/get.seqs/'>get.seqs</a></b>)\n    K --> L(<b><a href='https://mothur.org/wiki/deunique.seqs/'>deunique.seqs</a></b>)\n    L --> END:::hidden\n\n    BEGIN:::hidden --> M\n    M(renamer.pl<br/>header rename)\n    M --> N(o-trim-uninformative-columns-from-alignment)\n    N --> O(decompose)    \n```\n\n:::\n\nNow that we have the necessary files we are almost ready to run the MED pipeline. First though we need to properly format the mothur files. For this we will use a custom script called [`mothur2oligo`](https://github.com/michberr/MicrobeMiseq) written by [Michelle Berry](https://github.com/michberr).\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nbash mothur2oligo.sh\n```\n:::\n\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### Expand to see mothur2oligo workflow\n\nThese steps are run automatically by the `mothur2oligo` script.\n\n\n```{.default}\nget.lineage(taxonomy=final_med.taxonomy, taxon='Bacteria;-Archaea;', count=final_med.count_table)\n```\n\n```         \n/******************************************/\nRunning command: get.seqs(accnos=final_med.pick.taxonomy.accnos, count=final_med.count_table)\nSelected 34628081 sequences from final_med.count_table.\n\nOutput File Names:\nfinal_med.pick.count_table\n\n/******************************************/\n\nOutput File Names:\nfinal_med.pick.taxonomy\nfinal_med.pick.taxonomy.accnos\nfinal_med.pick.count_table\n```\n\n\n```{.default}\nlist.seqs(count=current)\n```\n\n```         \nUsing final_med.pick.count_table as input file for the count parameter.\n\nOutput File Names: \nfinal_med.pick.accnos\n```\n\n\n```{.default}\nget.seqs(accnos=current, fasta=final_med.fasta)\n```\n\n```         \nUsing pipelineFiles_med/final_med.pick.accnos as input file for the accnos parameter.\nSelected 3433440 sequences from pipelineFiles_med/final_med.fasta.\n\nOutput File Names:\nfinal_med.pick.fasta\n```\n\n\n```{.default}\ndeunique.seqs(fasta=current, count=current)\n```\n\n```         \nUsing final_med.pick.count_table as input file for the count parameter.\nUsing final_med.pick.fasta as input file for the fasta parameter.\n\nOutput File Names: \nfinal_med.pick.redundant.fasta\nfinal_med.pick.redundant.groups\n```\n\nThe eventual output of the `mothur2oligo` workflow is a fasta file with reformatted headers. The script `renamer.pl` adds sample names to fasta headers and appends the extension `_headers-replaced.fasta` to a new file with these reformatted data. \n\n```\nfinal_med.pick.redundant.fasta_headers-replaced.fasta\n```\n\n:::\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\no-trim-uninformative-columns-from-alignment \\\n        final_med.pick.redundant.renamed.fasta\ndecompose final_med.pick.redundant.renamed.fasta-TRIMMED \\\n        --sample-mapping med_mapping.txt \\\n        --output-directory MED \\\n        --number-of-threads 24 \\\n        --skip-gen-figures\n```\n:::\n\n\n\nTo run the `decompose` command you will need this [sample mapping file](/share/ssu/med/med_mapping.txt).\n\n::: {.callout-tip appearance=\"default\"}\n\n## MED Results\n\nOnce this is finished, the MED software will output a [summary of the run](/share/ssu/med/med_results/index.html). \n:::\n\n# Data Set Prep\n\nIn this next part of the workflow our main goal is to create a *microtable object* using the R package [microeco](https://joey711.github.io/phyloseq/) [@liu2021microeco]. The microtable will be used to store the OTU by sample data as well the taxonomic, fasta, and sample data in a single object. More on that in a moment.\n\n## Assign Taxonomy\n\nOur first step is to classify the node representatives from the MED output. The `classify.seqs` command requires properly formatted reference and taxonomy databases. For taxonomic assignment, we are using the GSR database [@molano2024gsr]. The developers of mothur maintain [formatted versions of popular databases](https://mothur.org/wiki/taxonomy_outline/), however the GSR-DB has not been formatted by the developers yet.\n\n::: callout-note\nHere you can download an appropriate version of the [GSR database](https://manichanh.vhir.org/gsrdb/download_db_links2.php).\n:::\n\nTo create a mothur formatted version GSR-DB, we perform the following steps (we went through this process above prior to removing contaminants but will repeat here for posterity). \n\n#### Download a data base\n\nHere we are using the [GSR V4 database](https://manichanh.vhir.org/gsrdb/GSR-DB_V4_cluster-1.tar.gz).\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nwget https://manichanh.vhir.org/gsrdb/GSR-DB_V4_cluster-1.tar.gz\ntar -xvzf GSR-DB_V4_cluster-1.tar.gz\n```\n:::\n\n\nFirst (in the command line) we remove first line of the taxonomy file.\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\ncp GSR-DB_V4_cluster-1_taxa.txt tmp0.txt\nsed '1d' tmp0.txt > tmp1.txt\n```\n:::\n\n\nNext, delete species and remove leading \\[a-z\\]\\_\\_ from taxa names\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nsed -E 's/s__.*//g' tmp1.txt > tmp2.txt\nsed -E 's/[a-zA-Z]__//g' tmp2.txt > gsrdb.tax\ncp GSR-DB_V4_cluster-1_seqs.fasta gsrdb.fasta\n```\n:::\n\n\nThe next thing we need to do is grab the `node-representatives.fa.txt` from the MED output so that we can classify these sequences. Of course, proper formatting is required.\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nseqkit replace --pattern ^ --replacement MED node-representatives.fa.txt > tmp1.fa\nseqkit replace --pattern \"\\|.*\" --replacement '' tmp1.fa > med_nodes.fasta\nrm tmp1.fa\n```\n:::\n\n\nGreat, the reference database is formatted. Now we need to make a few files that mimics the normal mothur output files because the MED pipeline does not exactly generate the files we need to create a `microtable` object. First we use the `matrix_counts.txt` file from the MED analysis to create a mothur-styled `count.table`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp_med_counts <- read_tsv(\n  here(work_here, \"med_results/matrix_counts.txt\"),\n    col_names = TRUE)\n\ntmp_med_counts <- tmp_med_counts %>% \n  dplyr::rename_with( ~ paste0(\"MED\", .x)) \n\ntmp_med_counts <- tmp_med_counts %>%\n  tidyr::pivot_longer(cols = c(-1), names_to = \"tmp\") %>%\n  tidyr::pivot_wider(names_from = c(1))\n\ntmp_med_counts <- tibble::column_to_rownames(tmp_med_counts, \"tmp\")\n\ntmp_med_counts <- tmp_med_counts %>%\n                  mutate(total = rowSums(.), .before = 1)\n\ntmp_med_counts <- tmp_med_counts %>% \n     tibble::rownames_to_column(\"Representative_Sequence\")\nmed_counts <- tmp_med_counts\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\nNow we can actually classify the representative sequences. \n\n\n```{.default}\nclassify.seqs(fasta=med_nodes.fasta, count=med_nodes.count.table, reference=reference_dbs/gsrdb.fasta, taxonomy=reference_dbs/gsrdb.tax)\n```\n\nNow we make a mothur styled `shared` file. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp_med_counts <- read_tsv(\n  here(work_here, \"med_results/matrix_counts.txt\"),\n    col_names = TRUE)\n\ntmp_n_meds <- ncol(tmp_med_counts) - 1\ntmp_med_counts <- tmp_med_counts %>% \n  dplyr::rename_with( ~ paste0(\"MED\", .x)) %>% \n  dplyr::rename(\"Group\" = \"MEDsamples\")\n\ntmp_med_counts <- tmp_med_counts %>% \n  tibble::add_column(label = 0.03, .before = \"Group\") %>% \n  tibble::add_column(numOtus = tmp_n_meds, .after = \"Group\")\n\nmed_shared <- tmp_med_counts\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n### A. Taxonomy Table\n\nHere is what the taxonomy table looks like in the microeco mock data.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n---------------------------------------------------------------------\n  &nbsp;      Kingdom           Phylum                 Class         \n---------- ------------- -------------------- -----------------------\n  OTU_50    k__Bacteria   p__Proteobacteria    c__Betaproteobacteria \n\n OTU_8058   k__Bacteria   p__Actinobacteria      c__Actinobacteria   \n\n OTU_7152   k__Bacteria   p__Verrucomicrobia    c__OPB35 soil group  \n---------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\nOur taxonomy file (below) needs a little wrangling to be properly formatted. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp_tax <- read_delim(\n             here(work_here, \"node_taxonomy/med_nodes.cons.taxonomy\"),\n             delim = \"\\t\")\n```\n:::\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n     OTU         Size                                                                                Taxonomy                                                                              \n-------------- -------- -------------------------------------------------------------------------------------------------------------------------------------------------------------------\n MED000013674   347929                     Bacteria(100);Proteobacteria(100);Gammaproteobacteria(100);Vibrionales(100);Vibrionaceae(100);Vibrionaceae_unclassified(100);                   \n\n MED000009391   314434         Bacteria(100);Bacteria_unclassified(100);Bacteria_unclassified(100);Bacteria_unclassified(100);Bacteria_unclassified(100);Bacteria_unclassified(100);       \n\n MED000010996   305834   Bacteria(100);Proteobacteria(90);Proteobacteria_unclassified(90);Proteobacteria_unclassified(90);Proteobacteria_unclassified(90);Proteobacteria_unclassified(90); \n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\nSome fancy string manipulation...\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp_tax <- data.frame(sapply(tmp_tax, \n                             gsub, \n                             pattern = \"\\\\(\\\\d+\\\\)\", \n                             replacement = \"\"))\ntmp_tax <- data.frame(sapply(tmp_tax, \n                             gsub, \n                             pattern = \";$\", \n                             replacement = \"\"))\ntmp_tax <- separate_wider_delim(tmp_tax, \n                              cols = Taxonomy, \n                              delim = \";\", names = c(\n                                \"Kingdom\", \"Phylum\", \n                                \"Class\", \"Order\", \n                                \"Family\", \"Genus\"))\ntmp_tax <- data.frame(sapply(tmp_tax, gsub, \n                           pattern = \"^.*_unclassified$\", \n                           replacement = \"\"))\ntmp_tax$Size <- NULL\ntmp_tax <- tibble::column_to_rownames(tmp_tax, \"OTU\")\n```\n:::\n\n\nAnd we get this ...\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n------------------------------------------------------------------------------\n    &nbsp;      Kingdom        Phylum              Class             Order    \n-------------- ---------- ---------------- --------------------- -------------\n MED000013674   Bacteria   Proteobacteria   Gammaproteobacteria   Vibrionales \n\n MED000009391   Bacteria                                                      \n\n MED000010996   Bacteria   Proteobacteria                                     \n\n MED000011677   Bacteria   Proteobacteria   Gammaproteobacteria               \n------------------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrank_prefixes <- c(\n  Kingdom = \"k\", \n  Phylum  = \"p\", \n  Class   = \"c\", \n  Order   = \"o\", \n  Family  = \"f\", \n  Genus   = \"g\"\n)\n\ntmp_tax <- tmp_tax %>%\n  mutate(across(everything(), ~replace_na(., \"\"))) %>% \n  mutate(across(names(rank_prefixes), \n                ~ paste0(rank_prefixes[cur_column()], \"__\", .))) %>%\ntidy_taxonomy()\n```\n:::\n\n\nAnd then this. Exactly like the mock data. \n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n------------------------------------------------------------------------------------------\n    &nbsp;        Kingdom          Phylum                 Class                Order      \n-------------- ------------- ------------------- ------------------------ ----------------\n MED000013674   k__Bacteria   p__Proteobacteria   c__Gammaproteobacteria   o__Vibrionales \n\n MED000009391   k__Bacteria          p__                   c__                  o__       \n\n MED000010996   k__Bacteria   p__Proteobacteria            c__                  o__       \n\n MED000011677   k__Bacteria   p__Proteobacteria   c__Gammaproteobacteria        o__       \n------------------------------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\n### B. Sequence Table\n\nHere is what the sequence table looks like in the mock data.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n--------------------------------------------------------------------\n  &nbsp;    S1   S2   S3   S4   S5   S6   S7   S9   S10   S11   S12 \n---------- ---- ---- ---- ---- ---- ---- ---- ---- ----- ----- -----\n OTU_4272   1    0    1    1    0    0    1    1     0     1     1  \n\n OTU_236    1    4    0    2    35   5    94   0    177   14    27  \n\n OTU_399    9    2    2    4    4    0    3    6     0     1     2  \n--------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\nThese code block will return a properly formatted sequence table. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp_st <- readr::read_delim(\n  here(work_here, \"node_taxonomy/med_nodes.shared\"),\n  delim = \"\\t\")\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp_st$numOtus <- NULL\ntmp_st$label <- NULL\ntmp_st <- tmp_st %>%\n  tidyr::pivot_longer(cols = c(-1), names_to = \"tmp\") %>%\n  tidyr::pivot_wider(names_from = c(1))\n\ntmp_st <- tibble::column_to_rownames(tmp_st, \"tmp\")\n```\n:::\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n--------------------------------------------------------------------------\n    &nbsp;      EP_A_AREN_EG_8651   EP_A_AREN_EG_8654   EP_A_AREN_EG_8698 \n-------------- ------------------- ------------------- -------------------\n MED000011539           0                   4                   1         \n\n MED000013720           7                  410                 25         \n\n MED000009147           0                   0                   0         \n\n MED000009218           0                   0                   0         \n--------------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\n### C. Sample Table\n\nHere is what the sample table looks like in the mock data.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n----------------------------------------------------\n &nbsp;   SampleID   Group   Type       Saline      \n-------- ---------- ------- ------ -----------------\n   S1        S1       IW      NE    Non-saline soil \n\n   S2        S2       IW      NE    Non-saline soil \n\n   S3        S3       IW      NE    Non-saline soil \n----------------------------------------------------\n```\n\n\n:::\n:::\n\n\nNo problem. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamdf <- read.table(\n  here(\"working_files/ssu/sampledata\", \"sample_data.txt\"),\n  header = TRUE, sep = \"\\t\")\n\nsamdf <- samdf %>% tibble::column_to_rownames(\"SampleID\")\nsamdf$SampleID <- rownames(samdf)\nsamdf <- samdf %>% relocate(SampleID)\n\nsamdf <- samdf %>%\n  dplyr::filter(\n    stringr::str_starts(SampleID, \"Control\", negate = TRUE))\n```\n:::\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n-------------------------------------------------------------------------\n      &nbsp;             SampleID        OCEAN   SPECIES   TISSUE    ID  \n------------------- ------------------- ------- --------- -------- ------\n EP_A_AREN_GL_8625   EP_A_AREN_GL_8625    EP     A_AREN      GL     8625 \n\n EP_A_AREN_GL_8626   EP_A_AREN_GL_8626    EP     A_AREN      GL     8626 \n\n EP_A_AREN_GL_8627   EP_A_AREN_GL_8627    EP     A_AREN      GL     8627 \n-------------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\n## Experiment-level Objects\n\nIn the following section we create `microtable` and `phyloseq` objects. These are single, consolidated experiment-level data objects that organize various data types, such as OTU tables (taxa abundance), sample metadata, taxonomic classifications, as well as phylogenetic trees and reference sequences, into one structured format. This unified approach facilitates reproducible, interactive analysis and visualization by bringing all related information together. \n\nWe begin by creating the `microtable` and then use the function `meco2phyloseq` from the [file2meco](https://github.com/ChiLiubio/file2meco) package to create the `phyloseq` object. This way all of the underlying data is identical across the two objects. \n\n::: callout-note\nThese objects contain an OTU table (taxa abundances), sample metadata, and taxonomy table (mapping between OTUs and higher-level taxonomic classifications).\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_info <- samdf\ntax_tab <- tmp_tax\notu_tab <- tmp_st\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp_me <- microtable$new(sample_table = sample_info, \n                         otu_table = otu_tab, \n                         tax_table = tax_tab)\ntmp_me\n```\n:::\n\n\n```         \nmicrotable-class object:\nsample_table have 1849 rows and 14 columns\notu_table have 436 rows and 1849 columns\ntax_table have 436 rows and 6 columns\n```\n\n### Add Representative Sequence\n\nThe fasta file returned by MED needs a little T.L.C. For that we use a tool called SeqKit [@shen2024seqkit2] for fasta defline manipulation.\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nseqkit replace -p ^ -r MED node-representatives.fa.txt > tmp1.fa\nseqkit replace -p \"\\|.*\" -r '' tmp1.fa > tmp2.fa \nseqkit replace -p \"-\" -r '$1' -s -w 0 tmp2.fa > med_rep.fasta\nrm tmp1.fa\nrm tmp2.fa\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrep_fasta <- Biostrings::readDNAStringSet(here(work_here, \"med_rep.fasta\"))\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp_me$rep_fasta <- rep_fasta\ntmp_me$tidy_dataset()\ntmp_me\nme_raw <- microeco::clone(tmp_me)\n```\n:::\n\n\n## Curate the Data Set\n\nPretty much the last thing to do is remove low-count samples.\n\n### Remove Low-Count Samples\n\n\n::: {.cell}\n\n```{.r .cell-code}\nthreshold <- 1000\ntmp_no_low <- microeco::clone(me_raw)\ntmp_no_low$otu_table <- me_raw$otu_table %>%\n          dplyr::select(where(~ is.numeric(.) && sum(.) >= threshold))\ntmp_no_low$tidy_dataset()\ntmp_no_low\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nmicrotable-class object:\nsample_table have 1799 rows and 14 columns\notu_table have 436 rows and 1799 columns\ntax_table have 436 rows and 6 columns\nrep_fasta have 436 sequences\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nme_final <- microeco::clone(tmp_no_low)\n```\n:::\n\n\nLastly, we can use the package [`file2meco`](https://github.com/ChiLiubio/file2meco) to generate a [phyloseq](https://joey711.github.io/phyloseq/) object. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nps_final <- file2meco::meco2phyloseq(me_final)\n```\n:::\n\n\n## Summary\n\nNow time to summarize the data. For this we use the R package [miaverse](https://microbiome.github.io) [@felix2024mia].\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|me_dataset         | total_asvs| total_reads| total_samples|\n|:------------------|----------:|-----------:|-------------:|\n|original           |        436|    24817013|          1849|\n|no low count samps |        436|    24785483|          1799|\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell tbl-cap='Dataset metrics before and after curation.'}\n::: {.cell-output-display}\n\n\n|Metric                |Start    |End      |\n|:---------------------|:--------|:--------|\n|Min. no. of reads     |1        |1010     |\n|Max. no. of reads     |185675   |185675   |\n|Total no. of reads    |24817013 |24785483 |\n|Avg. no. of reads     |13422    |13777    |\n|Median no. of reads   |10360    |10566    |\n|Total ASVs            |436      |436      |\n|No. of singleton ASVs |NA       |NA       |\n|% of singleton ASVs   |NA       |NA       |\n|Sparsity              |0.846    |0.845    |\n\n\n:::\n:::\n\n\nWe started off with 436 ASVs and 1849 samples. After removing low-count samples, there were 436 ASVs and 1799 samples remaining.\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\nWe lost a total of 110 samples after curating the dataset. This includes 60 NC samples and 50 non-NC samples.\n\nHere is a list of non-NC samples that were removed. \n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"EP_A_HEBE_ST_8937\" \"EP_A_UMBO_EG_9126\" \"EP_A_UTRI_MG_9071\"\n [4] \"EP_A_UTRI_ST_9071\" \"EP_E_SAMP_MD_9290\" \"EP_E_SAMP_MD_9293\"\n [7] \"EP_E_SAMP_SD_9285\" \"EP_E_SAMP_SD_9286\" \"EP_E_SAMP_SD_sed\" \n[10] \"WA_A_BAHA_GL_7474\" \"WA_A_BOUV_GL_7580\" \"WA_A_BOUV_ST_7585\"\n[13] \"WA_A_CRIS_GL_9624\" \"WA_A_CRIS_HP_7536\" \"WA_A_CRIS_ST_7539\"\n[16] \"WA_A_CRIS_ST_9471\" \"WA_A_FLOR_EG_7685\" \"WA_A_FLOR_EG_9662\"\n[19] \"WA_A_FLOR_GL_9659\" \"WA_A_FLOR_HP_9659\" \"WA_A_FLOR_ST_9655\"\n[22] \"WA_A_FORM_EG_7457\" \"WA_A_FORM_MG_7752\" \"WA_A_NUTT_EG_9410\"\n[25] \"WA_A_NUTT_GL_9405\" \"WA_A_NUTT_ST_7430\" \"WA_A_NUTT_ST_9405\"\n[28] \"WA_A_PARA_GL_9474\" \"WA_A_PARA_HP_9364\" \"WA_A_PARA_MG_9364\"\n[31] \"WA_A_PARA_ST_7460\" \"WA_A_PARA_ST_7708\" \"WA_A_PARA_ST_9477\"\n[34] \"WA_A_PCNS_HP_9513\" \"WA_A_PCNS_MG_9513\" \"WA_A_PCWS_EG_9438\"\n[37] \"WA_A_PCWS_ST_9438\" \"WA_A_THOM_HP_7392\" \"WA_A_THOM_ST_7392\"\n[40] \"WA_A_THOM_ST_9385\" \"WA_A_THOM_ST_9389\" \"WA_A_VERR_EG_9501\"\n[43] \"WA_A_VERR_MG_9501\" \"WA_A_WEBS_HP_7727\" \"WA_A_WEBS_HP_9411\"\n[46] \"WA_A_WEBS_MG_7612\" \"WA_A_WEBS_ST_9411\" \"WA_E_SAMP_RB_7531\"\n[49] \"WA_E_SAMP_RB_7533\" \"WA_E_SAMP_RB_7535\"\n```\n\n\n:::\n:::\n\n\n# Download Results {#download-results}\n\nQuick access to read changes through the pipeline and repseqs detected in negative control samples.\n\n::: {#med-listing .column-body}\n:::\n\n<!------------------------------------------------------------------------>\n<!-------------------- Use this area to save things ---------------------->\n<!------------------------------------------------------------------------>\n\n<!--------------------------------------->\n<!-- These chunks are for curated data -->\n<!--------------------------------------->\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n<!-------------------------------------------->\n<!-- These chunks are for  processing data  -->\n<!-------------------------------------------->\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n#### References {.appendix}\n\n::: {#refs}\n:::\n\n\n::: {.cell}\n\n:::\n\n\n#### Detailed Session Info {.appendix}\n\n{{< dstart summary=\"Expand to see Session Info\" >}}\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nâ”€ Session info â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n setting  value\n version  R version 4.5.1 (2025-06-13)\n os       macOS Ventura 13.7.8\n system   x86_64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/Los_Angeles\n date     2025-10-03\n pandoc   3.6.3 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/x86_64/ (via rmarkdown)\n quarto   1.8.25 @ /Applications/quarto/bin/quarto\n\nâ”€ Packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n package                  * version   date (UTC) lib source\n Biobase                  * 2.68.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n BiocGenerics             * 0.54.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n Biostrings               * 2.76.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n downloadthis             * 0.5.0     2025-09-26 [1] Github (fmmattioni/downloadthis@18e3e5a)\n dplyr                    * 1.1.4     2023-11-17 [1] CRAN (R 4.5.0)\n fontawesome              * 0.5.3     2024-11-16 [1] CRAN (R 4.5.0)\n forcats                  * 1.0.0     2023-01-29 [1] CRAN (R 4.5.0)\n fs                       * 1.6.6     2025-04-12 [1] CRAN (R 4.5.0)\n generics                 * 0.1.4     2025-05-09 [1] CRAN (R 4.5.0)\n GenomeInfoDb             * 1.44.2    2025-08-18 [1] Bioconductor 3.21 (R 4.5.1)\n GenomicRanges            * 1.60.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n ggplot2                  * 4.0.0     2025-09-11 [1] CRAN (R 4.5.1)\n here                     * 1.0.2     2025-09-15 [1] CRAN (R 4.5.1)\n htmltools                * 0.5.8.1   2024-04-04 [1] CRAN (R 4.5.0)\n IRanges                  * 2.42.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n lubridate                * 1.9.4     2024-12-08 [1] CRAN (R 4.5.0)\n magrittr                 * 2.0.4     2025-09-12 [1] CRAN (R 4.5.1)\n MatrixGenerics           * 1.20.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n matrixStats              * 1.5.0     2025-01-07 [1] CRAN (R 4.5.0)\n mia                      * 1.15.6    2024-11-22 [1] Bioconductor 3.21 (R 4.5.0)\n microbiome               * 1.30.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n microeco                 * 1.15.0    2025-05-18 [1] CRAN (R 4.5.0)\n microViz                 * 0.12.7    2025-08-01 [1] https://david-barnett.r-universe.dev (R 4.5.1)\n MultiAssayExperiment     * 1.34.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n pander                   * 0.6.6     2025-03-01 [1] CRAN (R 4.5.0)\n phyloseq                 * 1.52.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n purrr                    * 1.1.0     2025-07-10 [1] CRAN (R 4.5.1)\n R.methodsS3              * 1.8.2     2022-06-13 [1] CRAN (R 4.5.0)\n R.oo                     * 1.27.1    2025-05-02 [1] CRAN (R 4.5.0)\n R.utils                  * 2.13.0    2025-02-24 [1] CRAN (R 4.5.0)\n reactable                * 0.4.4     2023-03-12 [1] CRAN (R 4.5.0)\n reactablefmtr            * 2.0.0     2022-03-16 [1] CRAN (R 4.5.0)\n readr                    * 2.1.5     2024-01-10 [1] CRAN (R 4.5.0)\n S4Vectors                * 0.46.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n seqinr                   * 4.2-36    2023-12-08 [1] CRAN (R 4.5.0)\n sessioninfo              * 1.2.3     2025-02-05 [1] CRAN (R 4.5.0)\n SingleCellExperiment     * 1.30.1    2025-05-05 [1] Bioconductor 3.21 (R 4.5.0)\n stringr                  * 1.5.2     2025-09-08 [1] CRAN (R 4.5.1)\n SummarizedExperiment     * 1.38.1    2025-04-28 [1] Bioconductor 3.21 (R 4.5.0)\n tibble                   * 3.3.0     2025-06-08 [1] CRAN (R 4.5.0)\n tidyr                    * 1.3.1     2024-01-24 [1] CRAN (R 4.5.0)\n tidyverse                * 2.0.0     2023-02-22 [1] CRAN (R 4.5.0)\n tinytable                * 0.13.0.10 2025-09-07 [1] https://vincentarelbundock.r-universe.dev (R 4.5.1)\n TreeSummarizedExperiment * 2.16.1    2025-05-08 [1] Bioconductor 3.21 (R 4.5.0)\n XVector                  * 0.48.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n zip                      * 2.3.3     2025-05-13 [1] CRAN (R 4.5.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.5-x86_64/Resources/library\n * â”€â”€ Packages attached to the search path.\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n======   Devtools Session info   ===================================\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n package                  * version   date (UTC) lib source\n Biobase                  * 2.68.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n BiocGenerics             * 0.54.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n Biostrings               * 2.76.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n downloadthis             * 0.5.0     2025-09-26 [1] Github (fmmattioni/downloadthis@18e3e5a)\n dplyr                    * 1.1.4     2023-11-17 [1] CRAN (R 4.5.0)\n fontawesome              * 0.5.3     2024-11-16 [1] CRAN (R 4.5.0)\n forcats                  * 1.0.0     2023-01-29 [1] CRAN (R 4.5.0)\n fs                       * 1.6.6     2025-04-12 [1] CRAN (R 4.5.0)\n generics                 * 0.1.4     2025-05-09 [1] CRAN (R 4.5.0)\n GenomeInfoDb             * 1.44.2    2025-08-18 [1] Bioconductor 3.21 (R 4.5.1)\n GenomicRanges            * 1.60.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n ggplot2                  * 4.0.0     2025-09-11 [1] CRAN (R 4.5.1)\n here                     * 1.0.2     2025-09-15 [1] CRAN (R 4.5.1)\n htmltools                * 0.5.8.1   2024-04-04 [1] CRAN (R 4.5.0)\n IRanges                  * 2.42.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n lubridate                * 1.9.4     2024-12-08 [1] CRAN (R 4.5.0)\n magrittr                 * 2.0.4     2025-09-12 [1] CRAN (R 4.5.1)\n MatrixGenerics           * 1.20.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n matrixStats              * 1.5.0     2025-01-07 [1] CRAN (R 4.5.0)\n mia                      * 1.15.6    2024-11-22 [1] Bioconductor 3.21 (R 4.5.0)\n microbiome               * 1.30.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n microeco                 * 1.15.0    2025-05-18 [1] CRAN (R 4.5.0)\n microViz                 * 0.12.7    2025-08-01 [1] https://david-barnett.r-universe.dev (R 4.5.1)\n MultiAssayExperiment     * 1.34.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n pander                   * 0.6.6     2025-03-01 [1] CRAN (R 4.5.0)\n phyloseq                 * 1.52.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n purrr                    * 1.1.0     2025-07-10 [1] CRAN (R 4.5.1)\n R.methodsS3              * 1.8.2     2022-06-13 [1] CRAN (R 4.5.0)\n R.oo                     * 1.27.1    2025-05-02 [1] CRAN (R 4.5.0)\n R.utils                  * 2.13.0    2025-02-24 [1] CRAN (R 4.5.0)\n reactable                * 0.4.4     2023-03-12 [1] CRAN (R 4.5.0)\n reactablefmtr            * 2.0.0     2022-03-16 [1] CRAN (R 4.5.0)\n readr                    * 2.1.5     2024-01-10 [1] CRAN (R 4.5.0)\n S4Vectors                * 0.46.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n seqinr                   * 4.2-36    2023-12-08 [1] CRAN (R 4.5.0)\n sessioninfo              * 1.2.3     2025-02-05 [1] CRAN (R 4.5.0)\n SingleCellExperiment     * 1.30.1    2025-05-05 [1] Bioconductor 3.21 (R 4.5.0)\n stringr                  * 1.5.2     2025-09-08 [1] CRAN (R 4.5.1)\n SummarizedExperiment     * 1.38.1    2025-04-28 [1] Bioconductor 3.21 (R 4.5.0)\n tibble                   * 3.3.0     2025-06-08 [1] CRAN (R 4.5.0)\n tidyr                    * 1.3.1     2024-01-24 [1] CRAN (R 4.5.0)\n tidyverse                * 2.0.0     2023-02-22 [1] CRAN (R 4.5.0)\n tinytable                * 0.13.0.10 2025-09-07 [1] https://vincentarelbundock.r-universe.dev (R 4.5.1)\n TreeSummarizedExperiment * 2.16.1    2025-05-08 [1] Bioconductor 3.21 (R 4.5.0)\n XVector                  * 0.48.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n zip                      * 2.3.3     2025-05-13 [1] CRAN (R 4.5.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.5-x86_64/Resources/library\n * â”€â”€ Packages attached to the search path.\n```\n\n\n:::\n:::\n\n\n{{< dstop >}}\n\n#### Last updated on {.appendix}\n\n2025-10-03\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}