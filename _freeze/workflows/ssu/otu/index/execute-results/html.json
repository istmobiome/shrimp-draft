{
  "hash": "9b97455f18961fc2ffc744b00b964c66",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"4. OTU Workflow\"\ndescription: |\n  Workflow for processing 16S rRNA samples for OTU analysis using mothur. Workflow uses paired end reads, beginning with raw fastq files, ending with sequence and taxonomy tables. A Microtable Object is produced to collate the data for downstream analysis. \nlisting: \n    id: otu-listing\n    contents: data-otu.yml\n    type: table\n    sort-ui: false\n    filter-ui: false\n    fields: \n      - filename\n      - description\n    field-links: \n      - filename\n    field-display-names: \n      filename: File Name\n      description: Description\n---\n\n\n::: {.cell}\n\n:::\n\n\n\n\n::: {.cell}\n\n:::\n\n\n## Workflow Input\n\n::: {.callout-note icon=false}\n## Data & Scripts\n\nFastq sequence files, scripts, and other assets for running this workflow can be found on the [OTU Data Portal](/workflows/portal/data-otu.qmd) page. \n\nThe Data Portal page also contains a link to the curated output of this pipelineâ€“-feature-sample count table, taxonomy table, sample metadata, & representative fasta sequences. Data is available as stand-alone text files, OR bundled in microtable and phyloseq objects. Archive also includes a table tracking sample read changes.\n:::\n\n#### Required Packages & Software\n\nTo run this workflow you need to have [mothur installed](https://mothur.org/wiki/installation/) and you will need the [tidyverse](https://www.tidyverse.org) package. \n\n## Overview\n\nThis is a standard pipeline for generating OTUs using [mothur](https://mothur.org/)[@schloss2009mothur]. We generally followed the mothur [MiSeq SOP](https://mothur.org/wiki/miseq_sop/) when building this pipeline. Since this SOP is heavily annotated we will keep our comments here to a minimum. \n\n:::{.columns}\n:::{.column width=\"50%\"}\n```{mermaid}\n%%| eval: true\n%%| echo: false\nflowchart TB\n  A(<b><a href='https://mothur.org/wiki/make.file/'>make.file</a></b>) --> B(<b><a href='https://mothur.org/wiki/make.contigs/'>make.contigs</a></b>)\n  B --> C(<b><b><a href='https://mothur.org/wiki/screen.seqs/'>screen.seqs</a></b><br/><a href='https://mothur.org/wiki/unique.seqs/'>unique.seqs</a></b>)\n  C --> E(<b><a href='https://mothur.org/wiki/align.seqs/'>align.seqs</a></b>)\n  L(reference db<br/> e.g. silva seed) --> F\n  F(<b><a href='https://mothur.org/wiki/pcr.seqs/'>pcr.seqs</a></b></br>trim ref db</br>to V4 region) --> E\n  E --> I(<b><a href='https://mothur.org/wiki/screen.seqs/'>screen.seqs</a><br/><a href='https://mothur.org/wiki/filter.seqs/'>filter.seqs</a><br/><a href='https://mothur.org/wiki/unique.seqs/'>unique.seqs</a></b>)\n  I --> K[Pass to MED]\n  I --> J(<b><a href='https://mothur.org/wiki/pre.cluster/'>pre.cluster</a></b>)\n  J --> M(remove</br>negative controls)\n```\n\n:::\n:::{.column width=\"50%\"}\n```{mermaid}\n%%| eval: true\n%%| echo: false\nflowchart TB\n  L(\"<b><a href='https://mothur.org/wiki/get.groups/'>get.groups</a></b><br/>(NC Samples)\")\n  L --> M(<b><a href='https://mothur.org/wiki/remove.seqs/'>remove.seqs</a></b>)\n  L --> N(<b><a href='https://mothur.org/wiki/remove.groups/'>remove.groups</a></b>)\n  M --> O(<b><a href='https://mothur.org/wiki/chimera.vsearch/'>chimera.vsearch</a></b>)\n  N --> O\n  O --> P(<b><a href='https://mothur.org/wiki/classify.seqs/'>classify.seqs</a></b>)\n  P --> Q(<b><a href='https://mothur.org/wiki/remove.lineage/'>remove.lineage</a></b>)\n  Q --> R(<b><a href='https://mothur.org/wiki/cluster.split/'>cluster.split</a></b>)\n  Q --> S(<b><a href='https://mothur.org/wiki/dist.seqs/'>dist.seqs</a></b>)\n\ntitle[Remove<br/>Negative<br/>Controls]\nstyle title stroke:#333,stroke-width:0px,mermaid-font-size:2.8rem\n```\n\n:::\n:::\n\n# Read Processing\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n\n### Expand to see environment variables\n\nWe are running mothur in [Batch Mode](https://mothur.org/wiki/batch_mode/) and using Environment Variables to generalize the batch commands for reuse. The format of environmental variables is `[tag]=[value]`. mothur will automatically pull in the systems environment variable, which we can set in bash and then run the batch file:\n\n```         \n$ export DATA=01_TRIMMED_DATA/\n$ export TYPE=gz\n$ export PROC=30\n\n$ export REF_LOC=reference_dbs\n$ export TAXREF_FASTA=gsrdb.fasta\n$ export TAXREF_TAX=gsrdb.tax\n$ export ALIGNREF=silva.v4.fasta\n\n$ export CONTAMINENTS=Chloroplast-Mitochondria-unknown-Eukaryota\n```\n:::\n\n\n::: {.cell}\n\n:::\n\n\n## Getting Started\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n\n### Expand for the MOTHUR batchfile\n```\n########################################################################\n#$ export DATA=01_TRIMMED_DATA/\n#$ export TYPE=gz\n#$ export PROC=30\n\n#$ export REF_LOC=reference_dbs\n#$ export TAXREF_FASTA=gsrdb.fasta\n#$ export TAXREF_TAX=gsrdb.tax\n#$ export ALIGNREF=silva.v4.fasta\n\n#$ export CONTAMINENTS=Chloroplast-Mitochondria-unknown-Eukaryota\n########################################################################\nset.dir(output=pipelineFiles/)\nmake.file(inputdir=$DATA/, type=$TYPE, prefix=shrimp)\n########################################################################\n#### had to fix shimp.files bc mothur splits name at first underscore (_)\n########################################################################\nmake.contigs(file=shrimp.files, processors=$PROC)\nsummary.seqs(fasta=shrimp.trim.contigs.fasta, count=shrimp.contigs.count_table, processors=$PROC)\ncount.groups(count=shrimp.contigs.count_table)\nscreen.seqs(fasta=current, count=current, maxambig=0, minlength=252, maxlength=254, maxhomop=6, processors=$PROC)\nsummary.seqs(fasta=current, count=current, processors=$PROC)\ncount.groups(count=current)\nunique.seqs(fasta=current, count=current)\nsummary.seqs(count=current, processors=$PROC)\n########################################################################\n#### Aligning reads\n########################################################################\n# https://mothur.org/wiki/silva_reference_files/\n#### Prep reference file\npcr.seqs(fasta=$REF_LOC/silva.seed_v138_2.align, start=13862, end=23445, keepdots=F, processors=$PROC)\nrename.file(input=pipelineFiles/silva.seed_v138_2.pcr.align, new=pipelineFiles/$ALIGNREF)\nsummary.seqs(fasta=pipelineFiles/$ALIGNREF, processors=$PROC)\n#### Align reads\nalign.seqs(fasta=shrimp.trim.contigs.good.unique.fasta, reference=pipelineFiles/$ALIGNREF, processors=$PROC)\nsummary.seqs(fasta=shrimp.trim.contigs.good.unique.align, count=shrimp.trim.contigs.good.count_table, processors=$PROC)\n########################################################################\n#### Further processing\n########################################################################\nscreen.seqs(fasta=current, count=current, start=1, end=9583, processors=$PROC)\nsummary.seqs(fasta=current, count=current, processors=$PROC)\ncount.groups(count=current)\nfilter.seqs(fasta=current, vertical=T, trump=., processors=$PROC)\nunique.seqs(fasta=current, count=current)\nsummary.seqs(fasta=current, count=current, processors=$PROC)\n########################################################################\n#### Copy files FOR MED\n########################################################################\nsystem(cp pipelineFiles/shrimp.trim.contigs.good.unique.good.filter.unique.fasta pipelineFiles_med/)\nsystem(cp pipelineFiles/shrimp.trim.contigs.good.unique.good.filter.count_table pipelineFiles_med/)\n########################################################################\n#### Proceed with mothur workflow\n########################################################################\npre.cluster(fasta=current, count=current, diffs=2, processors=$PROC)\nsummary.seqs(fasta=current, count=current, processors=$PROC)\ncount.groups(count=current)\n########################################################################\n#### Remove Negative Control samples (files generated in R)\n########################################################################\n\n########## IN R ########################################################\n#tmp_accnos <- readr::read_delim(here(med_here, \"nc_screen/shrimp.files\"), delim = \"\\t\", col_names = FALSE)\n#tmp_accnos[, 2:3] <- NULL\n#tmp_accnos <- tmp_accnos[grepl(\"Control_\", tmp_accnos$X1), ]\n#readr::write_delim(tmp_accnos, file = here(med_here, \"nc_screen/nc_samples.accnos\"), col_names = FALSE)\n########################################################################\nget.groups(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.fasta, count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table, accnos=nc_samples.accnos)\nrename.file(input=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta, new=nc.fasta)\nrename.file(input=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table, new=nc.count_table)\nsummary.seqs(fasta=nc.fasta, count=nc.count_table, processors=$PROC)\nlist.seqs(count=nc.count_table)\ncount.seqs(count=nc.count_table, compress=f)\nget.seqs(accnos=nc.accnos, fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.fasta, count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table)\nrename.file(input=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta, new=subset.fasta)\nrename.file(input=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table, new=subset.count_table)\ncount.seqs(count=subset.count_table, compress=f)\n########################################################################\n########## IN R ########################################################\n########################################################################\n#full_count_tab <- readr::read_delim(here(med_here, \"nc_screen/subset.full.count_table\"), delim = \"\\t\", col_names = TRUE)\n#control_cols     <- grep(\"^Control_\", names(full_count_tab), value = TRUE)\n#noncontrol_cols  <- setdiff(names(full_count_tab)[-(1:2)], control_cols)\n## now do the rowwise sums\n#read_totals <- full_count_tab %>%\n#  rowwise() %>%\n#  mutate(\n#    total_reads_nc   = sum(c_across(all_of(control_cols)), na.rm = TRUE),\n#    total_reads_non_nc = sum(c_across(all_of(noncontrol_cols)), na.rm = TRUE)\n#  ) %>%\n#  ungroup() %>%\n#  select(1, 2, total_reads_nc, total_reads_non_nc)\n#\n#read_totals <- read_totals %>% dplyr::rename(\"total_reads\" = 2)\n#tmp_read_totals <- read_totals %>% dplyr::mutate(perc_reads_in_nc = 100*(total_reads_nc / (total_reads_nc + total_reads_non_nc)), .after = \"total_reads_non_nc\")\n#tmp_read_totals$perc_reads_in_nc <- round(tmp_read_totals$perc_reads_in_nc, digits = 6)\n#\n#control_cols     <- grep(\"^Control_\", names(full_count_tab), value = TRUE)\n#noncontrol_cols  <- setdiff(names(full_count_tab)[-(1:2)], control_cols)\n## rowwise tally of non-zero columns\n#samp_totals <- full_count_tab %>%\n#  rowwise() %>%\n#  mutate(\n#    num_nc_samp     = sum(c_across(all_of(control_cols)) != 0, na.rm = TRUE),\n#    num_non_nc_samp = sum(c_across(all_of(noncontrol_cols)) != 0, na.rm = TRUE)\n#  ) %>%\n#  ungroup() %>%\n#  select(1, num_nc_samp, num_non_nc_samp)\n#\n#samp_totals$total_samp <- samp_totals$num_nc_samp + samp_totals$num_non_nc_samp\n#samp_totals <- samp_totals %>%  dplyr::relocate(\"total_samp\", .after = \"Representative_Sequence\")\n#samp_totals <- samp_totals %>% dplyr::mutate(perc_nc_samp = 100*( num_nc_samp / (num_nc_samp + num_non_nc_samp)), .after = \"num_non_nc_samp\")\n#\n#nc_check <- dplyr::left_join(tmp_read_totals, samp_totals, by = \"Representative_Sequence\")\n#write_delim(nc_check, here(med_here, \"nc_screen/reads_in_nc_samples.txt\"),delim = \"\\t\")\n#\n#nc_remove <- nc_check %>% filter(perc_reads_in_nc > 10 | perc_nc_samp > 10)\n#nc_remain <- dplyr::anti_join(nc_check, nc_remove)\n#rem_nc_reads <- sum(nc_remove$total_reads_nc)\n#rem_sam_reads <- sum(nc_remove$total_reads_non_nc)\n#per_reads_rem <- round(100*( rem_nc_reads / (rem_nc_reads + rem_sam_reads)), digits = 3)\n#\n#ret_nc_reads <- sum(nc_remain$total_reads_nc)\n#ret_sam_reads <- sum(nc_remain$total_reads_non_nc)\n#per_reads_ret <- round(100*( ret_nc_reads / (ret_nc_reads + ret_sam_reads)), digits = 3)\n########################################################################\nremove.seqs(accnos=nc_repseq_remove.accnos, fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.fasta, count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table)\ncount.groups(count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table)\n########################################################################\n########## IN R ########################################################\n########################################################################\n#tmp_before <- read_tsv(here(med_here, \"nc_screen/shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count.summary\"), col_names = FALSE, col_select = 1)\n#tmp_after <- read_tsv(here(med_here, \"nc_screen/shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count.summary\"), col_names = FALSE, col_select = 1)\n#tmp_nc_lost <- anti_join(tmp_before, tmp_after)\n#tmp_nc_lost$X1\n#\n#nc_to_remove <- semi_join(tmp_before, tmp_after)\n#nc_to_remove <- nc_to_remove %>% dplyr::filter(stringr::str_starts(X1, \"Control\"))\n# \n#readr::write_delim(nc_to_remove, file = here(med_here, \"nc_screen/nc_samples_remove.accnos\"), col_names = FALSE)\n########################################################################\nremove.groups(count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table, fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta, accnos=nc_samples_remove.accnos)\nsummary.seqs(fasta=current, count=current, processors=30)\ncount.groups(count=current)\nchimera.vsearch(fasta=current, count=current, dereplicate=t, processors=$PROC)\nsummary.seqs(fasta=current, count=current, processors=30)\ncount.groups(count=current)\nclassify.seqs(fasta=current, count=current, reference=reference_dbs/gsrdb.fasta, taxonomy=reference_dbs/gsrdb.tax, processors=3)\nremove.lineage(fasta=current, count=current, taxonomy=current, taxon=Chloroplast-Mitochondria-unknown-Eukaryota)\nsummary.tax(taxonomy=current, count=current)\ncount.groups(count=current)\n\n##########################\n##rename.file(\nfasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.pick.fasta, count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.pick.count_table, taxonomy=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.pick.taxonomy, prefix=final)\n\n##########################\n###    CLUSTERING      ###\n##########################\n\n##########################\n###    cluster.split   ###\n##########################\n\ncluster.split(fasta=final.fasta, count=final.count_table, taxonomy=final.taxonomy, taxlevel=4, cluster=f, processors=30) \ncluster.split(file=final.file, count=final.count_table, processors=30)\n\nsystem(mkdir pipelineFiles/cluster.split.gsrdb)\nsystem(mv pipelineFiles/final.opti_mcc.list pipelineFiles/cluster.split.gsrdb/)\nsystem(mv pipelineFiles/final.file pipelineFiles/cluster.split.gsrdb/)\nsystem(mv pipelineFiles/final.dist pipelineFiles/cluster.split.gsrdb/)\n\n##########################\n###    cluster         ###\n##########################\n\ndist.seqs(fasta=final.fasta, cutoff=0.03, processors=30)\ncluster(column=final.dist, count=final.count_table)\n\nquit()\n```\n\n\n\n:::\n\n\n```{.default}\nset.dir(output=pipelineFiles/)\n```\n\n```         \nMothur's directories:\noutputDir=pipelineFiles/\n```\n\n\n```{.default}\nmake.file(inputdir=$DATA, type=$TYPE, prefix=shrimp)\n# OR\nmake.file(inputdir=01_TRIMMED_DATA/, type=gz, prefix=shrimp)\n```\n\n```         \nSetting input directories to: \n    01_TRIMMED_DATA/\n\nOutput File Names: \nshrimp.files\n```\n\n## Reducing Sequencing & PCR Errors\n\n\n```{.default}\nmake.contigs(file=current, processors=$PROC)\n# OR\nmake.contigs(file=shrimp.files, processors=30)\n```\n\nWe will get the following message if sample names contain dash (`-`) characters. Mothur will change this for us.\n\n```         \n[WARNING]: group Control-10 contains illegal characters in the name. \nGroup names should not include :, -, or / characters.  The ':' character \nis a special character used in trees. Using ':' will result in your tree \nbeing unreadable by tree reading software.  The '-' character is a special \ncharacter used by mothur to parse group names.  Using the '-' character \nwill prevent you from selecting groups. The '/' character will created \nunreadable filenames when mothur includes the group in an output filename.\n\n[NOTE] Updating Control-10 to Control_10 to avoid downstream issues.\n\n...\n\nTotal of all groups is 44710450\n\nIt took 1257 secs to process 44710450 sequences.\n\nOutput File Names: \nshrimp.trim.contigs.fasta\nshrimp.scrap.contigs.fasta\nshrimp.contigs_report\nshrimp.contigs.count_table\n```\n\n\n```{.default}\nsummary.seqs(fasta=current, count=current, processors=$PROC)\nsummary.seqs(fasta=shrimp.trim.contigs.fasta, count=shrimp.contigs.count_table, processors=30)\n```\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n\n### Expand to see data set summary\n\n|             | Start | End | NBases | Ambigs | Polymer | NumSeqs  |\n|:------------|:-----:|:---:|:------:|:------:|:-------:|:--------:|\n| Minimum     |   1   | 226 |  226   |   0    |    2    |    1     |\n| 2.5%-tile:  |   1   | 252 |  252   |   0    |    3    | 1117762  |\n| 25%-tile:   |   1   | 253 |  253   |   0    |    4    | 11177613 |\n| Median:     |   1   | 253 |  253   |   0    |    4    | 22355226 |\n| 75%-tile:   |   1   | 253 |  253   |   0    |    5    | 33532838 |\n| 97.5%-tile: |   1   | 254 |  254   |   6    |    6    | 43592689 |\n| Maximum:    |   1   | 480 |  480   |   95   |   233   | 44710450 |\n| Mean:       |   1   | 254 |  254   |   0    |    4    |          |\n\n```         \n# of unique seqs:   44710450\ntotal # of seqs:    44710450\n\nIt took 782 secs to summarize 44710450 sequences.\n\nOutput File Names:\nshrimp.trim.contigs.summary\n```\n:::\n\n\n```{.default}\ncount.groups(count=current)\ncount.groups(count=shrimp.contigs.count_table)\n```\n\n```         \nSize of smallest group: 58.\n\nTotal seqs: 44710450.\n```\n\n\n```{.default}\nscreen.seqs(fasta=current, count=current, maxambig=0, minlength=252, maxlength=254, maxhomop=6, processors=$PROC)\nscreen.seqs(fasta=current, count=current, maxambig=0, minlength=252, maxlength=254, maxhomop=6, processors=30)\n```\n\n```         \nUsing 30 processors.\n\nIt took 107 secs to screen 44710450 sequences, removed 8308318.\n/******************************************/\nRunning command: remove.seqs(accnos=shrimp.trim.contigs.bad.accnos.temp, count=shrimp.contigs.count_table)\nRemoved 8308318 sequences from shrimp.contigs.count_table.\n\nOutput File Names:\nshrimp.contigs.pick.count_table\n\n/******************************************/\n\nOutput File Names:\nshrimp.trim.contigs.good.fasta\nshrimp.trim.contigs.bad.accnos\nshrimp.contigs.good.count_table\n\nIt took 557 secs to screen 44710450 sequences.\n```\n\n\n```{.default}\nsummary.seqs(fasta=current, count=current, processors=$PROC)\nsummary.seqs(fasta=shrimp.trim.contigs.good.fasta, count=shrimp.contigs.good.count_table, processors=30)\n```\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n\n### Expand to see data set summary\n\n|             | Start | End | NBases | Ambigs | Polymer | NumSeqs  |\n|:------------|:-----:|:---:|:------:|:------:|:-------:|:--------:|\n| Minimum     |   1   | 252 |  252   |   0    |    3    |    1     |\n| 2.5%-tile:  |   1   | 252 |  252   |   0    |    3    |  910054  |\n| 25%-tile:   |   1   | 253 |  253   |   0    |    4    | 9100534  |\n| Median:     |   1   | 253 |  253   |   0    |    4    | 18201067 |\n| 75%-tile:   |   1   | 253 |  253   |   0    |    5    | 27301600 |\n| 97.5%-tile: |   1   | 254 |  254   |   6    |    6    | 35492079 |\n| Maximum:    |   1   | 254 |  254   |   0    |    6    | 36402132 |\n| Mean:       |   1   | 252 |  252   |   0    |    4    |          |\n\n```         \n# of unique seqs:   36402132\ntotal # of seqs:    36402132\n\nIt took 610 secs to summarize 36402132 sequences.\n\nOutput File Names:\nshrimp.trim.contigs.good.summary\n```\n:::\n\n\n```{.default}\ncount.groups(count=current)\ncount.groups(count=shrimp.contigs.good.count_table)\n```\n\n```         \nSize of smallest group: 57.\n\nTotal seqs: 36402132.\nOutput File Names: \nshrimp.contigs.good.count.summary\n```\n\n## Processing Improved Reads\n\n\n```{.default}\nunique.seqs(fasta=current, count=current)\nunique.seqs(fasta=shrimp.trim.contigs.good.fasta, count=shrimp.contigs.good.count_table)\n```\n\n```         \n36402132    4224192\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.fasta\nshrimp.trim.contigs.good.count_table\n```\n\n\n```{.default}\nsummary.seqs(count=current, processors=$PROC)\nsummary.seqs(count=shrimp.trim.contigs.good.count_table, processors=30)\n```\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n\n### Expand to see data set summary\n\n|             | Start | End | NBases | Ambigs | Polymer | NumSeqs  |\n|:------------|:-----:|:---:|:------:|:------:|:-------:|:--------:|\n| Minimum     |   1   | 252 |  252   |   0    |    3    |    1     |\n| 2.5%-tile:  |   1   | 252 |  252   |   0    |    3    |  910054  |\n| 25%-tile:   |   1   | 253 |  253   |   0    |    4    | 9100534  |\n| Median:     |   1   | 253 |  253   |   0    |    4    | 18201067 |\n| 75%-tile:   |   1   | 253 |  253   |   0    |    5    | 27301600 |\n| 97.5%-tile: |   1   | 254 |  254   |   6    |    6    | 35492079 |\n| Maximum:    |   1   | 254 |  254   |   0    |    6    | 36402132 |\n| Mean:       |   1   | 252 |  252   |   0    |    4    |          |\n\n```         \n# of unique seqs:   4224192\ntotal # of seqs:    36402132\n\nIt took 78 secs to summarize 36402132 sequences.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.summary\n```\n:::\n\n## Aligning Reads\n\nSince we are using the `silva.nr_v132.align` to align sequences, we can check where our reads start and end with the [ARB-SILVA web aligner](https://www.arb-silva.de/aligner/). After uploading a few sequences we find they start at postion 13862 and end at position 23445. Neat. We will pad these numbers to make sure we do not miss anything. \n\n\n```{.default}\npcr.seqs(fasta=$REF_LOC/silva.seed_v138_2.align, start=13862, end=23445, keepdots=F, processors=$PROC)\npcr.seqs(fasta=reference_dbs/silva.seed_v138_2.align, start=13862, end=23445, keepdots=F, processors=30)\n```\n\n```         \nUsing 30 processors.\n[NOTE]: no sequences were bad, removing silva.nr_v132.bad.accnos\n\nIt took 10 secs to screen 8696 sequences.\n\nOutput File Names: \nsilva.seed_v138_2.pcr.align\n```\n\n\n```{.default}\nrename.file(input=pipelineFiles/silva.nr_v132.pcr.align, new=pipelineFiles/$ALIGNREF)\nrename.file(input=pipelineFiles/silva.seed_v138_2.pcr.align, new=pipelineFiles/silva.v4.fasta)\n\nsummary.seqs(fasta=pipelineFiles/$ALIGNREF, processors=$PROC)\nsummary.seqs(fasta=pipelineFiles/silva.v4.fasta, processors=30)\n```\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n\n### Expand to see data set summary\n\n|             | Start |  End | NBases | Ambigs | Polymer | NumSeqs |\n|:------------|:-----:|:----:|:------:|:------:|:-------:|:-------:|\n| Minimum     |   1   | 8722 |  221   |   0    |    3    |    1    |\n| 2.5%-tile:  |   1   | 9583 |  252   |   0    |    4    |   218   |\n| 25%-tile:   |   1   | 9583 |  253   |   0    |    4    |   2175  |\n| Median:     |   1   | 9583 |  253   |   0    |    5    |   4349  |\n| 75%-tile:   |   1   | 9583 |  254   |   0    |    6    |   6523  |\n| 97.5%-tile: |   1   | 9583 |  421   |   1    |    6    |   8479  |\n| Maximum:    |   15  | 9584 |  1082  |   5    |   10    |   8696  |\n| Mean:       |   1   | 9582 |  288   |   0    |    4    |         |\n\n```         \n# of Seqs:  8696\n\nIt took 1 secs to summarize 8696 sequences.\n\nOutput File Names:\nsilva.v4.summary\n```\n:::\n\n\n```{.default}\nalign.seqs(fasta=current, reference=$REF_LOC/$ALIGNREF, processors=$PROC)\nalign.seqs(fasta=shrimp.trim.contigs.good.unique.fasta, reference=pipelineFiles/silva.v4.fasta, processors=30)\n```\n\n```         \nUsing 30 processors.\n\nReading in the silva.v4.fasta template sequences... DONE.\nIt took 34 to read  213119 sequences.\n\nAligning sequences from shrimp.trim.contigs.good.unique.fasta ...\nIt took 2 to read  8696 sequences.\n\n[WARNING]: 854 of your sequences generated alignments that \neliminated too many bases, a list is provided in \nshrimp.trim.contigs.good.unique.flip.accnos.\n[NOTE]: 444 of your sequences were reversed to produce a better alignment.\n\nIt took 780 seconds to align 4224192 sequences.\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.align\nshrimp.trim.contigs.good.unique.align_report\nshrimp.trim.contigs.good.unique.flip.accnos\n```\n\n\n```{.default}\nsummary.seqs(fasta=current, count=current, processors=$PROC)\nsummary.seqs(fasta=shrimp.trim.contigs.good.unique.align, count=shrimp.trim.contigs.good.count_table, processors=30)\n```\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n\n### Expand to see data set summary\n\n|             | Start |  End | NBases | Ambigs | Polymer | NumSeqs  |\n|:------------|:-----:|:----:|:------:|:------:|:-------:|:--------:|\n| Minimum     |   0   |   0  |   0    |   0    |    1    |    1     |\n| 2.5%-tile:  |   1   | 9583 |  252   |   0    |    3    |  910054  |\n| 25%-tile:   |   1   | 9583 |  253   |   0    |    4    | 9100534  |\n| Median:     |   1   | 9583 |  253   |   0    |    4    | 18201067 |\n| 75%-tile:   |   1   | 9583 |  253   |   0    |    5    | 27301600 |\n| 97.5%-tile: |   1   | 9583 |  253   |   0    |    6    | 35492079 |\n| Maximum:    | 9583  | 9584 |  254   |   0    |    6    | 36402132 |\n| Mean:       |   1   | 9581 |  252   |   0    |    4    |          |\n\n```         \n# of unique seqs:   4224192\ntotal # of seqs:    36402132\n\nIt took 141 secs to summarize 36402132 sequences.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.summary\n```\n:::\n\n\n```{.default}\nscreen.seqs(fasta=current, count=current, start=1968, end=11550, processors=$PROC)\nscreen.seqs(fasta=shrimp.trim.contigs.good.unique.align, count=shrimp.trim.contigs.good.count_table, start=1, end=9583, processors=30)\n```\n\n```         \nUsing 30 processors.\n\nIt took 66 secs to screen 4224192 sequences, removed 21326.\n\n/******************************************/\nRunning command: \nremove.seqs(accnos=shrimp.trim.contigs.good.unique.bad.accnos.temp, \ncount=shrimp.trim.contigs.good.count_table)\nRemoved 79070 sequences from shrimp.trim.contigs.good.count_table.\n\nOutput File Names:\nshrimp.trim.contigs.good.pick.count_table\n\n/******************************************/\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.align\nshrimp.trim.contigs.good.unique.bad.accnos\nshrimp.trim.contigs.good.good.count_table\n```\n\n\n```{.default}\nsummary.seqs(fasta=current, count=current, processors=$PROC)\nsummary.seqs(fasta=shrimp.trim.contigs.good.unique.good.align, count=shrimp.trim.contigs.good.good.count_table, processors=30)\n```\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n\n### Expand to see data set summary\n\n|             | Start |  End | NBases | Ambigs | Polymer | NumSeqs  |\n|:------------|:-----:|:----:|:------:|:------:|:-------:|:--------:|\n| Minimum     |   1   | 9583 |  208   |   0    |    3    |    1     |\n| 2.5%-tile:  |   1   | 9583 |  252   |   0    |    3    |  908077  |\n| 25%-tile:   |   1   | 9583 |  253   |   0    |    4    | 9080766  |\n| Median:     |   1   | 9583 |  253   |   0    |    4    | 18161532 |\n| 75%-tile:   |   1   | 9583 |  253   |   0    |    5    | 27242297 |\n| 97.5%-tile: |   1   | 9583 |  253   |   0    |    6    | 35414986 |\n| Maximum:    |   1   | 9584 |  254   |   0    |    6    | 36323062 |\n| Mean:       |   1   | 9583 |  252   |   0    |    4    |          |\n\n```         \n# of unique seqs:   4202866\ntotal # of seqs:    36323062\n\nIt took 152 secs to summarize 36323062 sequences.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.summary\n```\n:::\n\n\n```{.default}\ncount.groups(count=current)\ncount.groups(count=shrimp.trim.contigs.good.good.count_table)\n```\n\n```         \nSize of smallest group: 57.\n\nTotal seqs: 36323062.\n\nOutput File Names: \nshrimp.trim.contigs.good.good.count.summary\n```\n\n\n```{.default}\nfilter.seqs(fasta=current, vertical=T, trump=., processors=$PROC)\nfilter.seqs(fasta=shrimp.trim.contigs.good.unique.good.align, vertical=T, trump=., processors=30)\n```\n\n```         \nUsing 30 processors.\nCreating Filter...\nIt took 31 secs to create filter for 4202866 sequences.\n\n\nRunning Filter...\nIt took 26 secs to filter 4202866 sequences.\n\nLength of filtered alignment: 554\nNumber of columns removed: 9030\nLength of the original alignment: 9584\nNumber of sequences used to construct filter: 4202866\n\nOutput File Names: \nshrimp.filter\nshrimp.trim.contigs.good.unique.good.filter.fasta\n```\n\n\n```{.default}\nunique.seqs(fasta=current, count=current)\nunique.seqs(fasta=shrimp.trim.contigs.good.unique.good.filter.fasta, count=shrimp.trim.contigs.good.good.count_table)\n```\n\n```         \n4202866\t4178668\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.good.filter.unique.fasta\nshrimp.trim.contigs.good.unique.good.filter.count_table\n```\n\n\n```{.default}\nsummary.seqs(fasta=current, count=current, processors=$PROC)\nsummary.seqs(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.fasta, count=shrimp.trim.contigs.good.unique.good.filter.count_table, processors=30)\n```\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n\n### Expand to see data set summary\n\n|             | Start | End | NBases | Ambigs | Polymer | NumSeqs  |\n|:------------|:-----:|:---:|:------:|:------:|:-------:|:--------:|\n| Minimum     |   1   | 554 |  208   |   0    |    3    |    1     |\n| 2.5%-tile:  |   1   | 554 |  252   |   0    |    3    |  908077  |\n| 25%-tile:   |   1   | 554 |  253   |   0    |    4    | 9080766  |\n| Median:     |   1   | 554 |  253   |   0    |    4    | 18161532 |\n| 75%-tile:   |   1   | 554 |  253   |   0    |    5    | 27242297 |\n| 97.5%-tile: |   1   | 554 |  253   |   0    |    6    | 35414986 |\n| Maximum:    |   1   | 554 |  254   |   0    |    6    | 36323062 |\n| Mean:       |   1   | 554 |  252   |   0    |    4    |          |\n\n```         \n# of unique seqs:   4178668\ntotal # of seqs:    36323062\n\nIt took 10 secs to summarize 36323062 sequences.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.summary\n```\n:::\n\nWe now have a fully aligned and curated dataset that we can now pass off to the [MED pipeline](workflows/ssu/med/). \n\n\n```{.default}\nsystem(cp pipelineFiles/shrimp.trim.contigs.good.unique.good.filter.unique.fasta pipelineFiles_med/)\nsystem(cp pipelineFiles/shrimp.trim.contigs.good.unique.good.filter.count_table pipelineFiles_med/)\n```\n\nMoving on, to the next step of the mothur OTU pipeline. \n\n## Precluster\n\n\n```{.default}\npre.cluster(fasta=current, count=current, diffs=2, processors=$PROC)\n```\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n\n### Expand to see partial output of `pre.cluster`\n\n```         \nUsing 30 processors.\n\n/******************************************/\nSplitting by sample: \n\nUsing 30 processors.\n\nSelecting sequences for groups Control_1-Control_10-Control_11-Control_12\n-Control_13-Control_14-Control_15-Control_16-Control_17-Control_18\n\nSelected 828 sequences from WA_A_FORM_EG_7457.\nSelected 2958 sequences from WA_A_FORM_EG_7752.\nSelected 5589 sequences from WA_A_FORM_EG_9400.\nSelected 4632 sequences from WA_A_FORM_GL_7402.\nSelected 3860 sequences from WA_A_FORM_GL_7403.\nSelected 5518 sequences from WA_A_FORM_GL_7455.\n\n/******************************************/\n\nDeconvoluting count table results...\nIt took 17 secs to merge 2465292 sequences group data.\n/******************************************/\nRunning get.seqs: \nSelected 1675449 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.fasta.\n/******************************************/\nIt took 8102 secs to run pre.cluster.\n\nUsing 3 processors.\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.fasta\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table\n\n```\n:::\n\n\n```{.default}\nsummary.seqs(fasta=current, count=current, processors=$PROC)\n```\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n\n### Expand to see data set summary\n\n|             | Start | End | NBases | Ambigs | Polymer | NumSeqs  |\n|:------------|:-----:|:---:|:------:|:------:|:-------:|:--------:|\n| Minimum     |   1   | 554 |  208   |   0    |    3    |    1     |\n| 2.5%-tile:  |   1   | 554 |  252   |   0    |    3    |  908077  |\n| 25%-tile:   |   1   | 554 |  253   |   0    |    4    | 9080766  |\n| Median:     |   1   | 554 |  253   |   0    |    4    | 18161532 |\n| 75%-tile:   |   1   | 554 |  253   |   0    |    5    | 27242297 |\n| 97.5%-tile: |   1   | 554 |  253   |   0    |    6    | 35414986 |\n| Maximum:    |   1   | 554 |  254   |   0    |    6    | 36323062 |\n| Mean:       |   1   | 554 |  252   |   0    |    4    |          |\n\n```         \n# of unique seqs:   1675449\ntotal # of seqs:    36323062\n\nIt took 35 secs to summarize 36323062 sequences.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.summary\n```\n:::\n\n\n```{.default}\ncount.groups(count=current)\n```\n\n```         \nSize of smallest group: 57.\n\nTotal seqs: 36323062.\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.count.summary\n```\n\n## Remove Negative Controls\n\nAs with the OTU workflow, we remove NC samples, but in this case we skip the `pre.cluster` step. Our goal here to remove all NC samples from the dataset, as well as any reads that are predominantly found in NC samples. More on what that means in a minute. Before we do anything we need to identify all reads present in NC samples. \n\nHere is what we are going to do:\n\n1. Subset the NC samples (and associated reads) from the `fasta` and `count.table`. To do this in mothur we need all of the NC sample names collected in an `.accnos` file, which is a text file used in  mothur  that contains a single column of names--these can be sequences, OTU labels, or sample names. This list is used to select or remove specific items from other mothur files. That way we can process subsets of the data without having to retype long lists of names. \n\nTo generate the  `.accnos` file of NC samples we can use the `shrimp.files` file generated at the beginning of the mothur pipeline.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp_accnos <- readr::read_delim(here(work_here, \"nc_screen/shrimp.files\"), \n                                delim = \"\\t\", col_names = FALSE)\ntmp_accnos[, 2:3] <- NULL\ntmp_accnos <- tmp_accnos[grepl(\"Control_\", tmp_accnos$X1), ]\nreadr::write_delim(tmp_accnos, file = here(work_here, \"nc_screen/nc_samples.accnos\"), \n                   col_names = FALSE)\n```\n:::\n\n\n2. Now we have a list of all NC sample names. The mothur command `get.groups` in conjunction with `accnos` file allows us to subset the full `fasta` and `count_table`\n\n\n\n```{.default}\nget.groups(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.fasta, count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table, accnos=nc_samples.accnos)\n```\n\n```         \nSelected 192842 sequences from your count file.\nSelected 4148 sequences from your fasta file.\n\nOutput File names:\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta\n\n```\n\n3. Next we rename the new files to something more informative (and shorter).\n\n\n```{.default}\nrename.file(input=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta, new=nc.fasta)\nrename.file(input=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table, new=nc.count_table)\n```\n\n4. And a quick summary of the NC subset. \n\n\n```{.default}\nsummary.seqs(fasta=nc.fasta, count=nc.count_table, processors=$PROC)\n```\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n\n### Expand to see negative control summary\n\n|             | Start | End | NBases | Ambigs | Polymer | NumSeqs |\n|:------------|:-----:|:---:|:------:|:------:|:-------:|:-------:|\n| Minimum     |   1   | 554 |  248   |   0    |    3    |    1    |\n| 2.5%-tile:  |   1   | 554 |  252   |   0    |    4    |  4822   |\n| 25%-tile:   |   1   | 554 |  253   |   0    |    4    |  48211  |\n| Median:     |   1   | 554 |  253   |   0    |    5    |  96422  |\n| 75%-tile:   |   1   | 554 |  253   |   0    |    5    | 144632  |\n| 97.5%-tile: |   1   | 554 |  254   |   0    |    6    | 188021  |\n| Maximum:    |   1   | 554 |  254   |   0    |    6    | 192842  |\n| Mean:       |   1   | 554 |  253   |   0    |    4    |         |\n\n```         \n# of unique seqs:   4148\ntotal # of seqs:    192842\n\nIt took 0 secs to summarize 192842 sequences.\n\nOutput File Names:\nneg_control.summary\n```\n:::\n\n5. Sweet. We use the command `list.seqs` to get a complete list of all repseq names in the NC subset.\n\n\n```{.default}\nlist.seqs(count=nc.count_table)\n```\n\n```         \nOutput File Names: \nnc.accnos\n```\n\nThis gives us all repseq IDs in the NC samples. \n\n6.  We could simply use the `nc.accnos` file from the `list.seqs` command to remove repseqs found in negative control (NC) samples from the main dataset. This seems reasonable enough. Except mothur will remove **all** repseqs found in a NC sample, in other words, any repseq in the accnos file. For example, let's consider the following scenario where we have two repseqs:\n\n`repseq01` is abundant in many NC samples but not found in any other samples.   \n`repseq02` on the other hand is represented by say one read in a single NC sample but very abundant in other samples.   \n\nIt makes sense to remove `repseq01` but not necessarily `repseq02`. Essentially, for each `repseq` in the `nc.accnos` file we want to calculate:\n\n-   The total number of reads in NC samples.\\\n-   The total number of reads in non-NC samples.\\\n-   The percent of reads in the NC samples.\n-   The total number of NC samples containing at least 1 read.\\\n-   The total number of non-NC samples containing at least 1 read.\\\n-   The percent of NC samples containing reads.\n\nWhere a final data table might look something like this\n\n| repseq     | rc_nc | rc_samps | %in_nc  | nc_samp | no_nc_samp | %_in_nc_samp   |\n|:-----------|:-----:|:--------:|:-------:|:-------:|:----------:|:--------------:|\n| repseq001  |   3   | 5        |  37.5   |   1     |    2       |    33.31       |\n| repseq002  |   196 | 308      |  38.9   |   17    |    38      |    30.7        |\n| repseq003  |   3   | 23       |  11.1   |   3     |    18      |    14.5        |\n\nTo accomplish this we will parse out relevant data from the `.count_table` files. We got the idea on how best to do this from a [discussion on the mothur forum](https://forum.mothur.org/t/negative-control/2754).\n\nTo save space and minimize file size, mothur formats the [`.count_table`](https://mothur.org/wiki/count_file/) using a sparse format by storing only non zero sample counts. However, we need the full format which lists each repseq and its abundance counts for each sample. Using the command `count.seqs` in conjunction with the `.count_table` will return a full format table. \n\n\n```{.default}\ncount.seqs(count=nc.count_table, compress=f)\n```\n\n```\nOutput File Names:\nnc.full.count_table\n```\n\nThen we use the accnos file (`nc.accnos`)--containing repseqs found in NC samples, to generate a subset of the complete dataset containing only the reseqs found in NC samples. \n\n\n```{.default}\nget.seqs(accnos=nc.accnos, fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.fasta, count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table)\n```\n\n```\nSelected 4148 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.precluster.fasta.\nSelected 15029155 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table\n```\n\n```{.default}\nrename.file(input=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta, new=subset.fasta)\nrename.file(input=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table, new=subset.count_table)\n```\n\nAnd again run `count.seqs` to get a full format table of the complete dataset, which we subsetted earlier to contain only repseqs found in NC samples. \n\n\n```{.default}\ncount.seqs(count=subset.count_table, compress=f)\n```\n\n```\nOutput File Names:\nsubset.full.count_table\n```\n\nFinally we can parse out read count data from the  two `subset.full.count_table` files. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nfull_count_tab <- readr::read_delim(here(work_here, \"nc_screen/subset.full.count_table\"), \n                                    delim = \"\\t\", col_names = TRUE)\n# figure out which columns to use\ncontrol_cols     <- grep(\"^Control_\", names(full_count_tab), value = TRUE)\nnoncontrol_cols  <- setdiff(names(full_count_tab)[-(1:2)], control_cols)\n\n# now do the rowwise sums\nread_totals <- full_count_tab %>%\n  rowwise() %>%\n  mutate(\n    total_reads_nc   = sum(c_across(all_of(control_cols)), na.rm = TRUE),\n    total_reads_non_nc = sum(c_across(all_of(noncontrol_cols)), na.rm = TRUE)\n  ) %>%\n  ungroup() %>%\n  select(1, 2, total_reads_nc, total_reads_non_nc)\n\nread_totals <- read_totals %>% dplyr::rename(\"total_reads\" = 2)\n```\n:::\n\n\nAnd here is what the new dataframe looks like. Three columns where the first is the repseq name, the second the total number of reads in NC samples, and the third the total number of reads in the entire dataset (this includes NC samples).\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n-------------------------------------------------------------------------------------------------\n           Representative_Sequence             total_reads   total_reads_nc   total_reads_non_nc \n--------------------------------------------- ------------- ---------------- --------------------\n M06508_12_000000000-CJG44_1_1101_16846_2451      51955          31810              20145        \n\n M06508_18_000000000-CNPPR_1_1101_15534_2103     304220            2                304218       \n\n M06508_12_000000000-CJG44_1_1101_15015_3135      9474            900                8574        \n\n M06508_12_000000000-CJG44_1_1101_15072_3643      39666           4326              35340        \n\n M06508_9_000000000-JTBW3_1_1106_8860_16108      509857            2                509855       \n\n M06508_9_000000000-JTBW3_1_1106_6953_16246      997443            9                997434       \n-------------------------------------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\nIn total there are 4148 repseqs that were potential contaminants.\n\nNow we add in a column that calculates the percent of reads in the NC samples. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp_read_totals <- read_totals %>%\n  dplyr::mutate(perc_reads_in_nc = 100*(\n    total_reads_nc / (total_reads_nc + total_reads_non_nc)),\n                .after = \"total_reads_non_nc\")\ntmp_read_totals$perc_reads_in_nc <- \n  round(tmp_read_totals$perc_reads_in_nc, digits = 6)\n```\n:::\n\n\nAnd then we calculate row sums to get the number of NC and non-NC samples containing these reads. For this we can use the `subset.full.count_table` we read in above.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncontrol_cols     <- grep(\"^Control_\", names(full_count_tab), value = TRUE)\nnoncontrol_cols  <- setdiff(names(full_count_tab)[-(1:2)], control_cols)\n# rowwise tally of non-zero columns\nsamp_totals <- full_count_tab %>%\n  rowwise() %>%\n  mutate(\n    num_nc_samp     = sum(c_across(all_of(control_cols)) != 0, na.rm = TRUE),\n    num_non_nc_samp = sum(c_across(all_of(noncontrol_cols)) != 0, na.rm = TRUE)\n  ) %>%\n  ungroup() %>%\n  select(1, num_nc_samp, num_non_nc_samp)\n```\n:::\n\n\nFinally add a column with the total number of samples and calculate the percent of NC samples containing these reads. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamp_totals$total_samp <- samp_totals$num_nc_samp + samp_totals$num_non_nc_samp\nsamp_totals <- samp_totals %>%  \n  dplyr::relocate(\"total_samp\", .after = \"Representative_Sequence\")\nsamp_totals <- samp_totals %>%\n  dplyr::mutate(perc_nc_samp = \n                  100*( num_nc_samp / (num_nc_samp + num_non_nc_samp)),\n                  .after = \"num_non_nc_samp\")\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\nAfter all of this wrangling here is a snippet of the table showing the distribution of NC reads across samples. \n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n           Representative_Sequence             total_reads   total_reads_nc   total_reads_non_nc   perc_reads_in_nc   total_samp   num_nc_samp   num_non_nc_samp   perc_nc_samp \n--------------------------------------------- ------------- ---------------- -------------------- ------------------ ------------ ------------- ----------------- --------------\n M06508_12_000000000-CJG44_1_1101_16846_2451      51955          31810              20145               61.23            641           41              600            6.396     \n\n M06508_18_000000000-CNPPR_1_1101_15534_2103     304220            2                304218             0.000657          1099           2             1097            0.182     \n\n M06508_12_000000000-CJG44_1_1101_15015_3135      9474            900                8574                9.5             386            8              378            2.073     \n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\nNow we remove any repseqs where:\n\n-   The number of reads found in NC samples accounted for more than 10% of total reads OR\n-   The percent of NC samples containing the repseq was greater than 10% of total samples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnc_remove <- nc_check %>% \n  filter(perc_reads_in_nc > 10 | perc_nc_samp > 10)\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n|          | Total rep seqs      | NC reads         | non NC reads      | \\% NC reads       |\n|----------|---------------------|------------------|-------------------|-------------------|\n| Removed  | 3886 | 156935 | 218220 | 41.832 |\n| Retained | 262 | 35907 | 14618093 | 0.245 |\n\nWe identified a total of **4148** representative sequences (`repseqs`) that were present in at least 1 NC sample by at least 1 read. We removed any repseq where more than 10% of total reads were found in NC samples OR any repseq found in more than 10% of NC samples. Based on these criteria we removed **3886 **repseqs from the data set, which accounted for **156935** total reads in NC samples and **218220** total reads in non-NC samples. Of the total reads removed **41.832%** came from NC samples. Of all repseqs identified in NC samples, **262** were retained because they fell below the threshold criteria. These repseqs accounted for **35907** reads in NC samples and **14618093** reads in non-NC samples. NC samples accounted for **0.245%** of these reads.\n\nOK, now we can create a new `neg_control.accnos` containing only repseqs abundant in NC samples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_delim(\n  data.frame(nc_remove$Representative_Sequence), \n  here(work_here, \"nc_screen/nc_repseq_remove.accnos\"), \n  col_names = FALSE)\n```\n:::\n\n\nAnd then use this file in conjunction with the mothur command `remove.seqs`. \n\n\n```{.default}\nremove.seqs(accnos=nc_repseq_remove.accnos, fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.fasta, count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table)\n```\n\n```         \nRemoved 3886 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.precluster.fasta.\nRemoved 375155 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table\n\n```\n\n\n```{.default}\ncount.groups(count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table)\n```\n\n```         \nSize of smallest group: 1.\n\nTotal seqs: 35947907.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count.summary\n```\n\nBefore we remove the NC samples we need to check whether some NC samples were already removed. When mothur runs the `remove.seqs` command it will automatically remove any samples where the read count has fallen to zero. If mothur did remove samples and we try to remove all NC samples, we will get an error. To check we can compare the `count.summary` files before and after the previous `remove.seqs` command.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp_before <- read_tsv(\n  here(work_here, \n       \"nc_screen/shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count.summary\"),\n  col_names = FALSE,\n  col_select = 1\n)\n\ntmp_after <- read_tsv(\n  here(work_here, \n       \"nc_screen/shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count.summary\"),\n  col_names = FALSE,\n  col_select = 1\n)\ntmp_nc_lost <- anti_join(tmp_before, tmp_after)\ntmp_nc_lost$X1\n```\n:::\n\n\nThese are the samples that were removed when we ran `remove.seqs`. We need to eliminate these sample IDs from our list of NC samples to remove or mothur will throw an error. \n\n```         \n[1] \"Control_15\" \"Control_18\" \"Control_21\" \"Control_29\" \"Control_5\"   \n```\n\nAs before, we can generate a list of NC samples to use in conjunction with the  `remove.groups` command to eliminate all NC samples. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nnc_to_remove <- semi_join(tmp_before, tmp_after)\nnc_to_remove <- nc_to_remove %>%\n  dplyr::filter(\n    stringr::str_starts(X1, \"Control\")\n    )\nreadr::write_delim(nc_to_remove, \n                   file = here(work_here, \"nc_screen/nc_samples_remove.accnos\"), \n                   col_names = FALSE)\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\nIn total the following mothur command should remove 55 negative control samples.\n\n\n```{.default}\nremove.groups(count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table, fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta, accnos=nc_samples_remove.accnos)\n```\n\n```         \nRemoved 35907 sequences from your count file.\nRemoved 0 sequences from your fasta file.\n\nOutput File names:\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.count_table\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.fasta\n```\n\n\n```{.default}\nsummary.seqs(fasta=current, count=current, processors=30)\n```\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n\n### Expand to see data set summary\n\n|             | Start | End | NBases | Ambigs | Polymer | NumSeqs  |\n|:------------|:-----:|:---:|:------:|:------:|:-------:|:--------:|\n| Minimum     |   1   | 554 |  208   |   0    |    3    |    1     |\n| 2.5%-tile:  |   1   | 554 |  252   |   0    |    3    |  897801  |\n| 25%-tile:   |   1   | 554 |  253   |   0    |    4    | 8978001  |\n| Median:     |   1   | 554 |  253   |   0    |    4    | 17956001 |\n| 75%-tile:   |   1   | 554 |  253   |   0    |    5    | 26934001 |\n| 97.5%-tile: |   1   | 554 |  253   |   0    |    6    | 35014201 |\n| Maximum:    |   1   | 554 |  254   |   0    |    6    | 35912000 |\n| Mean:       |   1   | 554 |  252   |   0    |    4    |          |\n\n```         \n# of unique seqs:   1671563\ntotal # of seqs:    35912000\n\nIt took 74 secs to summarize 35912000 sequences.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.summary\n```\n\n:::\n\n\n```{.default}\ncount.groups(count=current)\n```\n\n```         \nSize of smallest group: 14.\n\nTotal seqs: 35912000.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.count.summary\n```\n\n## Remove Chimeras\n\n\n```{.default}\nchimera.vsearch(fasta=current, count=current, dereplicate=t, processors=$PROC)\n```\n\n```         \nUsing vsearch version v2.30.0.\nChecking sequences from shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.fasta ...\n/******************************************/\nSplitting by sample: \n\n...\n\nRemoving chimeras from your input files:\n/******************************************/\nRunning command: remove.seqs(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.fasta, accnos=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.accnos)\nRemoved 619952 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.fasta.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.pick.fasta\n\n/******************************************/\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.count_table\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.chimeras\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.accnos\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.fasta\n```\n\n\n```{.default}\nsummary.seqs(fasta=current, count=current, processors=30)\n```\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n\n### Expand to see data set summary\n\n|             | Start | End | NBases | Ambigs | Polymer | NumSeqs  |\n|:------------|:-----:|:---:|:------:|:------:|:-------:|:--------:|\n| Minimum     |   1   | 554 |  208   |   0    |    3    |    1     |\n| 2.5%-tile:  |   1   | 554 |  252   |   0    |    3    |  863358  |\n| 25%-tile:   |   1   | 554 |  253   |   0    |    4    | 8633575  |\n| Median:     |   1   | 554 |  253   |   0    |    4    | 17267149 |\n| 75%-tile:   |   1   | 554 |  253   |   0    |    5    | 25900723 |\n| 97.5%-tile: |   1   | 554 |  253   |   0    |    6    | 33670940 |\n| Maximum:    |   1   | 554 |  254   |   0    |    6    | 34534297 |\n| Mean:       |   1   | 554 |  252   |   0    |    4    |          |\n\n```         \n# of unique seqs:   1051611\ntotal # of seqs:    34534297\n\nIt took 22 secs to summarize 34534297 sequences.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.summary\n```\n:::\n\n\n```{.default}\ncount.groups(count=current)\n```\n\n```         \nSize of smallest group: 14.\n\nTotal seqs: 34534297.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.count.summary\n```\n\n## Assign Taxonomy\n\nThe `classify.seqs` command requires properly formatted reference and taxonomy databases. For taxonomic assignment, we are using the GSR database [@molano2024gsr]. The developers of mothur maintain [formatted versions of popular databases](https://mothur.org/wiki/taxonomy_outline/), however the GSR-DB has not been formatted by the developers yet.\n\n::: callout-note\nHere can download an appropriate version of the [GSR database](https://manichanh.vhir.org/gsrdb/download_db_links2.php).\n:::\n\nTo create a mothur formatted version GSR-DB[^_merge_runs-1], we perform the following steps.\n\n[^_merge_runs-1]: From the developers: GSR database (Greengenes, SILVA, and RDP database) is an integrated and manually curated database for bacterial and archaeal 16S amplicon taxonomy analysis. Unlike previous integration approaches, this database creation pipeline includes a taxonomy unification step to ensure consistency in taxonomical annotations. The database was validated with three mock communities and two real datasets and compared with existing 16S databases such as Greengenes, GTDB, ITGDB, SILVA, RDP, and MetaSquare. Results showed that the GSR database enhances taxonomical annotations of 16S sequences, outperforming current 16S databases at the species level. The GSR database is available for full-length 16S sequences and the most commonly used hypervariable regions: V4, V1-V3, V3-V4, and V3-V5.\n\n### Make a custom DB\n\nHere we are using the [GSR V4 database](https://manichanh.vhir.org/gsrdb/GSR-DB_V4_cluster-1.tar.gz). \n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nwget https://manichanh.vhir.org/gsrdb/GSR-DB_V4_cluster-1.tar.gz\ntar -xvzf GSR-DB_V4_cluster-1.tar.gz\n```\n:::\n\n\nFirst (in the command line) we remove first line of the taxonomy file. \n\n\n::: {.cell}\n\n```{.zsh .cell-code}\ncp GSR-DB_V4_cluster-1_taxa.txt tmp0.txt\nsed '1d' tmp0.txt > tmp1.txt\n```\n:::\n\n\nNext, delete species and remove leading \\[a-z\\]\\_\\_ from taxa names\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nsed -E 's/s__.*//g' tmp1.txt > tmp2.txt\nsed -E 's/[a-zA-Z]__//g' tmp2.txt > gsrdb.tax\ncp GSR-DB_V4_cluster-1_seqs.fasta gsrdb.fasta\n```\n:::\n\n\n\n```{.default}\nclassify.seqs(fasta=current, count=current, reference=reference_dbs/gsrdb.fasta, taxonomy=reference_dbs/gsrdb.tax, processors=3)\n```\n\n```         \nUsing 30 processors.\nGenerating search database...    DONE.\nIt took 2 seconds generate search database.\n\nReading in the reference_dbs/gsrdb.txt taxonomy...  DONE.\nCalculating template taxonomy tree...     DONE.\nCalculating template probabilities...     DONE.\nIt took 6 seconds get probabilities.\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.fasta ...\n[WARNING]: M06508_9_000000000-JTBW3_1_1102_26159_16839 could not be classified. \nYou can use the remove.lineage command with taxon=unknown; \nto remove such sequences.\n...\n\nIt took 348 secs to classify 1051611 sequences.\n\nIt took 503 secs to create the summary file for 1051611 sequences.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.taxonomy\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.tax.summary\n```\n\n## Remove Contaminants\n\n\n```{.default}\nremove.lineage(fasta=current, count=current, taxonomy=current, taxon=$CONTAMINENTS)\n```\n\n```         \nRunning command: \nremove.seqs(accnos=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.accnos, \ncount=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.count_table, \nfasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.fasta)\n\nRemoved 617 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.fasta.\nRemoved 2776 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.count_table.\n\n/******************************************/\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.pick.taxonomy\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.accnos\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.pick.count_table\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.pick.fasta\n```\n\n\n```{.default}\nsummary.tax(taxonomy=current, count=current)\n```\n\n```         \nUsing shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.pick.count_table \nas input file for the count parameter.\nUsing shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.pick.taxonomy \nas input file for the taxonomy parameter.\n\nIt took 489 secs to create the summary file for 34531521 sequences.\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.pick.tax.summary\n```\n\n\n```{.default}\ncount.groups(count=current)\n```\n\n```         \nSize of smallest group: 14.\n\nTotal seqs: 34531521.\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.pick.count.summary\n```\n\n## Track Reads through Workflow\n\nAt this point we can look at the number of reads that made it through each step of the workflow for every sample.\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nread_change <- read_tsv(\n  here(work_here, \"mothur_pipeline_read_changes.txt\"),\n  col_names = TRUE\n)\n```\n:::\n\n\n## Preparing for analysis\n\n\n```{.default}\nrename.file(fasta=current, count=current, taxonomy=current, prefix=final)\n```\n\n```         \nCurrent files saved by mothur:\nfasta=final.fasta\ntaxonomy=final.taxonomy\ncount=final.count_table\n```\n\n## Clustering\n\n\n```{.default}\ncluster.split(fasta=final.fasta, count=final.count_table, taxonomy=final.taxonomy, taxlevel=4, cluster=f, processors=$PROC)\ncluster.split(file=final.file, count=final.count_table, processors=$PROC)\n```\n\n```         \nUsing 30 processors.\nSplitting the file...\n/******************************************/\nSelecting sequences for group Vibrionales (1 of 364)\nNumber of unique sequences: 92783\n\nSelected 5390956 sequences from final.count_table.\n\nCalculating distances for group Vibrionales (1 of 364):\n\nSequence    Time    Num_Dists_Below_Cutoff\n\nIt took 902 secs to find distances for 92783 sequences. \n477552179 distances below cutoff 0.03.\n\nOutput File Names:\nfinal.0.dist\n\n...\n\nIt took 8671 seconds to cluster\nMerging the clustered files...\nIt took 14 seconds to merge.\n[WARNING]: Cannot run sens.spec analysis without a column file, \nskipping.\nOutput File Names: \nfinal.opti_mcc.list\n```\n\n\n```{.default}\nsystem(mkdir cluster.split.gsrdb) \nsystem(mv final.opti_mcc.list cluster.split.gsrdb/) \nsystem(mv final.file cluster.split.gsrdb/) \nsystem(mv final.dist cluster.split.gsrdb/)\n```\n\n\n```{.default}\ndist.seqs(fasta=final.fasta, cutoff=0.03, processors=\\$PROC) cluster(column=final.dist, count=final.count_table)\n```\n\n```         \nSequence    Time    Num_Dists_Below_Cutoff\n\nIt took 91935 secs to find distances for 1022766 sequences. \n1096480673 distances below cutoff 0.03.\n\nOutput File Names: \nfinal.dist\n\nYou did not set a cutoff, using 0.03.\n\nClustering final.dist\n\niter    time    label   num_otus    cutoff  tp  tn  fp  fn  sensitivity specificity ppv npv fdr accuracy    mcc f1score\n\n0.03\n0   0   0.03    1022766 0.03    0   5.21928e+11 0   1.09648e+09 0   1   0   0.997904    1   0.997904    0   0   \n1   3187    0.03    130371  0.03    7.80436e+08 5.21829e+11 9.9517e+07  3.16045e+08 0.711764    0.999809    0.886906    0.999395    0.886906    0.999205    0.794146    0.789741    \n2   3706    0.03    119919  0.03    7.82225e+08 5.21828e+11 9.99504e+07 3.14256e+08 0.713396    0.999808    0.8867  0.999398    0.8867  0.999208    0.794965    0.790663    \n3   3712    0.03    119453  0.03    7.82257e+08 5.21828e+11 9.99331e+07 3.14224e+08 0.713425    0.999809    0.886722    0.999398    0.886722    0.999208    0.794991    0.790689    \n\nIt took 21013 seconds to cluster\n\nOutput File Names: \nfinal.opti_mcc.list\nfinal.opti_mcc.steps\nfinal.opti_mcc.sensspec\n```\n\n<!------------------------------------------------------------------------>\n<!-------------------- Use this area to save things ---------------------->\n<!------------------------------------------------------------------------>\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n# Dataset Prep\n\nIn this next part of the workflow our main goal is to create a *microtable object* using the R package [microeco](https://joey711.github.io/phyloseq/) [@liu2021microeco]. The microtable will be used to store the OTU by sample data as well the taxonomic, fasta, and sample data in a single object. More on that in a moment.\n\n\n::: {.cell}\n\n:::\n\n\n## Getting Files from Mothur\n\nTo create a microtable object we need a a sequence table, taxonomy table, and a sample data table. To generate the sequence table we need a shared file from mothur, which we can generate using the command `make.shared`. The data in a shared file represent the number of times that an OTU is observed in multiple samples. \n\n\n```{.default}\nmake.shared(list=final.opti_mcc.list, count=final.count_table, label=0.03)\n```\n\n```         \n0.03\n\nOutput File Names:\nfinal.opti_mcc.shared\n```\nNext we use `classify.otu` to get the OTU taxonomy table. \n\n\n```{.default}\nclassify.otu(list=final.opti_mcc.list, count=final.count_table, taxonomy=final.taxonomy, label=0.03)\n```\n\n```         \n0.03\n\nOutput File Names: \nfinal.opti_mcc.0.03.cons.taxonomy\nfinal.opti_mcc.0.03.cons.tax.summary\n```\n\n\n```{.default}\ncount.groups(shared=final.opti_mcc.shared)\n```\n\n```         \nSize of smallest group: 14.\n\nTotal seqs: 34611554.\n\nOutput File Names: \nfinal.opti_mcc.count.summary\n```\n\n## Prep Data for `microeco`\n\nLike any tool, the microeco package needs the data in a specific form. I formatted our data to match the mock data in the microeco tutorial, specifically [this section](https://chiliubio.github.io/microeco_tutorial/basic-class.html#prepare-the-example-data).\n\n### A. Taxonomy Table\n\nHere is what the taxonomy table looks like in the mock data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npandoc.table(taxonomy_table_16S[10:12, 1:3], emphasize.rownames = FALSE)\n```\n:::\n\n\nOur taxonomy file (below) needs a little wrangling to be properly formatted. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp_tax <- read_delim(here(work_here, \"final.opti_mcc.0.03.cons.taxonomy\"), \n                      delim = \"\\t\")\n```\n:::\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    OTU       Size                                                                                      Taxonomy                                                                                    \n----------- -------- -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n Otu000011   460531                           Bacteria(100);Proteobacteria(100);Gammaproteobacteria(100);Vibrionales(100);Vibrionaceae(100);Vibrionaceae_unclassified(84);                          \n\n Otu000012   445273   Bacteria(100);Proteobacteria(100);Alphaproteobacteria(100);Alphaproteobacteria_unclassified(100);Alphaproteobacteria_unclassified(100);Alphaproteobacteria_unclassified(100); \n\n Otu000013   369620               Bacteria(100);Bacteria_unclassified(100);Bacteria_unclassified(100);Bacteria_unclassified(100);Bacteria_unclassified(100);Bacteria_unclassified(100);             \n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\nSome fancy string manipulation...\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp_tax <- data.frame(sapply(tmp_tax, \n                             gsub, \n                             pattern = \"\\\\(\\\\d+\\\\)\", \n                             replacement = \"\"))\ntmp_tax <- data.frame(sapply(tmp_tax, \n                             gsub, \n                             pattern = \";$\", \n                             replacement = \"\"))\ntmp_tax <- separate_wider_delim(tmp_tax, \n                              cols = Taxonomy, \n                              delim = \";\", names = c(\n                                \"Kingdom\", \"Phylum\", \n                                \"Class\", \"Order\", \n                                \"Family\", \"Genus\" \n                                )\n                              )\ntmp_tax <- data.frame(sapply(tmp_tax, gsub, \n                           pattern = \"^.*_unclassified$\", \n                           replacement = \"\"))\ntmp_tax$Size <- NULL\ntmp_tax <- tibble::column_to_rownames(tmp_tax, \"OTU\")\n```\n:::\n\n\nAnd we get this ...\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n-------------------------------------------------------------\n  &nbsp;     Kingdom        Phylum              Class        \n----------- ---------- ---------------- ---------------------\n Otu000011   Bacteria   Proteobacteria   Gammaproteobacteria \n\n Otu000012   Bacteria   Proteobacteria   Alphaproteobacteria \n\n Otu000013   Bacteria                                        \n-------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrank_prefixes <- c(\n  Kingdom = \"k\", \n  Phylum  = \"p\", \n  Class   = \"c\", \n  Order   = \"o\", \n  Family  = \"f\", \n  Genus   = \"g\"\n)\n\ntmp_tax <- tmp_tax %>%\n  mutate(across(everything(), ~replace_na(., \"\"))) %>% \n  mutate(across(names(rank_prefixes), \n                ~ paste0(rank_prefixes[cur_column()], \"__\", .))) %>%\ntidy_taxonomy()\n```\n:::\n\n\nAnd then this. Excatly like the mock data. \n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n----------------------------------------------------------------------\n  &nbsp;       Kingdom          Phylum                 Class          \n----------- ------------- ------------------- ------------------------\n Otu000011   k__Bacteria   p__Proteobacteria   c__Gammaproteobacteria \n\n Otu000012   k__Bacteria   p__Proteobacteria   c__Alphaproteobacteria \n\n Otu000013   k__Bacteria          p__                   c__           \n----------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\n### B. Sequence Table\n\nHere is what the sequence table looks like in the mock data.\n\nHere is what the sequence table looks like in the mock data.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n--------------------------------------------------------------------\n  &nbsp;    S1   S2   S3   S4   S5   S6   S7   S9   S10   S11   S12 \n---------- ---- ---- ---- ---- ---- ---- ---- ---- ----- ----- -----\n OTU_4272   1    0    1    1    0    0    1    1     0     1     1  \n\n OTU_236    1    4    0    2    35   5    94   0    177   14    27  \n\n OTU_399    9    2    2    4    4    0    3    6     0     1     2  \n--------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\nThese code block will return a properly formatted sequence table. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp_st <- readr::read_delim(\n  here(work_here, \"final.opti_mcc.shared\"),  delim = \"\\t\")\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp_st$numOtus <- NULL\ntmp_st$label <- NULL\ntmp_st <- tmp_st %>%\n  tidyr::pivot_longer(cols = c(-1), names_to = \"tmp\") %>%\n  tidyr::pivot_wider(names_from = c(1))\n\ntmp_st <- tibble::column_to_rownames(tmp_st, \"tmp\")\n```\n:::\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n-----------------------------------------------------------------------\n  &nbsp;     EP_A_AREN_EG_8651   EP_A_AREN_EG_8654   EP_A_AREN_EG_8698 \n----------- ------------------- ------------------- -------------------\n Otu000001          16                 1598                 49         \n\n Otu000002           0                   0                   0         \n\n Otu000003           0                   4                   1         \n\n Otu000004           0                   1                   0         \n\n Otu000005           1                  216                1330        \n-----------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\n# only need to run this if reading in processed files\n# code adds a tab to the beginning of first line\nsed '1s/^/\\t/' tmp_final.opti_mcc.fixed.shared > final.opti_mcc.fixed.shared\n```\n:::\n\n\n### C. Sample Table\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n----------------------------------------------------\n &nbsp;   SampleID   Group   Type       Saline      \n-------- ---------- ------- ------ -----------------\n   S1        S1       IW      NE    Non-saline soil \n\n   S2        S2       IW      NE    Non-saline soil \n\n   S3        S3       IW      NE    Non-saline soil \n----------------------------------------------------\n```\n\n\n:::\n:::\n\n\nNo problem. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamdf <- readRDS(here(\"working_files/ssu/sampledata\", \"sample_data.rds\"))\n\nsamdf <- samdf %>% tibble::column_to_rownames(\"SampleID\")\nsamdf$SampleID <- rownames(samdf)\nsamdf <- samdf %>% dplyr::relocate(SampleID)\n\nsamdf <- samdf %>%\n  dplyr::filter(\n    stringr::str_starts(SampleID, \"Control\", negate = TRUE)\n    )\n```\n:::\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n------------------------------------------------------------------\n      &nbsp;             SampleID        OCEAN   SPECIES   TISSUE \n------------------- ------------------- ------- --------- --------\n EP_A_AREN_EG_8651   EP_A_AREN_EG_8651    EP     A_AREN      EG   \n\n EP_A_AREN_EG_8654   EP_A_AREN_EG_8654    EP     A_AREN      EG   \n\n EP_A_AREN_EG_8698   EP_A_AREN_EG_8698    EP     A_AREN      EG   \n------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\n## Experiment-level Objects\n\nIn the following section we create `microtable` and `phyloseq` objects. These are single, consolidated experiment-level data objects that organize various data types, such as OTU tables (taxa abundance), sample metadata, taxonomic classifications, as well as phylogenetic trees and reference sequences, into one structured format. This unified approach facilitates reproducible, interactive analysis and visualization by bringing all related information together. \n\nWe begin by creating the `microtable` and then use the function `meco2phyloseq` from the [file2meco](https://github.com/ChiLiubio/file2meco) package to create the `phyloseq` object. This way all of the underlying data is identical across the two objects. \n\n::: callout-note\nThese objects contain an OTU table (taxa abundances), sample metadata, and taxonomy table (mapping between OTUs and higher-level taxonomic classifications).\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_info <- samdf\ntax_tab <- tmp_tax\notu_tab <- tmp_st\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp_me <- microtable$new(sample_table = sample_info, \n                         otu_table = otu_tab, \n                         tax_table = tax_tab)\ntmp_me\n```\n:::\n\n\n```         \nmicrotable-class object:\nsample_table have 1849 rows and 13 columns\notu_table have 119453 rows and 1849 columns\ntax_table have 119453 rows and 6 columns\n```\n\n### D. Add Representative Sequence\n\nWe can also add representative sequences for each OTU/ASV. For this step, we use the mothur command `get.oturep`. \n\n\n```{.default}\nget.oturep(column=final.dist, list=final.opti_mcc.list, count=final.count_table, fasta=final.fasta)\n```\n\nThe fasta file it returns needs a little T.L.C.\n\n```\nYou did not provide a label, using 0.03.\n0.03\t119453\n\nOutput File Names: \nfinal.opti_mcc.0.03.rep.count_table\nfinal.opti_mcc.0.03.rep.fasta\n```\n\nFor that we use a tool called SeqKit [@shen2024seqkit2] for fasta defline manipulation.\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nseqkit replace -p \"\\|.*\" -r '' final.opti_mcc.0.03.rep.fasta > tmp2.fa\nseqkit replace -p \".*\\\\t\" -r '' tmp2.fa > tmp3.fa\nseqkit replace -p \"-\" -r '$1' -s -w 0 tmp3.fa > otu_reps.fasta\nrm tmp*\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrep_fasta <- Biostrings::readDNAStringSet(here(work_here, \"otu_reps.fasta\"))\ntmp_me$rep_fasta <- rep_fasta\ntmp_me$tidy_dataset()\ntmp_me\n```\n:::\n\n\n```\nmicrotable-class object:\nsample_table have 1849 rows and 14 columns\notu_table have 119453 rows and 1849 columns\ntax_table have 119453 rows and 6 columns\nrep_fasta have 119453 sequence\n```\n\n\n::: {.cell}\n\n:::\n\n\n## Curate the Data Set\n\nPretty much the last thing to do is remove  low-count samples.\n\n### Remove Low-Count Samples\n\n\n::: {.cell}\n\n```{.r .cell-code}\nthreshold <- 1000\ntmp_no_low <- microeco::clone(me_raw)\ntmp_no_low$otu_table <- me_raw$otu_table %>%\n          dplyr::select(where(~ is.numeric(.) && sum(.) >= threshold))\ntmp_no_low$tidy_dataset()\ntmp_no_low\n```\n:::\n\n\n```         \n41 taxa with 0 abundance are removed from the otu_table ...\n```\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nmicrotable-class object:\nsample_table have 1838 rows and 14 columns\notu_table have 119412 rows and 1838 columns\ntax_table have 119412 rows and 6 columns\nrep_fasta have 119412 sequences\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nme_final <- microeco::clone(tmp_no_low)\n```\n:::\n\n\nLastly, we can use the package [`file2meco`](https://github.com/ChiLiubio/file2meco) to generate a [phyloseq](https://joey711.github.io/phyloseq/) object. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nps_final <- file2meco::meco2phyloseq(me_final)\n```\n:::\n\n\n## Summary\n\nNow time to summarize the data. For this we use the R package [miaverse](https://microbiome.github.io) [@felix2024mia].\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|me_dataset | total_asvs| total_reads| total_samples|\n|:----------|----------:|-----------:|-------------:|\n|original   |     119453|    34611554|          1849|\n|final      |     119412|    34606505|          1838|\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell tbl-cap='Dataset metrics before and after curation.'}\n::: {.cell-output-display}\n\n\n|Metric                |Start    |End      |\n|:---------------------|:--------|:--------|\n|Min. no. of reads     |14       |1160     |\n|Max. no. of reads     |244661   |244661   |\n|Total no. of reads    |34611554 |34606505 |\n|Avg. no. of reads     |18719    |18828    |\n|Median no. of reads   |14834    |14885    |\n|Total ASVs            |119453   |119412   |\n|No. of singleton ASVs |71988    |71959    |\n|% of singleton ASVs   |60.265   |60.261   |\n|Sparsity              |0.996    |0.996    |\n\n\n:::\n:::\n\n\nWe started off with 119453 ASVs and 1849 samples. After removing low-count samples, there were 119412 ASVs and 1838 samples remaining.\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\nWe lost a total of 71 samples after curating the dataset. This includes 60 NC samples and 11 non-NC samples.\n\nHere is a list of non-NC samples that were removed. \n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"WA_A_BAHA_GL_7474\" \"WA_A_CRIS_GL_9624\" \"WA_A_CRIS_HP_7536\"\n [4] \"WA_A_FLOR_GL_9659\" \"WA_A_NUTT_EG_9410\" \"WA_A_PARA_GL_9474\"\n [7] \"WA_A_PARA_MG_9364\" \"WA_A_PARA_ST_9477\" \"WA_A_PCNS_MG_9513\"\n[10] \"WA_A_THOM_ST_9389\" \"WA_A_WEBS_HP_9411\"\n```\n\n\n:::\n:::\n\n\n# Download Results {#download-results}\n\nQuick access to read changes through the pipeline and repseqs detected in negative control samples.\n\n::: {#otu-listing .column-body}\n:::\n\n<!------------------------------------------------------------------------>\n<!-------------------- Use this area to save things ---------------------->\n<!------------------------------------------------------------------------>\n\n<!--------------------------------------->\n<!-- These chunks are for curated data -->\n<!--------------------------------------->\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n::: {.cell}\n\n:::\n\n\n<!-------------------------------------------->\n<!-- These chunks are for  processing data  -->\n<!-------------------------------------------->\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n#### References {.appendix}\n\n::: {#refs}\n:::\n\n\n::: {.cell}\n\n:::\n\n\n#### Detailed Session Info {.appendix}\n\n{{< dstart summary=\"Expand to see Session Info\" >}}\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nâ”€ Session info â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n setting  value\n version  R version 4.5.1 (2025-06-13)\n os       macOS Ventura 13.7.8\n system   x86_64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/Los_Angeles\n date     2025-10-02\n pandoc   3.6.3 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/x86_64/ (via rmarkdown)\n quarto   1.8.25 @ /Applications/quarto/bin/quarto\n\nâ”€ Packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n package                  * version   date (UTC) lib source\n Biobase                  * 2.68.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n BiocGenerics             * 0.54.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n Biostrings               * 2.76.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n downloadthis             * 0.5.0     2025-09-26 [1] Github (fmmattioni/downloadthis@18e3e5a)\n dplyr                    * 1.1.4     2023-11-17 [1] CRAN (R 4.5.0)\n fontawesome              * 0.5.3     2024-11-16 [1] CRAN (R 4.5.0)\n forcats                  * 1.0.0     2023-01-29 [1] CRAN (R 4.5.0)\n fs                       * 1.6.6     2025-04-12 [1] CRAN (R 4.5.0)\n generics                 * 0.1.4     2025-05-09 [1] CRAN (R 4.5.0)\n GenomeInfoDb             * 1.44.2    2025-08-18 [1] Bioconductor 3.21 (R 4.5.1)\n GenomicRanges            * 1.60.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n ggplot2                  * 4.0.0     2025-09-11 [1] CRAN (R 4.5.1)\n here                     * 1.0.2     2025-09-15 [1] CRAN (R 4.5.1)\n htmltools                * 0.5.8.1   2024-04-04 [1] CRAN (R 4.5.0)\n IRanges                  * 2.42.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n lubridate                * 1.9.4     2024-12-08 [1] CRAN (R 4.5.0)\n magrittr                 * 2.0.4     2025-09-12 [1] CRAN (R 4.5.1)\n MatrixGenerics           * 1.20.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n matrixStats              * 1.5.0     2025-01-07 [1] CRAN (R 4.5.0)\n mia                      * 1.15.6    2024-11-22 [1] Bioconductor 3.21 (R 4.5.0)\n microbiome               * 1.30.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n microeco                 * 1.15.0    2025-05-18 [1] CRAN (R 4.5.0)\n microViz                 * 0.12.7    2025-08-01 [1] https://david-barnett.r-universe.dev (R 4.5.1)\n MultiAssayExperiment     * 1.34.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n pander                   * 0.6.6     2025-03-01 [1] CRAN (R 4.5.0)\n phyloseq                 * 1.52.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n purrr                    * 1.1.0     2025-07-10 [1] CRAN (R 4.5.1)\n R.methodsS3              * 1.8.2     2022-06-13 [1] CRAN (R 4.5.0)\n R.oo                     * 1.27.1    2025-05-02 [1] CRAN (R 4.5.0)\n R.utils                  * 2.13.0    2025-02-24 [1] CRAN (R 4.5.0)\n reactable                * 0.4.4     2023-03-12 [1] CRAN (R 4.5.0)\n reactablefmtr            * 2.0.0     2022-03-16 [1] CRAN (R 4.5.0)\n readr                    * 2.1.5     2024-01-10 [1] CRAN (R 4.5.0)\n S4Vectors                * 0.46.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n seqinr                   * 4.2-36    2023-12-08 [1] CRAN (R 4.5.0)\n sessioninfo              * 1.2.3     2025-02-05 [1] CRAN (R 4.5.0)\n SingleCellExperiment     * 1.30.1    2025-05-05 [1] Bioconductor 3.21 (R 4.5.0)\n stringr                  * 1.5.2     2025-09-08 [1] CRAN (R 4.5.1)\n SummarizedExperiment     * 1.38.1    2025-04-28 [1] Bioconductor 3.21 (R 4.5.0)\n tibble                   * 3.3.0     2025-06-08 [1] CRAN (R 4.5.0)\n tidyr                    * 1.3.1     2024-01-24 [1] CRAN (R 4.5.0)\n tidyverse                * 2.0.0     2023-02-22 [1] CRAN (R 4.5.0)\n tinytable                * 0.13.0.10 2025-09-07 [1] https://vincentarelbundock.r-universe.dev (R 4.5.1)\n TreeSummarizedExperiment * 2.16.1    2025-05-08 [1] Bioconductor 3.21 (R 4.5.0)\n XVector                  * 0.48.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n zip                      * 2.3.3     2025-05-13 [1] CRAN (R 4.5.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.5-x86_64/Resources/library\n * â”€â”€ Packages attached to the search path.\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n======   Devtools Session info   ===================================\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n package                  * version   date (UTC) lib source\n Biobase                  * 2.68.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n BiocGenerics             * 0.54.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n Biostrings               * 2.76.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n downloadthis             * 0.5.0     2025-09-26 [1] Github (fmmattioni/downloadthis@18e3e5a)\n dplyr                    * 1.1.4     2023-11-17 [1] CRAN (R 4.5.0)\n fontawesome              * 0.5.3     2024-11-16 [1] CRAN (R 4.5.0)\n forcats                  * 1.0.0     2023-01-29 [1] CRAN (R 4.5.0)\n fs                       * 1.6.6     2025-04-12 [1] CRAN (R 4.5.0)\n generics                 * 0.1.4     2025-05-09 [1] CRAN (R 4.5.0)\n GenomeInfoDb             * 1.44.2    2025-08-18 [1] Bioconductor 3.21 (R 4.5.1)\n GenomicRanges            * 1.60.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n ggplot2                  * 4.0.0     2025-09-11 [1] CRAN (R 4.5.1)\n here                     * 1.0.2     2025-09-15 [1] CRAN (R 4.5.1)\n htmltools                * 0.5.8.1   2024-04-04 [1] CRAN (R 4.5.0)\n IRanges                  * 2.42.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n lubridate                * 1.9.4     2024-12-08 [1] CRAN (R 4.5.0)\n magrittr                 * 2.0.4     2025-09-12 [1] CRAN (R 4.5.1)\n MatrixGenerics           * 1.20.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n matrixStats              * 1.5.0     2025-01-07 [1] CRAN (R 4.5.0)\n mia                      * 1.15.6    2024-11-22 [1] Bioconductor 3.21 (R 4.5.0)\n microbiome               * 1.30.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n microeco                 * 1.15.0    2025-05-18 [1] CRAN (R 4.5.0)\n microViz                 * 0.12.7    2025-08-01 [1] https://david-barnett.r-universe.dev (R 4.5.1)\n MultiAssayExperiment     * 1.34.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n pander                   * 0.6.6     2025-03-01 [1] CRAN (R 4.5.0)\n phyloseq                 * 1.52.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n purrr                    * 1.1.0     2025-07-10 [1] CRAN (R 4.5.1)\n R.methodsS3              * 1.8.2     2022-06-13 [1] CRAN (R 4.5.0)\n R.oo                     * 1.27.1    2025-05-02 [1] CRAN (R 4.5.0)\n R.utils                  * 2.13.0    2025-02-24 [1] CRAN (R 4.5.0)\n reactable                * 0.4.4     2023-03-12 [1] CRAN (R 4.5.0)\n reactablefmtr            * 2.0.0     2022-03-16 [1] CRAN (R 4.5.0)\n readr                    * 2.1.5     2024-01-10 [1] CRAN (R 4.5.0)\n S4Vectors                * 0.46.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n seqinr                   * 4.2-36    2023-12-08 [1] CRAN (R 4.5.0)\n sessioninfo              * 1.2.3     2025-02-05 [1] CRAN (R 4.5.0)\n SingleCellExperiment     * 1.30.1    2025-05-05 [1] Bioconductor 3.21 (R 4.5.0)\n stringr                  * 1.5.2     2025-09-08 [1] CRAN (R 4.5.1)\n SummarizedExperiment     * 1.38.1    2025-04-28 [1] Bioconductor 3.21 (R 4.5.0)\n tibble                   * 3.3.0     2025-06-08 [1] CRAN (R 4.5.0)\n tidyr                    * 1.3.1     2024-01-24 [1] CRAN (R 4.5.0)\n tidyverse                * 2.0.0     2023-02-22 [1] CRAN (R 4.5.0)\n tinytable                * 0.13.0.10 2025-09-07 [1] https://vincentarelbundock.r-universe.dev (R 4.5.1)\n TreeSummarizedExperiment * 2.16.1    2025-05-08 [1] Bioconductor 3.21 (R 4.5.0)\n XVector                  * 0.48.0    2025-04-15 [1] Bioconductor 3.21 (R 4.5.0)\n zip                      * 2.3.3     2025-05-13 [1] CRAN (R 4.5.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.5-x86_64/Resources/library\n * â”€â”€ Packages attached to the search path.\n```\n\n\n:::\n:::\n\n\n{{< dstop >}}\n\n#### Last updated on {.appendix}\n\n2025-10-02\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}